{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: Flash Attention 2 is not available. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m cache_dir \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m attn_implementation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mflash_attention_2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m model \u001b[39m=\u001b[39m LlavaQwenForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m                 model_name_or_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m                 cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m                 attn_implementation\u001b[39m=\u001b[39;49mattn_implementation,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m                 torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m                 low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcustomized_kwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m             )\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:3769\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3766\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   3768\u001b[0m config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)  \u001b[39m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m-> 3769\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_autoset_attn_implementation(\n\u001b[1;32m   3770\u001b[0m     config, use_flash_attention_2\u001b[39m=\u001b[39;49muse_flash_attention_2, torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype, device_map\u001b[39m=\u001b[39;49mdevice_map\n\u001b[1;32m   3771\u001b[0m )\n\u001b[1;32m   3773\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3774\u001b[0m     \u001b[39m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m   3775\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:1519\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     config\u001b[39m.\u001b[39m_attn_implementation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mflash_attention_2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39m_attn_implementation \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mflash_attention_2\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_check_and_enable_flash_attn_2(\n\u001b[1;32m   1520\u001b[0m         config,\n\u001b[1;32m   1521\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   1522\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   1523\u001b[0m         hard_check_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1524\u001b[0m         check_device_map\u001b[39m=\u001b[39;49mcheck_device_map,\n\u001b[1;32m   1525\u001b[0m     )\n\u001b[1;32m   1526\u001b[0m \u001b[39melif\u001b[39;00m requested_attn_implementation \u001b[39min\u001b[39;00m [\u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39msdpa\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1527\u001b[0m     \u001b[39m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_sdpa(\n\u001b[1;32m   1529\u001b[0m         config,\n\u001b[1;32m   1530\u001b[0m         hard_check_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m requested_attn_implementation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1531\u001b[0m     )\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:1630\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   1627\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpreface\u001b[39m}\u001b[39;00m\u001b[39m you need flash_attn package version to be greater or equal than 2.1.0. Detected version \u001b[39m\u001b[39m{\u001b[39;00mflash_attention_version\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00minstall_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1628\u001b[0m         )\n\u001b[1;32m   1629\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpreface\u001b[39m}\u001b[39;00m\u001b[39m Flash Attention 2 is not available. \u001b[39m\u001b[39m{\u001b[39;00minstall_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1631\u001b[0m \u001b[39melif\u001b[39;00m torch\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39mhip:\n\u001b[1;32m   1632\u001b[0m     \u001b[39mif\u001b[39;00m flash_attention_version \u001b[39m<\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.4\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: Flash Attention 2 is not available. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/share/LXRlxr0_0/code/videoxl2/videoxl2/longva')\n",
    "\n",
    "from longva.model import *\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_name_or_path = '/share/LXRlxr0_0/code/videoxl2/videoxl2/checkpoints/videoxl2_0410'\n",
    "\n",
    "customized_kwargs = dict()\n",
    "cfg_pretrained = None\n",
    "\n",
    "overwrite_config = {}\n",
    "cfg_pretrained = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "overwrite_config[\"mm_resampler_type\"] = \"spatial_pool\"\n",
    "overwrite_config[\"mm_spatial_pool_stride\"] = 2\n",
    "overwrite_config[\"mm_spatial_pool_out_channels\"] = 1152\n",
    "overwrite_config[\"mm_spatial_pool_mode\"] = \"average\"\n",
    "\n",
    "if overwrite_config:\n",
    "    assert cfg_pretrained is not None, \"cfg_pretrained is None\"\n",
    "\n",
    "    for k, v in overwrite_config.items():\n",
    "        setattr(cfg_pretrained, k, v)\n",
    "\n",
    "    customized_kwargs[\"config\"] = cfg_pretrained\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type siglip to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: /share/LXRlxr0_0/code/videoxlturbo2.0/videoxl/google/siglip-so400m-patch14-384\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CLIPVisionModel:\n\tsize mismatch for vision_model.embeddings.patch_embedding.weight: copying a param with shape torch.Size([1152, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 32, 32]).\n\tsize mismatch for vision_model.embeddings.position_embedding.weight: copying a param with shape torch.Size([729, 1152]) from checkpoint, the shape in current model is torch.Size([50, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.post_layernorm.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.post_layernorm.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cache_dir \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m attn_implementation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msdpa\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m LlavaQwenForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                 model_name_or_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                 cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                 attn_implementation\u001b[39m=\u001b[39;49mattn_implementation,\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                 torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                 low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcustomized_kwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-wulanchabu.data.aliyun.com/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/train/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             )\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:3775\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3769\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3770\u001b[0m     config, use_flash_attention_2\u001b[39m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map\n\u001b[1;32m   3771\u001b[0m )\n\u001b[1;32m   3773\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3774\u001b[0m     \u001b[39m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3775\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3777\u001b[0m \u001b[39m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3778\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/language_model/llava_qwen.py:55\u001b[0m, in \u001b[0;36mLlavaQwenForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m config\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllava_qwen\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m config\u001b[39m.\u001b[39mrope_scaling \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlavaQwenModel(config)\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/language_model/llava_qwen.py:43\u001b[0m, in \u001b[0;36mLlavaQwenModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: Qwen2Config):\n\u001b[0;32m---> 43\u001b[0m     \u001b[39msuper\u001b[39;49m(LlavaQwenModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config)\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/llava_arch.py:44\u001b[0m, in \u001b[0;36mLlavaMetaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mmm_vision_tower\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     43\u001b[0m     delay_load \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mdelay_load\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower \u001b[39m=\u001b[39m build_vision_tower(config, delay_load\u001b[39m=\u001b[39;49mdelay_load)\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_resampler \u001b[39m=\u001b[39m build_vision_resampler(config, vision_tower\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower)\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmm_projector \u001b[39m=\u001b[39m build_vision_projector(config, vision_cfg\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower\u001b[39m.\u001b[39mconfig)\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/multimodal_encoder/builder.py:22\u001b[0m, in \u001b[0;36mbuild_vision_tower\u001b[0;34m(vision_tower_cfg, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m CLIPVisionTowerS2(vision_tower, args\u001b[39m=\u001b[39mvision_tower_cfg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     21\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         \u001b[39mreturn\u001b[39;00m CLIPVisionTower(vision_tower, args\u001b[39m=\u001b[39;49mvision_tower_cfg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     23\u001b[0m \u001b[39m# elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown vision tower: \u001b[39m\u001b[39m{\u001b[39;00mvision_tower\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/multimodal_encoder/clip_encoder.py:24\u001b[0m, in \u001b[0;36mCLIPVisionTower.__init__\u001b[0;34m(self, vision_tower, args, delay_load)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m delay_load:\n\u001b[1;32m     23\u001b[0m     rank0_print(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading vision tower: \u001b[39m\u001b[39m{\u001b[39;00mvision_tower\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_model()\n\u001b[1;32m     25\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mgetattr\u001b[39m(args, \u001b[39m\"\u001b[39m\u001b[39munfreeze_mm_vision_tower\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     26\u001b[0m     \u001b[39m# TODO: better detector is needed.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     rank0_print(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/share/LXRlxr0_0/code/videoxl2/videoxl2/longva/longva/model/multimodal_encoder/clip_encoder.py:41\u001b[0m, in \u001b[0;36mCLIPVisionTower.load_model\u001b[0;34m(self, device_map)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor \u001b[39m=\u001b[39m CLIPImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower_name)\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower \u001b[39m=\u001b[39m CLIPVisionModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_tower_name, device_map\u001b[39m=\u001b[39;49mdevice_map)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_tower\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:3903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3893\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3894\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3896\u001b[0m     (\n\u001b[1;32m   3897\u001b[0m         model,\n\u001b[1;32m   3898\u001b[0m         missing_keys,\n\u001b[1;32m   3899\u001b[0m         unexpected_keys,\n\u001b[1;32m   3900\u001b[0m         mismatched_keys,\n\u001b[1;32m   3901\u001b[0m         offload_index,\n\u001b[1;32m   3902\u001b[0m         error_msgs,\n\u001b[0;32m-> 3903\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3904\u001b[0m         model,\n\u001b[1;32m   3905\u001b[0m         state_dict,\n\u001b[1;32m   3906\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3907\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3908\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3909\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3910\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3911\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3912\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3913\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3914\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3915\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3916\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3917\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3918\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3919\u001b[0m         gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   3920\u001b[0m     )\n\u001b[1;32m   3922\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3923\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/share/LXRlxr0_0/env/xl/lib/python3.10/site-packages/transformers/modeling_utils.py:4435\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   4432\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   4433\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4434\u001b[0m         )\n\u001b[0;32m-> 4435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unexpected_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   4438\u001b[0m     archs \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CLIPVisionModel:\n\tsize mismatch for vision_model.embeddings.patch_embedding.weight: copying a param with shape torch.Size([1152, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 32, 32]).\n\tsize mismatch for vision_model.embeddings.position_embedding.weight: copying a param with shape torch.Size([729, 1152]) from checkpoint, the shape in current model is torch.Size([50, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.0.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.0.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.1.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.1.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.2.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.2.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.3.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.3.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.4.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.4.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.5.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.5.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.6.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.6.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.7.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.7.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.8.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.8.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.9.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.9.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.10.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([1152, 1152]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm1.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([4304, 1152]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([4304]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([1152, 4304]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for vision_model.encoder.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm2.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.encoder.layers.11.layer_norm2.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.post_layernorm.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for vision_model.post_layernorm.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "cache_dir = None\n",
    "attn_implementation = \"sdpa\"\n",
    "model = LlavaQwenForCausalLM.from_pretrained(\n",
    "                model_name_or_path,\n",
    "                cache_dir=cache_dir,\n",
    "                attn_implementation=attn_implementation,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=False,\n",
    "                **customized_kwargs,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
