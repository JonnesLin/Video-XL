{
    "args": {
        "config": "",
        "model": "videoxl2",
        "tasks": "lvbench_documentary",
        "model_args": "pretrained=/share/minghao/Models2/Video-XL-2,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=600,fps=1,max_fps=4,video_decode_backend=decord",
        "num_fewshot": null,
        "batch_size": "1",
        "max_batch_size": null,
        "device": null,
        "output_path": "/share/minghao/VideoProjects/Upload/Video-XL/Video-XL-2/eval/lvu/pure/logs/videoxl2-lvbench/0629_1948_videoxl2_videoxl2_model_args_f59ce9",
        "limit": null,
        "use_cache": null,
        "cache_requests": null,
        "check_integrity": false,
        "write_out": false,
        "log_samples": true,
        "wandb_log_samples": false,
        "log_samples_suffix": "videoxl2",
        "system_instruction": null,
        "apply_chat_template": false,
        "fewshot_as_multiturn": false,
        "show_config": false,
        "include_path": null,
        "gen_kwargs": "",
        "verbosity": "INFO",
        "wandb_args": "",
        "timezone": "Asia/Singapore",
        "hf_hub_log_args": ",output_path=./logs/videoxl2-lvbench,token=YOUR_HF_KEY",
        "predict_only": false,
        "seed": [
            0,
            1234,
            1234,
            1234
        ],
        "trust_remote_code": false
    },
    "model_configs": {
        "task": "lvbench_documentary",
        "dataset_path": "/share/minghao/Datasets/LVBench",
        "dataset_name": "lvbench_documentary",
        "dataset_kwargs": {
            "token": true
        },
        "test_split": "train",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function lvbench_mc_doc_to_visual at 0x7f6f41d8b880>",
        "doc_to_text": "<function lvbench_mc_doc_to_text at 0x7f6edb3d9b40>",
        "doc_to_target": "answer",
        "process_results": "<function lvbench_mc_process_results at 0x7f6edb3da200>",
        "description": "",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
            {
                "metric": "lvbench_mc_accuracy",
                "aggregation": "<function lvbench_mc_aggregate_results at 0x7f6edb3da830>",
                "higher_is_better": true
            }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
            "max_new_tokens": 16,
            "temperature": 0.0,
            "top_p": 1.0,
            "num_beams": 1,
            "do_sample": false,
            "until": [
                "\n\n"
            ]
        },
        "repeats": 1,
        "should_decontaminate": false,
        "lmms_eval_specific_kwargs": {
            "default": {
                "sub_task": "lvbench_documentary",
                "post_prompt": "Answer with the option's letter from the given choices directly."
            },
            "sub_task": "lvbench_documentary",
            "post_prompt": "Answer with the option's letter from the given choices directly."
        }
    },
    "logs": [
        {
            "doc_id": 0,
            "doc": {
                "time_reference": "06:23-06:23",
                "video": "TiQBTesZUJQ",
                "answer": "(A) A skull and bones",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) A skull and bones",
                    "(B) A cat",
                    "(C) A word",
                    "(D) A Dog"
                ],
                "type": "documentary",
                "question": "There is a scene of of cutting trees to get the sap in the video. Immediately after it there is a scene of man's arm. What's tattooed on the man's arm?"
            },
            "target": "(A) A skull and bones",
            "arguments": [
                "There is a scene of of cutting trees to get the sap in the video. Immediately after it there is a scene of man's arm. What's tattooed on the man's arm?\nA. (A) A skull and bones\nB. (B) A cat\nC. (C) A word\nD. (D) A Dog\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8b20>"
                    ]
                },
                0,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a0cea840a7bdf73ba9c62ef7c17d4d0f1d30805eae5fe6fa590c3185e14a08e4",
            "target_hash": "71565cd097ddeee5367d1f9cbf7dc1167fc690b67ee63459a425a871c16d4bee",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 1,
            "doc": {
                "time_reference": "08:45-09:00",
                "video": "TiQBTesZUJQ",
                "answer": "(B) Night",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Noon",
                    "(B) Night",
                    "(C) Early morning",
                    "(D) Morning"
                ],
                "type": "documentary",
                "question": "When do animals move out?"
            },
            "target": "(B) Night",
            "arguments": [
                "When do animals move out?\nA. (A) Noon\nB. (B) Night\nC. (C) Early morning\nD. (D) Morning\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1ab0>"
                    ]
                },
                1,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e7bf3c71225d9fce41b0984e66ceb8552e1935db5ae021053a236447ebca0325",
            "target_hash": "6839078ff44c8cce6160dbd5832eea2ecaae54e74742689943e01460ba17b62b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 2,
            "doc": {
                "time_reference": "13:30-13:39",
                "video": "TiQBTesZUJQ",
                "answer": "(D) Wild boar",
                "question_type": [
                    "event understanding",
                    "temporal grounding",
                    "reasoning"
                ],
                "candidates": [
                    "(A) Fish",
                    "(B) Lamb",
                    "(C) Deer",
                    "(D) Wild boar"
                ],
                "type": "documentary",
                "question": "What is the most likely animal the men try to hunt at 13:13?"
            },
            "target": "(D) Wild boar",
            "arguments": [
                "What is the most likely animal the men try to hunt at 13:13?\nA. (A) Fish\nB. (B) Lamb\nC. (C) Deer\nD. (D) Wild boar\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bfd90>"
                    ]
                },
                2,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "610b7f84fbdf321dd59e82dd1e2c5fef28606056a6305773d219b4c7f4ed154f",
            "target_hash": "0b172bde3525212da94ee0ce3c7931035571db8d7155b48357444a65acf0537a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 3,
            "doc": {
                "time_reference": "07:00-17:00",
                "video": "TiQBTesZUJQ",
                "answer": "(C) Rain forest",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Marine",
                    "(B) Plateau",
                    "(C) Rain forest",
                    "(D) Forest"
                ],
                "type": "documentary",
                "question": "The video contains lots of ___ clips."
            },
            "target": "(C) Rain forest",
            "arguments": [
                "The video contains lots of ___ clips.\nA. (A) Marine\nB. (B) Plateau\nC. (C) Rain forest\nD. (D) Forest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1090>"
                    ]
                },
                3,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d61ec87c8176ffce0e26aec9a401f33b0fc6b6a17b00aaff0575bdd22daa81a9",
            "target_hash": "229248ff3c9e7d4221c51119d4a1781e95711234cbcab4c9bb9b7326e90560f0",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 4,
            "doc": {
                "time_reference": "19:02-19:03",
                "video": "TiQBTesZUJQ",
                "answer": "(A) Chopsticks",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Chopsticks",
                    "(B) Spoon",
                    "(C) Spatulas",
                    "(D) Syringes"
                ],
                "type": "documentary",
                "question": "What tool do people use to pick up a sticky, white to transparent food substance?"
            },
            "target": "(A) Chopsticks",
            "arguments": [
                "What tool do people use to pick up a sticky, white to transparent food substance?\nA. (A) Chopsticks\nB. (B) Spoon\nC. (C) Spatulas\nD. (D) Syringes\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8250>"
                    ]
                },
                4,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "fe2c581ee49a9ed98440808cd1b0748a8af21894eac837c562dae07b0f5d6571",
            "target_hash": "593ae058373e510391b060415c31a734ef04d8753df4999631410f8f413bb9d4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 5,
            "doc": {
                "time_reference": "16:30-17:15",
                "video": "TiQBTesZUJQ",
                "answer": "(C) Fishing",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Swimming",
                    "(B) Rowing",
                    "(C) Fishing",
                    "(D) Diving"
                ],
                "type": "documentary",
                "question": "What are the people going to do from 16:30-16:35?"
            },
            "target": "(C) Fishing",
            "arguments": [
                "What are the people going to do from 16:30-16:35?\nA. (A) Swimming\nB. (B) Rowing\nC. (C) Fishing\nD. (D) Diving\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea6e0>"
                    ]
                },
                5,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "7c2323c3b1582a985a34ff0ea81643b58c16f06e2b0fbd2e53237b14d292acf4",
            "target_hash": "269befd351d45641797efb67d1aec89d67e4f3b69c5dfce33d48e6420e860f81",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 6,
            "doc": {
                "time_reference": "31:02-32:00",
                "video": "TiQBTesZUJQ",
                "answer": "(C) Motocycle",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Car",
                    "(B) Bike",
                    "(C) Motocycle",
                    "(D) Skateboard"
                ],
                "type": "documentary",
                "question": "What kind of transportation do the people carry on the boat?"
            },
            "target": "(C) Motocycle",
            "arguments": [
                "What kind of transportation do the people carry on the boat?\nA. (A) Car\nB. (B) Bike\nC. (C) Motocycle\nD. (D) Skateboard\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb640>"
                    ]
                },
                6,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "38b30d3d8127ab54e9faf736529851da5e4fd750c2369b63480bbec6129eb2a1",
            "target_hash": "a51ebbe2cdadfa17707070e2141b53781568df1368e6a19085fd7838631bc13c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 7,
            "doc": {
                "time_reference": "22:17-42:05",
                "video": "TiQBTesZUJQ",
                "answer": "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Reporter spends three months with the Penan people, understanding their psychological characteristics and documenting the lifestyle of some Penan people living in the city",
                    "(B) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also documented the lifestyle of some Penan people living in the city",
                    "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
                    "(D) Reporter spends three months with the Penan people, understanding their psychological characteristics and recording the lifestyle of some of the Penan people living in the countryside"
                ],
                "type": "documentary",
                "question": "What happens between 22:17-42:05?"
            },
            "target": "(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village",
            "arguments": [
                "What happens between 22:17-42:05?\nA. (A) Reporter spends three months with the Penan people, understanding their psychological characteristics and documenting the lifestyle of some Penan people living in the city\nB. (B) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also documented the lifestyle of some Penan people living in the city\nC. (C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some Penan people living in the village\nD. (D) Reporter spends three months with the Penan people, understanding their psychological characteristics and recording the lifestyle of some of the Penan people living in the countryside\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c190>"
                    ]
                },
                7,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4a72c5369f823027b8a0c0fb96950f285116dcd252ac7c7fbcdd8542defab4fc",
            "target_hash": "72ab04df2816ae9b6693073cb4aa2bc467ad61ac1a0d211a92143529a3169393",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 8,
            "doc": {
                "time_reference": "32:40-33:00",
                "video": "TiQBTesZUJQ",
                "answer": "(D) Rice wine",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Voldka",
                    "(B) Sparkling water",
                    "(C) Spirte",
                    "(D) Rice wine"
                ],
                "type": "documentary",
                "question": "One of the woman give the reporter a kind of beverage in a glass cup, that is ___."
            },
            "target": "(D) Rice wine",
            "arguments": [
                "One of the woman give the reporter a kind of beverage in a glass cup, that is ___.\nA. (A) Voldka\nB. (B) Sparkling water\nC. (C) Spirte\nD. (D) Rice wine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1150>"
                    ]
                },
                8,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3c1a2b7519ad8db7e3e47384b663e332c1865ee38301835cbe81af1bc9411fb5",
            "target_hash": "276e9d0a21ae717991e61bd49b3cfd002b551d81283320ee06c5d5fedc375070",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 9,
            "doc": {
                "time_reference": "34:22-34:22",
                "video": "TiQBTesZUJQ",
                "answer": "(D) Playing with monkey",
                "question_type": [
                    "entity recognition",
                    "temporal grounding",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Playing with toys",
                    "(B) Writing homework",
                    "(C) Scratching her leg",
                    "(D) Playing with monkey"
                ],
                "type": "documentary",
                "question": "What is Saya's daughter doing at 34:22?"
            },
            "target": "(D) Playing with monkey",
            "arguments": [
                "What is Saya's daughter doing at 34:22?\nA. (A) Playing with toys\nB. (B) Writing homework\nC. (C) Scratching her leg\nD. (D) Playing with monkey\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c02b0>"
                    ]
                },
                9,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "06b22ed0888bd7df4be15d3bb4f399b4f513ba0710d785c88349bed9b399c1d4",
            "target_hash": "452c267539ddaae2e3889c08ef81b4dea37c11be1a402537c9f32eaf898c5272",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 10,
            "doc": {
                "time_reference": "38:00-39:00",
                "video": "TiQBTesZUJQ",
                "answer": "(A) Making a fire",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Making a fire",
                    "(B) Peeling a tree",
                    "(C) Sharpening a knife",
                    "(D) Making a food"
                ],
                "type": "documentary",
                "question": "There is a close-up scene of a woman doing something on a leaf. She is ___."
            },
            "target": "(A) Making a fire",
            "arguments": [
                "There is a close-up scene of a woman doing something on a leaf. She is ___.\nA. (A) Making a fire\nB. (B) Peeling a tree\nC. (C) Sharpening a knife\nD. (D) Making a food\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bcc70>"
                    ]
                },
                10,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "61a4466eefd986542b6fb6cbd85129022b73588f55ffb699c3f88f5c3a5a85be",
            "target_hash": "61ca6c74f5f4250a161c0e17aa5c3e338de2bd4bc2c95802f341b587aec9d7c9",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 11,
            "doc": {
                "time_reference": "39:49-39:49",
                "video": "TiQBTesZUJQ",
                "answer": "(B) Scorpion",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Mantis",
                    "(B) Scorpion",
                    "(C) Spider",
                    "(D) Cockroach"
                ],
                "type": "documentary",
                "question": "___ is an insect appears in the video."
            },
            "target": "(B) Scorpion",
            "arguments": [
                "___ is an insect appears in the video.\nA. (A) Mantis\nB. (B) Scorpion\nC. (C) Spider\nD. (D) Cockroach\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea320>"
                    ]
                },
                11,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "713c9b56e6c31a749df4c57f6ac3eff7d673b2134f55127f549d7bdd55281e92",
            "target_hash": "024b60c62fe1a4f6c8aef3bc2b4ad5def4f717b31d820436202550e439d34c4b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 12,
            "doc": {
                "time_reference": "39:00-41:00",
                "video": "TiQBTesZUJQ",
                "answer": "(D) Car",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Boat",
                    "(B) Bike",
                    "(C) Motorcycle",
                    "(D) Car"
                ],
                "type": "documentary",
                "question": "What kind of transportation does the reporter use to leave the tribe?"
            },
            "target": "(D) Car",
            "arguments": [
                "What kind of transportation does the reporter use to leave the tribe?\nA. (A) Boat\nB. (B) Bike\nC. (C) Motorcycle\nD. (D) Car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb6a0>"
                    ]
                },
                12,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b59121093dcbb2c158a066680770b302dce9feec0a3203eb36bc403df593c169",
            "target_hash": "a3843657b7986d8d8ccb797019a8c4eb24798e4a4499fd1af2f6f7994dd68c9c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 13,
            "doc": {
                "time_reference": "51:00-52:00",
                "video": "TiQBTesZUJQ",
                "answer": "(D) Plan to visit the Penan tribe in the more distant future",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Plan to visit the Penan tribe in the near future",
                    "(B) Plan to live in the Penan tribe in the distant future",
                    "(C) Plan to live in the Penan tribe in the near future",
                    "(D) Plan to visit the Penan tribe in the more distant future"
                ],
                "type": "documentary",
                "question": "What might the reporter do?"
            },
            "target": "(D) Plan to visit the Penan tribe in the more distant future",
            "arguments": [
                "What might the reporter do?\nA. (A) Plan to visit the Penan tribe in the near future\nB. (B) Plan to live in the Penan tribe in the distant future\nC. (C) Plan to live in the Penan tribe in the near future\nD. (D) Plan to visit the Penan tribe in the more distant future\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9930>"
                    ]
                },
                13,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a2d1b02d52822fd2fc730edd9ccd49672e0c089158e71b43a3d22cfdbaa22e23",
            "target_hash": "a028146ec0ee47879161804c7166cae579a2f0bb6adab908f86946c6bb0373d4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 14,
            "doc": {
                "time_reference": "13:17-13:17",
                "video": "5dZ_lvDgevk",
                "answer": "(C) Li Kaifu",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Ma Huateng",
                    "(B) Jack Ma",
                    "(C) Li Kaifu",
                    "(D) Robin Li"
                ],
                "type": "documentary",
                "question": "Who is the man wearing glasses, a black suit, and a blue patterned tie?"
            },
            "target": "(C) Li Kaifu",
            "arguments": [
                "Who is the man wearing glasses, a black suit, and a blue patterned tie?\nA. (A) Ma Huateng\nB. (B) Jack Ma\nC. (C) Li Kaifu\nD. (D) Robin Li\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9ff0>"
                    ]
                },
                14,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8ed200028f3350168ce04d86f71481b8f8abba0f1e5fda734deb57fc1f674c19",
            "target_hash": "718b8427d7bad2cd0ab73e95d0fd678d0faf2c28b09f4b906d60b9197ef30f80",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 15,
            "doc": {
                "time_reference": "00:00-114:16",
                "video": "5dZ_lvDgevk",
                "answer": "(A) A technology documentary",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) A technology documentary",
                    "(B) A news report",
                    "(C) A travel documentary",
                    "(D) A travel vlog"
                ],
                "type": "documentary",
                "question": "What type of video is this?"
            },
            "target": "(A) A technology documentary",
            "arguments": [
                "What type of video is this?\nA. (A) A technology documentary\nB. (B) A news report\nC. (C) A travel documentary\nD. (D) A travel vlog\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7de40>"
                    ]
                },
                15,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "60af01fee1dc125a3591a5507992c585cb4b0c294e6fea57c229faf13d809388",
            "target_hash": "06244efa2d238d1729f7d6cf1b14335da81dfb36e5fc67001cef029ccfb6e817",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 16,
            "doc": {
                "time_reference": "22:35-22:35",
                "video": "5dZ_lvDgevk",
                "answer": "(A) Engaged",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Engaged",
                    "(B) Technology",
                    "(C) Embark",
                    "(D) Speed"
                ],
                "type": "documentary",
                "question": "What is the label on the toggle switch button after the first driver sitting in a truck without driving in chapter 2 'The promise' of the video?"
            },
            "target": "(A) Engaged",
            "arguments": [
                "What is the label on the toggle switch button after the first driver sitting in a truck without driving in chapter 2 'The promise' of the video?\nA. (A) Engaged\nB. (B) Technology\nC. (C) Embark\nD. (D) Speed\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1660>"
                    ]
                },
                16,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5863c5f2e6fe985024a8b829ed754ffd2a952d32a519aa5eed6d0d63ef16a4f0",
            "target_hash": "4bf2a9f588a1ae714b32283f093ba7a20d23bc4bafd456667c776c23b720805e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 17,
            "doc": {
                "time_reference": "21:44-25:52",
                "video": "5dZ_lvDgevk",
                "answer": "(A) The scene is filmed in reality",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) The scene is filmed in reality",
                    "(B) The scene is a simulated representation of reality",
                    "(C) This scene has neither simulation nor reality",
                    "(D) This scene has both simulation and real-life scenarios"
                ],
                "type": "documentary",
                "question": "Is the autonomous vehicle driving scene after a man in black said something real or simulated?"
            },
            "target": "(A) The scene is filmed in reality",
            "arguments": [
                "Is the autonomous vehicle driving scene after a man in black said something real or simulated?\nA. (A) The scene is filmed in reality\nB. (B) The scene is a simulated representation of reality\nC. (C) This scene has neither simulation nor reality\nD. (D) This scene has both simulation and real-life scenarios\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c05b0>"
                    ]
                },
                17,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "835f181f1ec853be50c261186d7bd8bf293e961cba274f1cf0332df2541d6675",
            "target_hash": "a4cf51942267819312e1bea76833c907743647f7ecdaec281947c3d6fddc1b34",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 18,
            "doc": {
                "time_reference": "37:00-42:00",
                "video": "5dZ_lvDgevk",
                "answer": "(C) Interviewer and interviewee",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Colleagues",
                    "(B) Husband and wife",
                    "(C) Interviewer and interviewee",
                    "(D) Brother and sister"
                ],
                "type": "documentary",
                "question": "What is the relationship between the first two people appear in the video after the appearance of blue words 'TRUCK WASH' on a blue factory?"
            },
            "target": "(C) Interviewer and interviewee",
            "arguments": [
                "What is the relationship between the first two people appear in the video after the appearance of blue words 'TRUCK WASH' on a blue factory?\nA. (A) Colleagues\nB. (B) Husband and wife\nC. (C) Interviewer and interviewee\nD. (D) Brother and sister\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bd360>"
                    ]
                },
                18,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be1dfa06545968a2b0f35cd0aaa2738e3e3bbc9fb762382e3c88f010297a0cd4",
            "target_hash": "a2814335b6535657d0976553a7b6fa384aa6501648f15f284131a30ddbbd4b6c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 19,
            "doc": {
                "time_reference": "63:10-63:30",
                "video": "5dZ_lvDgevk",
                "answer": "(B) Around 1996",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Around 2006",
                    "(B) Around 1996",
                    "(C) Around 2001",
                    "(D) Around 1991"
                ],
                "type": "documentary",
                "question": "Around what year do the blue and red lines intersect?"
            },
            "target": "(B) Around 1996",
            "arguments": [
                "Around what year do the blue and red lines intersect?\nA. (A) Around 2006\nB. (B) Around 1996\nC. (C) Around 2001\nD. (D) Around 1991\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea0e0>"
                    ]
                },
                19,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "09290c391f30b980f2e043c41a95b84cffd0bc793cafbbed282d633168d48e26",
            "target_hash": "3ff7c02f1228b7f0933de01228e28999922c27be50e60be874972c85209aa650",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 20,
            "doc": {
                "time_reference": "76:11-76:43",
                "video": "5dZ_lvDgevk",
                "answer": "(A) People listening to a speaker on stage",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) People listening to a speaker on stage",
                    "(B) A mentor guiding a student",
                    "(C) Individuals collaborating on a project",
                    "(D) Characters engaging in a group discussion"
                ],
                "type": "documentary",
                "question": "How do the main characters interact with each other after the appearance of book 'Zurked' with a thumb down on the book?"
            },
            "target": "(A) People listening to a speaker on stage",
            "arguments": [
                "How do the main characters interact with each other after the appearance of book 'Zurked' with a thumb down on the book?\nA. (A) People listening to a speaker on stage\nB. (B) A mentor guiding a student\nC. (C) Individuals collaborating on a project\nD. (D) Characters engaging in a group discussion\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb2b0>"
                    ]
                },
                20,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "09aa9b7836d6525d00b790e6b2614f6645a7a504a29039acc271a7b6ded4a7b2",
            "target_hash": "da2a612c7eff753dcca88e43fc5a58a231c6a60cbd838b697628c876727a6bb2",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 21,
            "doc": {
                "time_reference": "80:05-80:56",
                "video": "5dZ_lvDgevk",
                "answer": "(D) Smart home scenes display",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Showcasing innovative working automation",
                    "(B) Demonstration of futuristic schools",
                    "(C) Exhibition of advanced labotoray technology",
                    "(D) Smart home scenes display"
                ],
                "type": "documentary",
                "question": "Based on the visual cues, what is on the scene before the scene focus on a water glass with white top and gray bottom with words 'Promotional Video' in the upper left corner of the video"
            },
            "target": "(D) Smart home scenes display",
            "arguments": [
                "Based on the visual cues, what is on the scene before the scene focus on a water glass with white top and gray bottom with words 'Promotional Video' in the upper left corner of the video\nA. (A) Showcasing innovative working automation\nB. (B) Demonstration of futuristic schools\nC. (C) Exhibition of advanced labotoray technology\nD. (D) Smart home scenes display\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea860>"
                    ]
                },
                21,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "21a327249f0a7721ab7c2e96e245bb66f79b4586679b3f1dd445a9a81962cfb6",
            "target_hash": "7467d9b7ca427df424ed2d48760d036f23847fe0d32220f9907c34bee3117ae8",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 22,
            "doc": {
                "time_reference": "86:31-86:31",
                "video": "5dZ_lvDgevk",
                "answer": "(D) Your News Wire",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Pope Benedict XVI",
                    "(B) Hillary Clinton",
                    "(C) Tell Me Now",
                    "(D) Your News Wire"
                ],
                "type": "documentary",
                "question": "Which account's Facebook page is displayed with 28402 likes?"
            },
            "target": "(D) Your News Wire",
            "arguments": [
                "Which account's Facebook page is displayed with 28402 likes?\nA. (A) Pope Benedict XVI\nB. (B) Hillary Clinton\nC. (C) Tell Me Now\nD. (D) Your News Wire\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebe50>"
                    ]
                },
                22,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a978d65bcda3704413cef6c77d64af63207ff412a6097c05b2b318a6ec97cbbf",
            "target_hash": "19dd6ac8752af163d8c0b808ef5ec697079a874742d4f12bd630cf66c129e0d9",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 23,
            "doc": {
                "time_reference": "32:08-32:19",
                "video": "5dZ_lvDgevk",
                "answer": "(B) 2",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 0",
                    "(B) 2",
                    "(C) 1",
                    "(D) 3"
                ],
                "type": "documentary",
                "question": "How many people are interviewed in the M.I.T. AI Lab for using AI to learn about cancer detection with mammogram data?"
            },
            "target": "(B) 2",
            "arguments": [
                "How many people are interviewed in the M.I.T. AI Lab for using AI to learn about cancer detection with mammogram data?\nA. (A) 0\nB. (B) 2\nC. (C) 1\nD. (D) 3\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dc00>"
                    ]
                },
                23,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2a1ad17eda3ab2ae7f2dd36ebd1e7e8cf03ef2a4a1b705c608cee190e59195c8",
            "target_hash": "4474ee930ab66e43cf7087501d2bde2bf2de61d92c49d0ee75219ed0f498d475",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 24,
            "doc": {
                "time_reference": "101:21-101:21",
                "video": "5dZ_lvDgevk",
                "answer": "(C) Arresting",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Ensuring safety",
                    "(B) Accompanying and showing courtesy",
                    "(C) Arresting",
                    "(D) Displaying friendliness"
                ],
                "type": "documentary",
                "question": "What goals or objectives are the man holding a woman's arm trying to achieve at 101:21?"
            },
            "target": "(C) Arresting",
            "arguments": [
                "What goals or objectives are the man holding a woman's arm trying to achieve at 101:21?\nA. (A) Ensuring safety\nB. (B) Accompanying and showing courtesy\nC. (C) Arresting\nD. (D) Displaying friendliness\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c19f0>"
                    ]
                },
                24,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2c009de86ba0acc05bed97733869aa8cced520cdc2484b15b5fe8c5c3d96c41d",
            "target_hash": "8eec06baf4a5263f905cef9ff91b44112b29dd0bd22839f9032ef8e868fc950a",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 25,
            "doc": {
                "time_reference": "103:57-103:58",
                "video": "5dZ_lvDgevk",
                "answer": "(C) Colleagues",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Family",
                    "(B) Classmates",
                    "(C) Colleagues",
                    "(D) Strangers"
                ],
                "type": "documentary",
                "question": "What is the most possible relationships among the people at 103:57?"
            },
            "target": "(C) Colleagues",
            "arguments": [
                "What is the most possible relationships among the people at 103:57?\nA. (A) Family\nB. (B) Classmates\nC. (C) Colleagues\nD. (D) Strangers\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1240>"
                    ]
                },
                25,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "04e8720fda357a91b187fab4f1e9ce360c38ee84e10ee111effce34737f100d1",
            "target_hash": "f7203c94559d0393fd796ca154cc8a6aab5ded2a59e3caa187601ce90ed69fa4",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 26,
            "doc": {
                "time_reference": "21:37-54:16",
                "video": "5dZ_lvDgevk",
                "answer": "(A) AI technology",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) AI technology",
                    "(B) Future cities",
                    "(C) Industrial innovation",
                    "(D) Automation and robotics"
                ],
                "type": "documentary",
                "question": "Based on the visual elements, can you identify the theme of this video?"
            },
            "target": "(A) AI technology",
            "arguments": [
                "Based on the visual elements, can you identify the theme of this video?\nA. (A) AI technology\nB. (B) Future cities\nC. (C) Industrial innovation\nD. (D) Automation and robotics\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0ca0>"
                    ]
                },
                26,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b863f1f9a1be2d34186049b73c3e4a204f0b64868e7d1bc3344b952de848215",
            "target_hash": "a30cc41ca293732cac02b3231787218463373903ccbad89b456498775089c2d3",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 27,
            "doc": {
                "time_reference": "00:52-00:52",
                "video": "GcRKREorGSc",
                "answer": "(D) The deer",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) The little lion",
                    "(B) The bat-eared fox",
                    "(C) The ostrich",
                    "(D) The deer"
                ],
                "type": "documentary",
                "question": "What is the animal in the first scene that appears after the opening cloud scene?"
            },
            "target": "(D) The deer",
            "arguments": [
                "What is the animal in the first scene that appears after the opening cloud scene?\nA. (A) The little lion\nB. (B) The bat-eared fox\nC. (C) The ostrich\nD. (D) The deer\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9900>"
                    ]
                },
                27,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28f8e3fe4773eb0099f9a613aa8cfbee60a42b1832b571a93ba4dfff1a666a7e",
            "target_hash": "fe753b4590cb0cf7de999f886ac1044fec32c1017ca8dba20537f8f10a249912",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 28,
            "doc": {
                "time_reference": "02:07-02:07",
                "video": "GcRKREorGSc",
                "answer": "(D) The ostriches",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) The deer",
                    "(B) The little lion",
                    "(C) The bat-eared fox",
                    "(D) The ostriches"
                ],
                "type": "documentary",
                "question": "What is the animal that appears in the picture after the first appearance of a lion?"
            },
            "target": "(D) The ostriches",
            "arguments": [
                "What is the animal that appears in the picture after the first appearance of a lion?\nA. (A) The deer\nB. (B) The little lion\nC. (C) The bat-eared fox\nD. (D) The ostriches\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eaad0>"
                    ]
                },
                28,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5c1f6181fe4ac4fb7f22721b799dfd41a5a5a49a2665c8464d5f1ab778454990",
            "target_hash": "84caa35272de7abdeee3370c9cd97c3708ffdbd188a7ec5dd97dca862743db23",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 29,
            "doc": {
                "time_reference": "05:50-06:18",
                "video": "GcRKREorGSc",
                "answer": "(A) They are sleeping",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) They are sleeping",
                    "(B) They are fighting",
                    "(C) They are mating",
                    "(D) They are hunting"
                ],
                "type": "documentary",
                "question": "What are the two deer seen in the first half of the video doing?"
            },
            "target": "(A) They are sleeping",
            "arguments": [
                "What are the two deer seen in the first half of the video doing?\nA. (A) They are sleeping\nB. (B) They are fighting\nC. (C) They are mating\nD. (D) They are hunting\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eaf80>"
                    ]
                },
                29,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4a18cba696832cce3a21e4372e54e52a7288d412020c0a2687b967713c8401fd",
            "target_hash": "7606b8627738eee6f6f25875ab8777a9fb994446086badca5811b2dce2bffa16",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 30,
            "doc": {
                "time_reference": "15:46-15:46",
                "video": "GcRKREorGSc",
                "answer": "(D) Tail",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Grass",
                    "(B) Another fox",
                    "(C) Hand",
                    "(D) Tail"
                ],
                "type": "documentary",
                "question": "What is a bat-eared fox licking in the grass in the middle of the picture?"
            },
            "target": "(D) Tail",
            "arguments": [
                "What is a bat-eared fox licking in the grass in the middle of the picture?\nA. (A) Grass\nB. (B) Another fox\nC. (C) Hand\nD. (D) Tail\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7ca90>"
                    ]
                },
                30,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "936fa2036391da918bb8d82d0a8e7edc594253ee6fddf5c591436c44628c5696",
            "target_hash": "8c45f71becf0adbb6eee3a3268c18d681648e2ed5c09ca31fbeab6aa56461dca",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 31,
            "doc": {
                "time_reference": "17:52-18:22",
                "video": "GcRKREorGSc",
                "answer": "(C) Three bat-eared foxes play with each other",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Three lions play with each other",
                    "(B) Three leopard foxes play with each other",
                    "(C) Three bat-eared foxes play with each other",
                    "(D) Three deers play with each other"
                ],
                "type": "documentary",
                "question": "What are the three animals that appear after two deer eat grass and play with their horns?"
            },
            "target": "(C) Three bat-eared foxes play with each other",
            "arguments": [
                "What are the three animals that appear after two deer eat grass and play with their horns?\nA. (A) Three lions play with each other\nB. (B) Three leopard foxes play with each other\nC. (C) Three bat-eared foxes play with each other\nD. (D) Three deers play with each other\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c760>"
                    ]
                },
                31,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "aecf2ac8e5ac0c0275006683c3ccaa598d1e10349ec773609a3fa6c0531f84a7",
            "target_hash": "b4d43a3dbc81527a212757b7ab22fee4499d5a1a3b5862f79dd26346e0ae4136",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 32,
            "doc": {
                "time_reference": "19:18-19:18",
                "video": "GcRKREorGSc",
                "answer": "(C) Golden",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black",
                    "(B) Pink",
                    "(C) Golden",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What color is the cloud layer captured for the second time in the video?"
            },
            "target": "(C) Golden",
            "arguments": [
                "What color is the cloud layer captured for the second time in the video?\nA. (A) Black\nB. (B) Pink\nC. (C) Golden\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1bd0>"
                    ]
                },
                32,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1b20670c16cad080b028d723828ebd2a8ea2a4069e625854f816ce06a5716252",
            "target_hash": "30e4673766fbe4d62784559eeff8a150d4176925e6c1bc40f62f7368bebf642e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 33,
            "doc": {
                "time_reference": "28:37-29:10",
                "video": "GcRKREorGSc",
                "answer": "(B) African goshawks fly in the sky",
                "question_type": [
                    "temporal grounding",
                    "event understanding"
                ],
                "candidates": [
                    "(A) African fish eagles fly in the sky",
                    "(B) African goshawks fly in the sky",
                    "(C) White-bellied sea eagles fly in the sky",
                    "(D) Namaqua doves fly in the sky"
                ],
                "type": "documentary",
                "question": "What happens from 28:37-29:10?"
            },
            "target": "(B) African goshawks fly in the sky",
            "arguments": [
                "What happens from 28:37-29:10?\nA. (A) African fish eagles fly in the sky\nB. (B) African goshawks fly in the sky\nC. (C) White-bellied sea eagles fly in the sky\nD. (D) Namaqua doves fly in the sky\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1270>"
                    ]
                },
                33,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bb1f03a68d3e851a036f68c3afb85f67927b1b515ebb7d9e884c2ba61625c9e9",
            "target_hash": "49c7e91e7f3c927f45d35432de7cd4ac33eb100adaee93fb7aca7a98743c867e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 34,
            "doc": {
                "time_reference": "31:38-31:38",
                "video": "GcRKREorGSc",
                "answer": "(C) Bateleur",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) African goshawks",
                    "(B) Namaqua doves",
                    "(C) Bateleur",
                    "(D) Helmeted guineafowl"
                ],
                "type": "documentary",
                "question": "What animal is one of the five animals that appears in the picture walking on the grass?"
            },
            "target": "(C) Bateleur",
            "arguments": [
                "What animal is one of the five animals that appears in the picture walking on the grass?\nA. (A) African goshawks\nB. (B) Namaqua doves\nC. (C) Bateleur\nD. (D) Helmeted guineafowl\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0e20>"
                    ]
                },
                34,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "062ae9520f6b24d4b80538f1aa9968401b49b6225573d9437ad6b3db6f68ab5b",
            "target_hash": "a9414980304a1ce46f3a39db5f52d573035ca44a18e8af29b570a6a6a85d9af2",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 35,
            "doc": {
                "time_reference": "31:54-31:54",
                "video": "GcRKREorGSc",
                "answer": "(A) Yellow",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Yellow",
                    "(B) Grey",
                    "(C) Black",
                    "(D) Green"
                ],
                "type": "documentary",
                "question": "What color is the grass around the lioness lying alone in the grass?"
            },
            "target": "(A) Yellow",
            "arguments": [
                "What color is the grass around the lioness lying alone in the grass?\nA. (A) Yellow\nB. (B) Grey\nC. (C) Black\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9060>"
                    ]
                },
                35,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "57ce4345679f9f2caf985d8ac2dff45c30e78f148ed631b56de97cb9cca49bb0",
            "target_hash": "17db5ef3256df7c9256c75ba0516a35208ef27948344ccc3af6325bf80ace62e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 36,
            "doc": {
                "time_reference": "32:19-32:35",
                "video": "GcRKREorGSc",
                "answer": "(D) Zebra eat the grass",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Ostrich drink water",
                    "(B) Deers eat the grass",
                    "(C) Giraffes eat the grass",
                    "(D) Zebra eat the grass"
                ],
                "type": "documentary",
                "question": "What happens after two lions lying on the grass enjoying sunshine, and then a bird standing on top of a rock?"
            },
            "target": "(D) Zebra eat the grass",
            "arguments": [
                "What happens after two lions lying on the grass enjoying sunshine, and then a bird standing on top of a rock?\nA. (A) Ostrich drink water\nB. (B) Deers eat the grass\nC. (C) Giraffes eat the grass\nD. (D) Zebra eat the grass\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea350>"
                    ]
                },
                36,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5c9b5ac79bb58a198c9598a8d2cd747765085f8a56598ca0c86664465ae993e8",
            "target_hash": "4ef815a247e494a0705a3e33c82a579e2f21dc5b8137c5068543f44c4f82c80e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 37,
            "doc": {
                "time_reference": "37:43-37:43",
                "video": "GcRKREorGSc",
                "answer": "(C) Greater kudu",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Bat-eared fox",
                    "(B) Bateleur",
                    "(C) Greater kudu",
                    "(D) Deer"
                ],
                "type": "documentary",
                "question": "What appears on the screen before five deers eating the grass and then two giraffes walking on the grass?"
            },
            "target": "(C) Greater kudu",
            "arguments": [
                "What appears on the screen before five deers eating the grass and then two giraffes walking on the grass?\nA. (A) Bat-eared fox\nB. (B) Bateleur\nC. (C) Greater kudu\nD. (D) Deer\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9b40>"
                    ]
                },
                37,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6976b913bdd4d89e950c284d5d29b2855f247f864346fa41dd6f32e00150ab62",
            "target_hash": "cdccd01012f08644f6eca438d59079c3f2359088b9ba76d983870cc785a5b6e3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 38,
            "doc": {
                "time_reference": "38:14-38:14",
                "video": "GcRKREorGSc",
                "answer": "(C) African fish eagle",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Helmeted guineafowl",
                    "(B) Namaqua doves",
                    "(C) African fish eagle",
                    "(D) Bateleur"
                ],
                "type": "documentary",
                "question": "What is the second other animal that appears after the great kudu?"
            },
            "target": "(C) African fish eagle",
            "arguments": [
                "What is the second other animal that appears after the great kudu?\nA. (A) Helmeted guineafowl\nB. (B) Namaqua doves\nC. (C) African fish eagle\nD. (D) Bateleur\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c100>"
                    ]
                },
                38,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a06e5fc47c6ec616587cfa82314b7de840e12c25c951eb9a8913748866d4647c",
            "target_hash": "01f9b1ab7ef2a6b41c3b5544b7a17c61f30d75a87149bc418b0291fbc99e3940",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 39,
            "doc": {
                "time_reference": "39:26-39:26",
                "video": "GcRKREorGSc",
                "answer": "(A) Hippo",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Hippo",
                    "(B) African elephant",
                    "(C) Crocodile",
                    "(D) African fish eagle"
                ],
                "type": "documentary",
                "question": "What appears in the water before the scene of a hippo eating grass?"
            },
            "target": "(A) Hippo",
            "arguments": [
                "What appears in the water before the scene of a hippo eating grass?\nA. (A) Hippo\nB. (B) African elephant\nC. (C) Crocodile\nD. (D) African fish eagle\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cd30>"
                    ]
                },
                39,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "03745f7d96672a8c18f2924d6c2232255a8a7324fa150ff8aea59bbb4242d490",
            "target_hash": "dff113febb5969a6ecebcccab01e73ecdd3758c2401bd23d0f0fc93ddc81d240",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 40,
            "doc": {
                "time_reference": "40:51-40:51",
                "video": "GcRKREorGSc",
                "answer": "(C) Lilac-breasted roller",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black-winged stilt",
                    "(B) Crimson-breasted shrike",
                    "(C) Lilac-breasted roller",
                    "(D) Bat-eared fox"
                ],
                "type": "documentary",
                "question": "What appears on the screen before the scene where a Nile monitor walks towards the right, making a hissing sound with its tongue, respective to the camera's view?"
            },
            "target": "(C) Lilac-breasted roller",
            "arguments": [
                "What appears on the screen before the scene where a Nile monitor walks towards the right, making a hissing sound with its tongue, respective to the camera's view?\nA. (A) Black-winged stilt\nB. (B) Crimson-breasted shrike\nC. (C) Lilac-breasted roller\nD. (D) Bat-eared fox\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c10c0>"
                    ]
                },
                40,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6ccec430f4af7f56b8877aefbd9adeaa9ba51ff1d745bd8a3537fc42a8b719f8",
            "target_hash": "3a8bbf557285b3db1f78b857800590157d0fcfbc3e43d8cb8f19a9cdccf5f8ff",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 41,
            "doc": {
                "time_reference": "41:02-41:02",
                "video": "GcRKREorGSc",
                "answer": "(D) Nile monitor",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Namaqua doves",
                    "(B) Helmeted guineafowl",
                    "(C) African fish eagle",
                    "(D) Nile monitor"
                ],
                "type": "documentary",
                "question": "What appears after a lilac-breasted roller shows up for the first time?"
            },
            "target": "(D) Nile monitor",
            "arguments": [
                "What appears after a lilac-breasted roller shows up for the first time?\nA. (A) Namaqua doves\nB. (B) Helmeted guineafowl\nC. (C) African fish eagle\nD. (D) Nile monitor\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1000>"
                    ]
                },
                41,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6587edad97c21f86d9388718310ed28a3ee7b42bf4cc1a2df7b03c45220e5699",
            "target_hash": "d8fa7f6be7d76ed8d47d873abce500eac6d68a68bf4e61fca2cf948af6e35376",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 42,
            "doc": {
                "time_reference": "43:00-43:27",
                "video": "GcRKREorGSc",
                "answer": "(C) African ground squirrel",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Tiger",
                    "(B) Bat-eared fox",
                    "(C) African ground squirrel",
                    "(D) African openbill"
                ],
                "type": "documentary",
                "question": "What animal licks its fur and then looks straight to the camera?"
            },
            "target": "(C) African ground squirrel",
            "arguments": [
                "What animal licks its fur and then looks straight to the camera?\nA. (A) Tiger\nB. (B) Bat-eared fox\nC. (C) African ground squirrel\nD. (D) African openbill\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c03d0>"
                    ]
                },
                42,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "78556105e00f75dcb6267b9d45711e5ec732e7945f4545b704d904c83367e571",
            "target_hash": "689f314f0d4d8c2d392efdddab526fa9c51f55619fbcce3515bbf6fc7d997b01",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 43,
            "doc": {
                "time_reference": "53:26-53:52",
                "video": "GcRKREorGSc",
                "answer": "(D) Zebra and deers run",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Leopards and deers run",
                    "(B) Lions and zebra run",
                    "(C) Lions and deers run",
                    "(D) Zebra and deers run"
                ],
                "type": "documentary",
                "question": "What happens after the series of close-up shots of lions?"
            },
            "target": "(D) Zebra and deers run",
            "arguments": [
                "What happens after the series of close-up shots of lions?\nA. (A) Leopards and deers run\nB. (B) Lions and zebra run\nC. (C) Lions and deers run\nD. (D) Zebra and deers run\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e86a0>"
                    ]
                },
                43,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d72014604a2f62a0a52b0d94221b3ce6bc3c5394655e21e70b7b541c98a156ed",
            "target_hash": "1a4b109e3be03fc9572bae66b10b73bd14c6844e4f012ca610c843b51c5d5c22",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 44,
            "doc": {
                "time_reference": "00:00-44:24",
                "video": "xi6r3hZe5Tg",
                "answer": "(C) National geographic",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Natural scenery",
                    "(B) History",
                    "(C) National geographic",
                    "(D) Humanities"
                ],
                "type": "documentary",
                "question": "What type of video is this?"
            },
            "target": "(C) National geographic",
            "arguments": [
                "What type of video is this?\nA. (A) Natural scenery\nB. (B) History\nC. (C) National geographic\nD. (D) Humanities\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8d60>"
                    ]
                },
                44,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8c20408f8c1cf0483c84ca4f8d94c5bc8f1e85b2d24bf87b53053febc5fd6f75",
            "target_hash": "097d4da7126d293aea5741ef5f85dce4c2b0e9b71d87107c0e0dbeb16865adcb",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 45,
            "doc": {
                "time_reference": "30:00-30:10",
                "video": "xi6r3hZe5Tg",
                "answer": "(B) Black",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Golden",
                    "(B) Black",
                    "(C) Red",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What color watch do respondents wearing blue short sleeves and glasses wear?"
            },
            "target": "(B) Black",
            "arguments": [
                "What color watch do respondents wearing blue short sleeves and glasses wear?\nA. (A) Golden\nB. (B) Black\nC. (C) Red\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebd30>"
                    ]
                },
                45,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6223f1686446b3a2ebe273560dc37163bfd22278342ef6e12429b2fb2887e38b",
            "target_hash": "bb359097fe322499e4d46a7bd79f43f2df73e9efbfad5cdf87cf32d127d6a8d5",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 46,
            "doc": {
                "time_reference": "34:00-34:10",
                "video": "xi6r3hZe5Tg",
                "answer": "(B) Orange",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black",
                    "(B) Orange",
                    "(C) Blue",
                    "(D) Red"
                ],
                "type": "documentary",
                "question": "What color hats do children wearing yellow short sleeves wear?"
            },
            "target": "(B) Orange",
            "arguments": [
                "What color hats do children wearing yellow short sleeves wear?\nA. (A) Black\nB. (B) Orange\nC. (C) Blue\nD. (D) Red\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c8b0>"
                    ]
                },
                46,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f67223c82e9b99b993103aba7baa88c4191a20b683a58ab556a4e276b980e8c4",
            "target_hash": "1d5e0e9af1a6546cb8118be7370935727ec3fc75f5a4a1739b481b79605c418d",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 47,
            "doc": {
                "time_reference": "34:00-35:00",
                "video": "xi6r3hZe5Tg",
                "answer": "(D) Teacher-student relationship",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Friendship",
                    "(B) Stranger relationship",
                    "(C) Mother-child relationship",
                    "(D) Teacher-student relationship"
                ],
                "type": "documentary",
                "question": "What is the relationship between the lady in blue clothes and the kid in yellow clothes?"
            },
            "target": "(D) Teacher-student relationship",
            "arguments": [
                "What is the relationship between the lady in blue clothes and the kid in yellow clothes?\nA. (A) Friendship\nB. (B) Stranger relationship\nC. (C) Mother-child relationship\nD. (D) Teacher-student relationship\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d570>"
                    ]
                },
                47,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d9065ca229319975bc848bc915642ddab3615e342486f6b91a423a95f05b540e",
            "target_hash": "d5757cb31231187fad53983f39b76b3ce907cc09cb7d101ede68290e600cd54c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 48,
            "doc": {
                "time_reference": "41:07-41:07",
                "video": "xi6r3hZe5Tg",
                "answer": "(A) Telescope",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Telescope",
                    "(B) Necklace",
                    "(C) Watercup",
                    "(D) Camera"
                ],
                "type": "documentary",
                "question": "What is hanging around the neck of the girl observing birds in the forest in the video?"
            },
            "target": "(A) Telescope",
            "arguments": [
                "What is hanging around the neck of the girl observing birds in the forest in the video?\nA. (A) Telescope\nB. (B) Necklace\nC. (C) Watercup\nD. (D) Camera\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0940>"
                    ]
                },
                48,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "963c2d84e4a91f540d53fe21a81bed00c3d370cecd108171e2c1fb69ca7e443d",
            "target_hash": "2b313197af7b720ea50fce97b02f0c73e0d505d63df8185f8699eddbd4439ba4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 49,
            "doc": {
                "time_reference": "42:00-42:00",
                "video": "xi6r3hZe5Tg",
                "answer": "(D) Chicken",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Horse",
                    "(B) Cow",
                    "(C) Bird",
                    "(D) Chicken"
                ],
                "type": "documentary",
                "question": "What animal is this at 42:00?"
            },
            "target": "(D) Chicken",
            "arguments": [
                "What animal is this at 42:00?\nA. (A) Horse\nB. (B) Cow\nC. (C) Bird\nD. (D) Chicken\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0ac0>"
                    ]
                },
                49,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e150b3243fc94d2c96cf7427b5e5c124f86a04cdb2cabab3d4f72072089c8bb3",
            "target_hash": "18f9b48aacb78fa17509426752cf9bc7212fb64a88f3f15c18fbf3cea7f37003",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 50,
            "doc": {
                "time_reference": "42:08-42:08",
                "video": "xi6r3hZe5Tg",
                "answer": "(B) Dragonfly",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Bird",
                    "(B) Dragonfly",
                    "(C) Butterfly",
                    "(D) Lizard"
                ],
                "type": "documentary",
                "question": "What animal appears after the interview with Karen Sim and Jeffrey Chong?"
            },
            "target": "(B) Dragonfly",
            "arguments": [
                "What animal appears after the interview with Karen Sim and Jeffrey Chong?\nA. (A) Bird\nB. (B) Dragonfly\nC. (C) Butterfly\nD. (D) Lizard\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e88b0>"
                    ]
                },
                50,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "473c9119d107e27730a03ee406f26cb60a5139cb0c120d442744ae4b3560764e",
            "target_hash": "0bacebd1aec1bab0876068086bcba843bd957a8108c4687e0a45fc4887804e19",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 51,
            "doc": {
                "time_reference": "42:50-42:50",
                "video": "xi6r3hZe5Tg",
                "answer": "(C) Lizard",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Whale",
                    "(B) Crocodile",
                    "(C) Lizard",
                    "(D) Hippo"
                ],
                "type": "documentary",
                "question": "What animal is shown in the scene that follows the scene Karen Sim submits a sighting photo and earns a badge on the phone?"
            },
            "target": "(C) Lizard",
            "arguments": [
                "What animal is shown in the scene that follows the scene Karen Sim submits a sighting photo and earns a badge on the phone?\nA. (A) Whale\nB. (B) Crocodile\nC. (C) Lizard\nD. (D) Hippo\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e94e0>"
                    ]
                },
                51,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be356cc1ffd8c64c04dd505281f932624c2525b94513a4523c872fd26d8baa2b",
            "target_hash": "40ba8802bc6dcfac268c1bda117e4579fee1001324f754b8f9fd55de2f4bcd17",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 52,
            "doc": {
                "time_reference": "17:12-17:30",
                "video": "xi6r3hZe5Tg",
                "answer": "(B) Dark blue",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black",
                    "(B) Dark blue",
                    "(C) Red",
                    "(D) Yellow"
                ],
                "type": "documentary",
                "question": "What color is the device that Mark Wong wears to monitor his health?"
            },
            "target": "(B) Dark blue",
            "arguments": [
                "What color is the device that Mark Wong wears to monitor his health?\nA. (A) Black\nB. (B) Dark blue\nC. (C) Red\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e91b0>"
                    ]
                },
                52,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "303bccc41c739a23e5bfd32b856a65e58d4e391fa42675f71de4f91834b5a58a",
            "target_hash": "596d37b8332506f81fe2e6705fa6cbe9b4a2e614e339bc345b2972a7baa77d21",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 53,
            "doc": {
                "time_reference": "20:33-20:33",
                "video": "xi6r3hZe5Tg",
                "answer": "(A) Carp",
                "question_type": [
                    "entity recognition",
                    "event understanding",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Carp",
                    "(B) Jellyfish",
                    "(C) Frog",
                    "(D) Squid"
                ],
                "type": "documentary",
                "question": "When the video discusses water reservoir, which animal lives in water appear?"
            },
            "target": "(A) Carp",
            "arguments": [
                "When the video discusses water reservoir, which animal lives in water appear?\nA. (A) Carp\nB. (B) Jellyfish\nC. (C) Frog\nD. (D) Squid\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebbb0>"
                    ]
                },
                53,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a26ab69c925e2d25d11eb4dd8e934e49a8be6b419192821700bf148e07b3dd82",
            "target_hash": "43da23c68a6f78d5b47a4ed7ed753353e66a849b139bbba23bbb5861fb0841d7",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 54,
            "doc": {
                "time_reference": "00:43-00:43",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Brake caliper",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Wheel",
                    "(B) Bearing",
                    "(C) Engine",
                    "(D) Brake caliper"
                ],
                "type": "documentary",
                "question": "What is the yellow semi-circular tool used by black clad engineers in cars?"
            },
            "target": "(D) Brake caliper",
            "arguments": [
                "What is the yellow semi-circular tool used by black clad engineers in cars?\nA. (A) Wheel\nB. (B) Bearing\nC. (C) Engine\nD. (D) Brake caliper\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cc10>"
                    ]
                },
                54,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c5f3f3287c9249872a19006eb8c7519f9f18cd96d42762010f5860b01143cbeb",
            "target_hash": "a234e3df166056ab0b70046eac542ce3fca1adbcf9aee5c753506f082738b6fd",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 55,
            "doc": {
                "time_reference": "01:20-01:20",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) Maybach",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Maybach",
                    "(B) AMG GT",
                    "(C) C class",
                    "(D) E class"
                ],
                "type": "documentary",
                "question": "What kind of Mercedes-Benz car is this black one?"
            },
            "target": "(A) Maybach",
            "arguments": [
                "What kind of Mercedes-Benz car is this black one?\nA. (A) Maybach\nB. (B) AMG GT\nC. (C) C class\nD. (D) E class\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d7e0>"
                    ]
                },
                55,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cd697f8b89bc1bcbf954895d44fcdbc7576bd2e8723302a0f26450ad6e88030d",
            "target_hash": "7e9c86109a7badd5020f06bd34f76b6fb2a5e39a5aa58a48bf5758564bf661ce",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 56,
            "doc": {
                "time_reference": "04:45-05:25",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) They replace the exhaust pipe of the car",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) They replace the exhaust pipe of the car",
                    "(B) They replace the tail lights of the car",
                    "(C) They change tires on cars",
                    "(D) They replace the brake pads on the car"
                ],
                "type": "documentary",
                "question": "What happens from 04:45-05:25?"
            },
            "target": "(A) They replace the exhaust pipe of the car",
            "arguments": [
                "What happens from 04:45-05:25?\nA. (A) They replace the exhaust pipe of the car\nB. (B) They replace the tail lights of the car\nC. (C) They change tires on cars\nD. (D) They replace the brake pads on the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1750>"
                    ]
                },
                56,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bbd0d586e4532290e85fbb4b0646205535580de0b018580dd3e6287dbcb39cba",
            "target_hash": "90f21c1e7add237cace46a53f935b5c2bf3710e390ebca2a5a3dde30dcad812e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 57,
            "doc": {
                "time_reference": "10:20-11:00",
                "video": "Z4HGQL_McDQ",
                "answer": "(C) He is replacing the brake pads on the car",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) He is replacing the exhaust pipe on his car",
                    "(B) He is changing the tires on the car",
                    "(C) He is replacing the brake pads on the car",
                    "(D) He is replacing the wheels on the car"
                ],
                "type": "documentary",
                "question": "What is Samir Boulahya doing when his name first appears?"
            },
            "target": "(C) He is replacing the brake pads on the car",
            "arguments": [
                "What is Samir Boulahya doing when his name first appears?\nA. (A) He is replacing the exhaust pipe on his car\nB. (B) He is changing the tires on the car\nC. (C) He is replacing the brake pads on the car\nD. (D) He is replacing the wheels on the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c07f0>"
                    ]
                },
                57,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cee0e721fb860087affd0815e3ebef55eb38e40d0745f148635651f8b989d45a",
            "target_hash": "0a042e0e3ed064750d1ac3d25e49b577d5bc83569f794fda61c87763c4e5bbc5",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 58,
            "doc": {
                "time_reference": "15:39-15:39",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Green",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black",
                    "(B) Red",
                    "(C) Blue",
                    "(D) Green"
                ],
                "type": "documentary",
                "question": "What color are the gloves when cleaning parts?"
            },
            "target": "(D) Green",
            "arguments": [
                "What color are the gloves when cleaning parts?\nA. (A) Black\nB. (B) Red\nC. (C) Blue\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8ca0>"
                    ]
                },
                58,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "92d2bc316d4c077d3d7f954bbc0e83d4e5782e3f47cd6e581ed5e94dd70cab28",
            "target_hash": "1467faa1684c871eccb61d0fe03dff196c5c3cb3e35d92428a306428f10f86fc",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 59,
            "doc": {
                "time_reference": "16:00-16:00",
                "video": "Z4HGQL_McDQ",
                "answer": "(B) Molds",
                "question_type": [
                    "event understanding",
                    "reasoning"
                ],
                "candidates": [
                    "(A) Adhesive tape",
                    "(B) Molds",
                    "(C) Boxs",
                    "(D) Tires"
                ],
                "type": "documentary",
                "question": "What is the item placed on the shelf after cleaning?"
            },
            "target": "(B) Molds",
            "arguments": [
                "What is the item placed on the shelf after cleaning?\nA. (A) Adhesive tape\nB. (B) Molds\nC. (C) Boxs\nD. (D) Tires\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9810>"
                    ]
                },
                59,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "aa390622e3a967780b887ea36a26e578ba31d4e309dc847c1b5f1e3965b677bd",
            "target_hash": "ca03c5d55002557ef92b4d10648bb187e1d552e6072f22629c7c07f702ae1b5d",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 60,
            "doc": {
                "time_reference": "33:00-33:30",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) He is checking the vehicle chassis",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) He is checking the vehicle chassis",
                    "(B) He is checking the vehicle interior",
                    "(C) He is checking the vehicle windows",
                    "(D) He is checking the vehicle headlights"
                ],
                "type": "documentary",
                "question": "What does a man wearing a black suit, black glasses, and a beard do?"
            },
            "target": "(A) He is checking the vehicle chassis",
            "arguments": [
                "What does a man wearing a black suit, black glasses, and a beard do?\nA. (A) He is checking the vehicle chassis\nB. (B) He is checking the vehicle interior\nC. (C) He is checking the vehicle windows\nD. (D) He is checking the vehicle headlights\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea470>"
                    ]
                },
                60,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2fddbc29b4ac2246f3ae428d362c6817f252c14857922f213b56c396967f0cd0",
            "target_hash": "58c1a150e28840b79834f2eb902ec7215d1150534f5b835e8c6402344660b60a",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 61,
            "doc": {
                "time_reference": "35:00-36:00",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Stripping the seats",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Cleaning the seats",
                    "(B) Restoring the seats",
                    "(C) Adjusting the seat",
                    "(D) Stripping the seats"
                ],
                "type": "documentary",
                "question": "What job is the man in red responsible for?"
            },
            "target": "(D) Stripping the seats",
            "arguments": [
                "What job is the man in red responsible for?\nA. (A) Cleaning the seats\nB. (B) Restoring the seats\nC. (C) Adjusting the seat\nD. (D) Stripping the seats\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb550>"
                    ]
                },
                61,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5eb159795b6c5a12de702700cd255e65cdddb94164fb7ccecd23ea49839d6547",
            "target_hash": "dd92aa752d12c172bb7359ec6f22a2dc57a17b5789bda4b2728975bad6046a39",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 62,
            "doc": {
                "time_reference": "36:55-36:55",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) Scissors",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Scissors",
                    "(B) Knife",
                    "(C) Pen",
                    "(D) Ruler"
                ],
                "type": "documentary",
                "question": "What tools does a woman wearing black glasses and silver earrings first use to work?"
            },
            "target": "(A) Scissors",
            "arguments": [
                "What tools does a woman wearing black glasses and silver earrings first use to work?\nA. (A) Scissors\nB. (B) Knife\nC. (C) Pen\nD. (D) Ruler\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cf70>"
                    ]
                },
                62,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f0ead3744b550768f9ca86be6319cb28b34e68742b74112aecd0658bf2e43cf8",
            "target_hash": "1e2678b0bd2ddf092006e309d0efa841ad7e99ff615c0ee578bfed35eb91ed99",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 63,
            "doc": {
                "time_reference": "40:00-40:50",
                "video": "Z4HGQL_McDQ",
                "answer": "(B) Assemble the dashboard",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Assemble tires",
                    "(B) Assemble the dashboard",
                    "(C) Assemble skylight",
                    "(D) Assemble the seat"
                ],
                "type": "documentary",
                "question": "What is this man doing from 40:00-40:50?"
            },
            "target": "(B) Assemble the dashboard",
            "arguments": [
                "What is this man doing from 40:00-40:50?\nA. (A) Assemble tires\nB. (B) Assemble the dashboard\nC. (C) Assemble skylight\nD. (D) Assemble the seat\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cd00>"
                    ]
                },
                63,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "79f4add5ca30e6f92fa15ee4c8af896463af2d551e5e253d396bf5352dfe3975",
            "target_hash": "bb61c49728e5e10ff47cebbda85dcbb50a3a38e4baf512a84727c136fd0cac5e",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 64,
            "doc": {
                "time_reference": "45:16-45:16",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Blue",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Purple",
                    "(B) Black",
                    "(C) Red",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What is the main color of the car interior?"
            },
            "target": "(D) Blue",
            "arguments": [
                "What is the main color of the car interior?\nA. (A) Purple\nB. (B) Black\nC. (C) Red\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1450>"
                    ]
                },
                64,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9fe13faf9e11b92a7a6f1ddad86b38d29752da87f4a944c84e4b07068c475e1a",
            "target_hash": "2af8475f8d12112df3b1487762f27a4f3153636b07666dd9be3936b9d3a38176",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 65,
            "doc": {
                "time_reference": "47:30-48:00",
                "video": "Z4HGQL_McDQ",
                "answer": "(C) He drives back to the factory",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) He drives in circles",
                    "(B) He drives uphill",
                    "(C) He drives back to the factory",
                    "(D) He drives down the street"
                ],
                "type": "documentary",
                "question": "How does Klaus Pachall drive after being interviewed?"
            },
            "target": "(C) He drives back to the factory",
            "arguments": [
                "How does Klaus Pachall drive after being interviewed?\nA. (A) He drives in circles\nB. (B) He drives uphill\nC. (C) He drives back to the factory\nD. (D) He drives down the street\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c16c0>"
                    ]
                },
                65,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f5fe7f0d1f198f0223d3f13674a53e98ba93bd9504cae5c9ac085610ff3622ee",
            "target_hash": "575c2c5c5bf1a90b940f952998d24461d826b97442ec07a9903b73c82ccc0018",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 66,
            "doc": {
                "time_reference": "48:00-48:50",
                "video": "Z4HGQL_McDQ",
                "answer": "(C) He brushes the car",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) He changes the interior",
                    "(B) He changes the brake pads",
                    "(C) He brushes the car",
                    "(D) He changes the tire"
                ],
                "type": "documentary",
                "question": "What does the man do after introducing a ton of remover liquid stored in white plastic buckets on a shelf?"
            },
            "target": "(C) He brushes the car",
            "arguments": [
                "What does the man do after introducing a ton of remover liquid stored in white plastic buckets on a shelf?\nA. (A) He changes the interior\nB. (B) He changes the brake pads\nC. (C) He brushes the car\nD. (D) He changes the tire\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e84f0>"
                    ]
                },
                66,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "77ef2c8c60400e08f08c858dd85bdbd042d13844c0cb5583f9b356f22d573bef",
            "target_hash": "0d0136de31042a80c3b4da3867813194bf2d77bdf7157607f79c0963b772be0e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 67,
            "doc": {
                "time_reference": "49:00-49:05",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) The mechanics fixes the wheel with a rope",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) The mechanics drives back to the factory",
                    "(B) The mechanics changes the tire",
                    "(C) The mechanics brushes the car",
                    "(D) The mechanics fixes the wheel with a rope"
                ],
                "type": "documentary",
                "question": "What is the second last event that happens in the video?"
            },
            "target": "(D) The mechanics fixes the wheel with a rope",
            "arguments": [
                "What is the second last event that happens in the video?\nA. (A) The mechanics drives back to the factory\nB. (B) The mechanics changes the tire\nC. (C) The mechanics brushes the car\nD. (D) The mechanics fixes the wheel with a rope\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9ba0>"
                    ]
                },
                67,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8b10fe71b0d46eec7a876f1b661c2a94c63d791928aff64a5c88f1aaab209036",
            "target_hash": "3e7ed02845327b6fec6b3640bf5dad8b30a19e49180d98815e7b722c24a46815",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 68,
            "doc": {
                "time_reference": "04:32-04:32",
                "video": "Z4HGQL_McDQ",
                "answer": "(B) 240",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 24",
                    "(B) 240",
                    "(C) 12",
                    "(D) 120"
                ],
                "type": "documentary",
                "question": "What is the serial number of the car that Samir Boulahya works on?"
            },
            "target": "(B) 240",
            "arguments": [
                "What is the serial number of the car that Samir Boulahya works on?\nA. (A) 24\nB. (B) 240\nC. (C) 12\nD. (D) 120\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eab00>"
                    ]
                },
                68,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a0fb1dcff4363e66cba324789f267a4007c5fa51c5cca8b20824352315c4c16e",
            "target_hash": "b1b1151e8b0f296beddfb21de10f4bde20bfeb4608d57cc8c03b28944cc62068",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 69,
            "doc": {
                "time_reference": "04:47-04:47",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) Blue",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Blue",
                    "(B) Black",
                    "(C) Red",
                    "(D) Purple"
                ],
                "type": "documentary",
                "question": "What color gloves does the mechanic, who removes the standard exhaust of a car, wear?"
            },
            "target": "(A) Blue",
            "arguments": [
                "What color gloves does the mechanic, who removes the standard exhaust of a car, wear?\nA. (A) Blue\nB. (B) Black\nC. (C) Red\nD. (D) Purple\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9db0>"
                    ]
                },
                69,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2aff2b46cd5293b88016ab1e5c1592463ec3acae823dbaef0c98425edd482d74",
            "target_hash": "d1c056af5b817d5e50d5a991afaee433ec04e7c15ec4caa84c4c00621c7cb2e9",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 70,
            "doc": {
                "time_reference": "05:23-05:23",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) Silver gray",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Silver gray",
                    "(B) Black",
                    "(C) White",
                    "(D) Red"
                ],
                "type": "documentary",
                "question": "What color is the car that mechanics are changing from the standard exhaust to the valve-controlled exhaust?"
            },
            "target": "(A) Silver gray",
            "arguments": [
                "What color is the car that mechanics are changing from the standard exhaust to the valve-controlled exhaust?\nA. (A) Silver gray\nB. (B) Black\nC. (C) White\nD. (D) Red\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cfd0>"
                    ]
                },
                70,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f2d27e8c98103ac58fcf7d1d6b0b9f72d773d7ad37f14642660cd2ddf73ae915",
            "target_hash": "1354d940b0eba3d3fd6c7328e4901192f8c1ed99add81b86b39d826f374e2ddd",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 71,
            "doc": {
                "time_reference": "06:01-06:01",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Laugh",
                "question_type": [
                    "entity recognition",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Numb",
                    "(B) Cry",
                    "(C) Angry",
                    "(D) Laugh"
                ],
                "type": "documentary",
                "question": "What's this man's expression like at 06:01?"
            },
            "target": "(D) Laugh",
            "arguments": [
                "What's this man's expression like at 06:01?\nA. (A) Numb\nB. (B) Cry\nC. (C) Angry\nD. (D) Laugh\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dc60>"
                    ]
                },
                71,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6bd445b197d3bd713caa5557e468e592a79f9f0b59c373e5cd1887c7b6e30d92",
            "target_hash": "9cb774f75c827a3f01dd39fbcf60653eb59db91aae9968df0d3237f55cafa90f",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 72,
            "doc": {
                "time_reference": "00:00-49:19",
                "video": "Z4HGQL_McDQ",
                "answer": "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
                "question_type": [
                    "summarization"
                ],
                "candidates": [
                    "(A) Introducing BRABUS's modification of BMW cars. This video records the process of BRABUS's modification of Mercedes Benz E Class",
                    "(B) Introducing BRABUS's modification of Audi cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
                    "(C) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz E Class",
                    "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class"
                ],
                "type": "documentary",
                "question": "What does this documentary mainly introduce?"
            },
            "target": "(D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class",
            "arguments": [
                "What does this documentary mainly introduce?\nA. (A) Introducing BRABUS's modification of BMW cars. This video records the process of BRABUS's modification of Mercedes Benz E Class\nB. (B) Introducing BRABUS's modification of Audi cars. This video records the process of BRABUS's modification of Mercedes Benz G Class\nC. (C) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz E Class\nD. (D) Introducing BRABUS's modification of Mercedes-Benz cars. This video records the process of BRABUS's modification of Mercedes Benz G Class\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0bb0>"
                    ]
                },
                72,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b66ba1555bf1a308d26ed8012d327aacea701f4a3ad3d57d8a789420e8ab0ba",
            "target_hash": "14c724d8aab35925c323e1f828a8db88ab828f7dc22e8a5367fd39bc8cabe472",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 73,
            "doc": {
                "time_reference": "06:09-06:09",
                "video": "Z4HGQL_McDQ",
                "answer": "(A) White",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) White",
                    "(B) Red",
                    "(C) Black",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What color is the steering wheel?"
            },
            "target": "(A) White",
            "arguments": [
                "What color is the steering wheel?\nA. (A) White\nB. (B) Red\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1720>"
                    ]
                },
                73,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "43477cc61079486f5efb6678e670f2eee2be737ca58c30639eddbc95761e5980",
            "target_hash": "671e4e48ceab719b952bd0c36eb44f6ecce11c1ff999800afe792a8f495c065b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 74,
            "doc": {
                "time_reference": "00:40-00:45",
                "video": "aJI8XTa_DII",
                "answer": "(B) Vladimir Putin",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Joe Biden",
                    "(B) Vladimir Putin",
                    "(C) Two soldiers",
                    "(D) Donald John Trump"
                ],
                "type": "documentary",
                "question": "Who first comes out of the golden door?"
            },
            "target": "(B) Vladimir Putin",
            "arguments": [
                "Who first comes out of the golden door?\nA. (A) Joe Biden\nB. (B) Vladimir Putin\nC. (C) Two soldiers\nD. (D) Donald John Trump\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9300>"
                    ]
                },
                74,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "65c7542e1d339bc192802ede6cc8c042fe91c64729e7dfd311a1fc67a290d927",
            "target_hash": "bd0b0351d85b23d2ddb833e8a6311d3baaf3dc7a836492736131098f3ecdc8db",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 75,
            "doc": {
                "time_reference": "00:53-01:26",
                "video": "aJI8XTa_DII",
                "answer": "(A) 5",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 5",
                    "(B) 1",
                    "(C) 9",
                    "(D) 4"
                ],
                "type": "documentary",
                "question": "How many flags of the Russian Federation are there on the stage when Putin delivers the first speech?"
            },
            "target": "(A) 5",
            "arguments": [
                "How many flags of the Russian Federation are there on the stage when Putin delivers the first speech?\nA. (A) 5\nB. (B) 1\nC. (C) 9\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9f00>"
                    ]
                },
                75,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6d5c4cbf07e8bb997702a32c8953747f5cb8dae99fd78d3c6032f8b7a4eee88d",
            "target_hash": "c99ecb129251ed73da1079670412a26a12d6e8986f0acfc969979db6c942bbf4",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 76,
            "doc": {
                "time_reference": "01:47-01:53",
                "video": "aJI8XTa_DII",
                "answer": "(C) Wine red",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Blue",
                    "(B) Black",
                    "(C) Wine red",
                    "(D) White"
                ],
                "type": "documentary",
                "question": "What color is Putin's tie between the interview with Antony Blinkoen and interview with Marie Yovanovitch?"
            },
            "target": "(C) Wine red",
            "arguments": [
                "What color is Putin's tie between the interview with Antony Blinkoen and interview with Marie Yovanovitch?\nA. (A) Blue\nB. (B) Black\nC. (C) Wine red\nD. (D) White\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eae60>"
                    ]
                },
                76,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "38822fd9c9d78986301d122ba53e46df6f8494a66bfb6e156ec6439954627be9",
            "target_hash": "ffa099135caf533cd031d964ee37ea699c80a2916f955d8f1c9d2de39bd3064a",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 77,
            "doc": {
                "time_reference": "07:57-08:01",
                "video": "aJI8XTa_DII",
                "answer": "(D) They stood up",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) They didn't do anything",
                    "(B) They applauded",
                    "(C) They squatted down",
                    "(D) They stood up"
                ],
                "type": "documentary",
                "question": "What did the audience do first after the soldiers wearing red hats shouted?"
            },
            "target": "(D) They stood up",
            "arguments": [
                "What did the audience do first after the soldiers wearing red hats shouted?\nA. (A) They didn't do anything\nB. (B) They applauded\nC. (C) They squatted down\nD. (D) They stood up\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eac50>"
                    ]
                },
                77,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "29921a7601f3dc1054df074800faa2f09c84c4da3abca3e311d8c2edb2f17c1c",
            "target_hash": "9a361a152b602c256415aaef391d091e306b909f256b5bde243588a81b301551",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 78,
            "doc": {
                "time_reference": "09:17-09:22",
                "video": "aJI8XTa_DII",
                "answer": "(D) Heather Conley",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Marie Yovanovitch",
                    "(B) John Bolton",
                    "(C) Fiona Hill",
                    "(D) Heather Conley"
                ],
                "type": "documentary",
                "question": "Who is the lady in green who is being interviewed?"
            },
            "target": "(D) Heather Conley",
            "arguments": [
                "Who is the lady in green who is being interviewed?\nA. (A) Marie Yovanovitch\nB. (B) John Bolton\nC. (C) Fiona Hill\nD. (D) Heather Conley\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d330>"
                    ]
                },
                78,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "789651453a08aaf50ed27f28de9603672a207f4c8e79cdb0f95c17061cc23d35",
            "target_hash": "7733490d2b26030bd856311a51368c6833a98f9b892fcdf0293e78dfe2a77003",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 79,
            "doc": {
                "time_reference": "12:27-12:33",
                "video": "aJI8XTa_DII",
                "answer": "(B) Flag of the United States of America",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Flag of the Russian Federation",
                    "(B) Flag of the United States of America",
                    "(C) A piece of white cloth",
                    "(D) A piece of black cloth"
                ],
                "type": "documentary",
                "question": "What did the man use to cover the sculpture's face?"
            },
            "target": "(B) Flag of the United States of America",
            "arguments": [
                "What did the man use to cover the sculpture's face?\nA. (A) Flag of the Russian Federation\nB. (B) Flag of the United States of America\nC. (C) A piece of white cloth\nD. (D) A piece of black cloth\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dfc0>"
                    ]
                },
                79,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f99a7135d65a857036a71288dd9d9dee514b48dacfc81f2a59c897db94ba9c74",
            "target_hash": "8d76b8ac20a38f056faeee0820ea8cd61b68103a3e508c367fa7cbb8bebb4f63",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 80,
            "doc": {
                "time_reference": "00:55-01:08",
                "video": "aJI8XTa_DII",
                "answer": "(C) 2",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 4",
                    "(B) 3",
                    "(C) 2",
                    "(D) 1"
                ],
                "type": "documentary",
                "question": "When Putin spoke for the first time in the video, how many microphones were on the stage?"
            },
            "target": "(C) 2",
            "arguments": [
                "When Putin spoke for the first time in the video, how many microphones were on the stage?\nA. (A) 4\nB. (B) 3\nC. (C) 2\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c08e0>"
                    ]
                },
                80,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6b6991ada243477ab4f8a8202cd0684720854a88e466aa36f79c829b3270f2a0",
            "target_hash": "af0f32bb40559440c5515f57dfbbfb2f51ecb56b390b4bcf1dddb14b635b6eab",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 81,
            "doc": {
                "time_reference": "01:00-01:05",
                "video": "aJI8XTa_DII",
                "answer": "(C) 1",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 2",
                    "(B) 4",
                    "(C) 1",
                    "(D) 0"
                ],
                "type": "documentary",
                "question": "During Putin's first speech on stage in the video, how many women appeared when the camera panned offstage?"
            },
            "target": "(C) 1",
            "arguments": [
                "During Putin's first speech on stage in the video, how many women appeared when the camera panned offstage?\nA. (A) 2\nB. (B) 4\nC. (C) 1\nD. (D) 0\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bcca0>"
                    ]
                },
                81,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4f0104c2f254fa8c28c120d8294269a2c2e56fa5cd8d4ae1820b1a1abdd5bb2e",
            "target_hash": "b0173dff8c7ec3413537675d4593b8d745fd673570d4d180edce7cb3b9cd7950",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 82,
            "doc": {
                "time_reference": "23:00-23:15",
                "video": "aJI8XTa_DII",
                "answer": "(D) Obama and Biden",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Trump and Biden",
                    "(B) Obama and Putin",
                    "(C) Obama and Trump",
                    "(D) Obama and Biden"
                ],
                "type": "documentary",
                "question": "Who are the two people who appear in the next frame after Evan Osnos finishes speaking?"
            },
            "target": "(D) Obama and Biden",
            "arguments": [
                "Who are the two people who appear in the next frame after Evan Osnos finishes speaking?\nA. (A) Trump and Biden\nB. (B) Obama and Putin\nC. (C) Obama and Trump\nD. (D) Obama and Biden\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9660>"
                    ]
                },
                82,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5e98bd4beadd36b723da19e67da0485c916ae384a04fec2220cd62d2cd080801",
            "target_hash": "e12bd72aaa5cd4082ea0a7d220f665ea043abb41492766952139a75065aaa705",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 83,
            "doc": {
                "time_reference": "35:00-35:07",
                "video": "aJI8XTa_DII",
                "answer": "(D) 1",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 3",
                    "(B) 2",
                    "(C) 0",
                    "(D) 1"
                ],
                "type": "documentary",
                "question": "How many microphones are on stage during Donald Trump's speech?"
            },
            "target": "(D) 1",
            "arguments": [
                "How many microphones are on stage during Donald Trump's speech?\nA. (A) 3\nB. (B) 2\nC. (C) 0\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea260>"
                    ]
                },
                83,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "86d8be9e5655e5366189a00c8d7d068c74c3621c130ced5d6c3b84d5849221cf",
            "target_hash": "130e553bd423b938999fb170663c3db89fe536737e0598ae8b8ff9079e800be9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 84,
            "doc": {
                "time_reference": "35:35-35:50",
                "video": "aJI8XTa_DII",
                "answer": "(D) Yellow",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Green",
                    "(B) Dark blue",
                    "(C) Bright red",
                    "(D) Yellow"
                ],
                "type": "documentary",
                "question": "What color of coat does Melania Trump wear for the meeting that is hosted at Helsinki, Finland?"
            },
            "target": "(D) Yellow",
            "arguments": [
                "What color of coat does Melania Trump wear for the meeting that is hosted at Helsinki, Finland?\nA. (A) Green\nB. (B) Dark blue\nC. (C) Bright red\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb1c0>"
                    ]
                },
                84,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "65c05d5b5f6cb8a4955fa5e773290639bc28b5870c657ee58ae6fa752107bec1",
            "target_hash": "0b742116fd1f4311d53c40fdbfbf2f5ef15ee874a453ec11b4e98eb3b1a02775",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 85,
            "doc": {
                "time_reference": "38:30-38:37",
                "video": "aJI8XTa_DII",
                "answer": "(D) Sneaking and smiling",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Angry and expressionless",
                    "(B) Without any expression",
                    "(C) Laugh with excitement",
                    "(D) Sneaking and smiling"
                ],
                "type": "documentary",
                "question": "What was Putin's expression when he heard the speech Trump made?"
            },
            "target": "(D) Sneaking and smiling",
            "arguments": [
                "What was Putin's expression when he heard the speech Trump made?\nA. (A) Angry and expressionless\nB. (B) Without any expression\nC. (C) Laugh with excitement\nD. (D) Sneaking and smiling\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c070>"
                    ]
                },
                85,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8d91cb7fd26459fbab402dc14760751373240ebdd6c019a2bc55ca428a424588",
            "target_hash": "5f665281c37546ef17681afe5df063fd58123deb5f9415e3ce59237e5b4ae6d6",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 86,
            "doc": {
                "time_reference": "43:38-43:41",
                "video": "aJI8XTa_DII",
                "answer": "(D) 10",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 12",
                    "(B) 9",
                    "(C) 8",
                    "(D) 10"
                ],
                "type": "documentary",
                "question": "How many trucks are there in the garage with soldiers in white direct the way in the next scene after the Russian plane takes off?"
            },
            "target": "(D) 10",
            "arguments": [
                "How many trucks are there in the garage with soldiers in white direct the way in the next scene after the Russian plane takes off?\nA. (A) 12\nB. (B) 9\nC. (C) 8\nD. (D) 10\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d240>"
                    ]
                },
                86,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c3e17ab1c0db5c71246ddebe432e43841d888770bb2448713eede2b4e69a71e2",
            "target_hash": "b3931203d1a663e06b612233d874ff901f5fe7ae7bb0614fbafa5decfa4ee2d3",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 87,
            "doc": {
                "time_reference": "43:44-43:46",
                "video": "aJI8XTa_DII",
                "answer": "(C) 132",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 321",
                    "(B) 123",
                    "(C) 132",
                    "(D) 312"
                ],
                "type": "documentary",
                "question": "What is the number on the tank can be seen, with two tanks appear in front of it, in the snow?"
            },
            "target": "(C) 132",
            "arguments": [
                "What is the number on the tank can be seen, with two tanks appear in front of it, in the snow?\nA. (A) 321\nB. (B) 123\nC. (C) 132\nD. (D) 312\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e320>"
                    ]
                },
                87,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5538db8ee303450b4f74b9bc9cd16b039126629329686d8cb75be5d5377e7df8",
            "target_hash": "b11e09441d2c35b8e4a70cca02f4080f988c284c3eb0576762e9c9d6ab1ab851",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 88,
            "doc": {
                "time_reference": "46:09-46:19",
                "video": "aJI8XTa_DII",
                "answer": "(C) Online video communication",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Communicate by letter",
                    "(B) Face-to-face communication",
                    "(C) Online video communication",
                    "(D) Communicate by telephone"
                ],
                "type": "documentary",
                "question": "After Yevgenia Albats spoke, how did Putin and Biden communicate in the next scene?"
            },
            "target": "(C) Online video communication",
            "arguments": [
                "After Yevgenia Albats spoke, how did Putin and Biden communicate in the next scene?\nA. (A) Communicate by letter\nB. (B) Face-to-face communication\nC. (C) Online video communication\nD. (D) Communicate by telephone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1300>"
                    ]
                },
                88,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "08e73be05a60d55d567e81951c37ec2200b15f5de5fcf094c4ddfcee8f85e801",
            "target_hash": "f57986711322dc90c741bd4cf11b8165d6a77cf8e453781c8519bb411e5f000b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 89,
            "doc": {
                "time_reference": "49:25-49:28",
                "video": "aJI8XTa_DII",
                "answer": "(A) Russian Federation",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Russian Federation",
                    "(B) The Republic of Belarus",
                    "(C) Ukraine",
                    "(D) France"
                ],
                "type": "documentary",
                "question": "Which country's flag is on the flagpole on the top of the building with gold clock?"
            },
            "target": "(A) Russian Federation",
            "arguments": [
                "Which country's flag is on the flagpole on the top of the building with gold clock?\nA. (A) Russian Federation\nB. (B) The Republic of Belarus\nC. (C) Ukraine\nD. (D) France\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bc040>"
                    ]
                },
                89,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1abdc7aa4b359337e9891f69dcb525923e0e20b91195d1ac551a30e280510a74",
            "target_hash": "5970e7bd93216d8a5dd0aaf1e7b7c1fc73bebb66a3f3e00a84a8b8fd863357cf",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 90,
            "doc": {
                "time_reference": "06:46-06:46",
                "video": "IsdbCjlZ5cQ",
                "answer": "(D) Donald John Trump",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Jane Mayer",
                    "(B) James Clapper",
                    "(C) Matt Apuzzo",
                    "(D) Donald John Trump"
                ],
                "type": "documentary",
                "question": "Who are the main individuals featured in this video?"
            },
            "target": "(D) Donald John Trump",
            "arguments": [
                "Who are the main individuals featured in this video?\nA. (A) Jane Mayer\nB. (B) James Clapper\nC. (C) Matt Apuzzo\nD. (D) Donald John Trump\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9180>"
                    ]
                },
                90,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5ef8416e1a51eca512743ca4c21c1409d9aa70d81ae829f8505f7a75d787ebb9",
            "target_hash": "c3c5da7dc207784cf97fb9f2c53dc41293f5e516abc3bf9971d273742368f353",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 91,
            "doc": {
                "time_reference": "24:20-24:20",
                "video": "IsdbCjlZ5cQ",
                "answer": "(D) A car",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) A mirror",
                    "(B) An industrial robot",
                    "(C) A camera",
                    "(D) A car"
                ],
                "type": "documentary",
                "question": "What object is in front of the J. Edgar Hoover FBI Building sign?"
            },
            "target": "(D) A car",
            "arguments": [
                "What object is in front of the J. Edgar Hoover FBI Building sign?\nA. (A) A mirror\nB. (B) An industrial robot\nC. (C) A camera\nD. (D) A car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea5c0>"
                    ]
                },
                91,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1feee214670d80ef04fe4bc8ae5f8407730ec0a2ca1d1ad8b574ba9685f6e8c2",
            "target_hash": "a029d5e1ab73f1ce1ae401a5c30477ed96cfb7075dde94aabc55ac88753c7b8c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 92,
            "doc": {
                "time_reference": "31:37-31:39",
                "video": "IsdbCjlZ5cQ",
                "answer": "(B) It's a gesture of politeness, indicating that people can start moving around",
                "question_type": [
                    "reasoning",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) He's preparing to shake hands",
                    "(B) It's a gesture of politeness, indicating that people can start moving around",
                    "(C) He's pointing the way for the audience",
                    "(D) He's waving to greet someone"
                ],
                "type": "documentary",
                "question": "What does it mean when Trump raises his hand in front of a green helicopter with United States of America written on it?"
            },
            "target": "(B) It's a gesture of politeness, indicating that people can start moving around",
            "arguments": [
                "What does it mean when Trump raises his hand in front of a green helicopter with United States of America written on it?\nA. (A) He's preparing to shake hands\nB. (B) It's a gesture of politeness, indicating that people can start moving around\nC. (C) He's pointing the way for the audience\nD. (D) He's waving to greet someone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb520>"
                    ]
                },
                92,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8b50ee071f7febd1c7770dd9763f13640f8006ad212c72b0ddc4952a708b68c3",
            "target_hash": "8c2f14e91983fdb102f0e01081968d75af4e831f4dc8ef8d53105c2f8a8c8f4e",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 93,
            "doc": {
                "time_reference": "41:04-41:04",
                "video": "IsdbCjlZ5cQ",
                "answer": "(A) Trump team needs to treat Russia as a serious issue",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Trump team needs to treat Russia as a serious issue",
                    "(B) Meetings could fuel new calls for special counse",
                    "(C) Sessions Controversy Heightens Trump's Feeling of Being UnderSiege",
                    "(D) Why Trump's attorney general has come under fire for earlier contaects"
                ],
                "type": "documentary",
                "question": "What is the headline of the second news report?"
            },
            "target": "(A) Trump team needs to treat Russia as a serious issue",
            "arguments": [
                "What is the headline of the second news report?\nA. (A) Trump team needs to treat Russia as a serious issue\nB. (B) Meetings could fuel new calls for special counse\nC. (C) Sessions Controversy Heightens Trump's Feeling of Being UnderSiege\nD. (D) Why Trump's attorney general has come under fire for earlier contaects\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c160>"
                    ]
                },
                93,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "29f75a7a6f38d718b9c1927a034f6cc5f10dab608a75c8d8be6ab6a794441ff3",
            "target_hash": "d4f1a42ed911d69717847e3b3cf31d7770d0e210d1d006598700dbf95a1a03df",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 94,
            "doc": {
                "time_reference": "49:34-49:40",
                "video": "IsdbCjlZ5cQ",
                "answer": "(B) He is looking at his right",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) He is looking at his left",
                    "(B) He is looking at his right",
                    "(C) He is looking at the ceiling",
                    "(D) He is looking at the floor"
                ],
                "type": "documentary",
                "question": "Which direction does Don Mcgahn look at in the black and white picture that has a portrait behind his back?"
            },
            "target": "(B) He is looking at his right",
            "arguments": [
                "Which direction does Don Mcgahn look at in the black and white picture that has a portrait behind his back?\nA. (A) He is looking at his left\nB. (B) He is looking at his right\nC. (C) He is looking at the ceiling\nD. (D) He is looking at the floor\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d3c0>"
                    ]
                },
                94,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f7379be48d854dacc26b00260baef1d1de69b6be608e53e01849d9cc31f78164",
            "target_hash": "6f9ef438f603548128728e8ed090f67214bdf9441bdd5034e6619e2ccce1b332",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 95,
            "doc": {
                "time_reference": "52:33-52:43",
                "video": "IsdbCjlZ5cQ",
                "answer": "(B) To support a specific point",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) To highlight the central theme of the documentary, serving as a thematic anchor",
                    "(B) To support a specific point",
                    "(C) As evidence presented in a court trial",
                    "(D) To visually emphasize the theme of the two documents"
                ],
                "type": "documentary",
                "question": "Why two sentences have red underlines on two different documents?"
            },
            "target": "(B) To support a specific point",
            "arguments": [
                "Why two sentences have red underlines on two different documents?\nA. (A) To highlight the central theme of the documentary, serving as a thematic anchor\nB. (B) To support a specific point\nC. (C) As evidence presented in a court trial\nD. (D) To visually emphasize the theme of the two documents\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d090>"
                    ]
                },
                95,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dd970b66a909c9c858ad26104327462374d388e32ed5d4009346bfb465bfe755",
            "target_hash": "11006d59f0a4de7b727bd0a000f2a532d383de66669435bbd4c49933f6d79fac",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 96,
            "doc": {
                "time_reference": "60:10-60:36",
                "video": "IsdbCjlZ5cQ",
                "answer": "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
                    "(B) Trump holds a press conference, addressing recent developments and answering questions from journalists",
                    "(C) Trump issues a national statement, addressing the nation on a significant issue or event",
                    "(D) Trump is seen attending a court hearing, likely related to a legal matter concerning his administration or personal affairs"
                ],
                "type": "documentary",
                "question": "What is the best summary for 60:10-60:36?"
            },
            "target": "(A) Donald Trump is seen engaging in a series of interviews with various hosts",
            "arguments": [
                "What is the best summary for 60:10-60:36?\nA. (A) Donald Trump is seen engaging in a series of interviews with various hosts\nB. (B) Trump holds a press conference, addressing recent developments and answering questions from journalists\nC. (C) Trump issues a national statement, addressing the nation on a significant issue or event\nD. (D) Trump is seen attending a court hearing, likely related to a legal matter concerning his administration or personal affairs\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0970>"
                    ]
                },
                96,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "832475ab27fe4e21601c34aaaebfa5ce6af38857f32c16a4a9259ecd0e004f7a",
            "target_hash": "54c54abaf6166b9e6762af127fe6d5e540d047b27fa42ec80aa866d7643f5607",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 97,
            "doc": {
                "time_reference": "83:44-83:51",
                "video": "IsdbCjlZ5cQ",
                "answer": "(D) The video is shot at a parking lot exit gate",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) The location seems to be a shopping mall entrance",
                    "(B) The video is likely taken in a public transportation station",
                    "(C) The setting appears to be an airport terminal",
                    "(D) The video is shot at a parking lot exit gate"
                ],
                "type": "documentary",
                "question": "What is the place that the footage shows a red car passes when the light is green, and a person wearing a white shirt with a red circle on her left sleeve, a black pant, standing in front of the light?"
            },
            "target": "(D) The video is shot at a parking lot exit gate",
            "arguments": [
                "What is the place that the footage shows a red car passes when the light is green, and a person wearing a white shirt with a red circle on her left sleeve, a black pant, standing in front of the light?\nA. (A) The location seems to be a shopping mall entrance\nB. (B) The video is likely taken in a public transportation station\nC. (C) The setting appears to be an airport terminal\nD. (D) The video is shot at a parking lot exit gate\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bfd00>"
                    ]
                },
                97,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bc6985f5549ce65c469105275bd12a4b4233d62b48e67d7e5560b79fc73bce6f",
            "target_hash": "bc71bcd82ff62ec1c79d8acc46932380df8bc6af734a67646454c223f02f478c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 98,
            "doc": {
                "time_reference": "90:09-90:09",
                "video": "IsdbCjlZ5cQ",
                "answer": "(C) A circle of white name tags",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) A circle of white dotted patterns",
                    "(B) A circle of white chevron patterns",
                    "(C) A circle of white name tags",
                    "(D) A circle of white government documents"
                ],
                "type": "documentary",
                "question": "What is in the center of the table?"
            },
            "target": "(C) A circle of white name tags",
            "arguments": [
                "What is in the center of the table?\nA. (A) A circle of white dotted patterns\nB. (B) A circle of white chevron patterns\nC. (C) A circle of white name tags\nD. (D) A circle of white government documents\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e95d0>"
                    ]
                },
                98,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "81548861aa037ba81b767b0e00cf6d9e5777f4df9294444bbeb7e579ee1edf6b",
            "target_hash": "59bfb9ad5845912092a1f29e2609c70247239aca14b14a561d0b646104cdeb43",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 99,
            "doc": {
                "time_reference": "12:30-14:00",
                "video": "EpMLAQbSYAw",
                "answer": "(A) From frowning to happy",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) From frowning to happy",
                    "(B) Keep frowning",
                    "(C) From happy to frowning",
                    "(D) From frowning to angry"
                ],
                "type": "documentary",
                "question": "How does the emotion change on Sarah's face when interviewed?"
            },
            "target": "(A) From frowning to happy",
            "arguments": [
                "How does the emotion change on Sarah's face when interviewed?\nA. (A) From frowning to happy\nB. (B) Keep frowning\nC. (C) From happy to frowning\nD. (D) From frowning to angry\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8580>"
                    ]
                },
                99,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "065adde600493bcf6c41014d45b91354955b7f89a1444d0d2ef31817fc83e3db",
            "target_hash": "0250d18a8a145648a51d827cf89e443267482439dc38b1093339e77aba7ce022",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 100,
            "doc": {
                "time_reference": "16:00-16:00",
                "video": "EpMLAQbSYAw",
                "answer": "(A) Volatility and Anger",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Volatility and Anger",
                    "(B) Whatever It Takes",
                    "(C) Orgy of Speculation",
                    "(D) The Fed Blinked"
                ],
                "type": "documentary",
                "question": "What is the name of Chapter2?"
            },
            "target": "(A) Volatility and Anger",
            "arguments": [
                "What is the name of Chapter2?\nA. (A) Volatility and Anger\nB. (B) Whatever It Takes\nC. (C) Orgy of Speculation\nD. (D) The Fed Blinked\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eba30>"
                    ]
                },
                100,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c7d26383214aa8d5d05b0a325f90d0a525fd23fbcdee1daeda1944d38eff26e1",
            "target_hash": "a6314dfd7ffd19dfad622636a57b7010df70f2ebf6737e05df393f457eafedb9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 101,
            "doc": {
                "time_reference": "18:00-18:53",
                "video": "EpMLAQbSYAw",
                "answer": "(C) They wear red clothes",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Thay are all men",
                    "(B) They are the elderly",
                    "(C) They wear red clothes",
                    "(D) They are tall"
                ],
                "type": "documentary",
                "question": "What characteristic does the Tea Party have in common in this people?"
            },
            "target": "(C) They wear red clothes",
            "arguments": [
                "What characteristic does the Tea Party have in common in this people?\nA. (A) Thay are all men\nB. (B) They are the elderly\nC. (C) They wear red clothes\nD. (D) They are tall\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c700>"
                    ]
                },
                101,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5e069b6b52f08ffa291f154ddee311a16780754d8219333523d5c6b30fb05fbd",
            "target_hash": "9f12d06d8cae236c4d41d0ea27759b076c2db2ac1567dcdd047a9de20b15b388",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 102,
            "doc": {
                "time_reference": "51:00-52:00",
                "video": "EpMLAQbSYAw",
                "answer": "(D) In his home",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) In his office",
                    "(B) In a garden",
                    "(C) In a beach",
                    "(D) In his home"
                ],
                "type": "documentary",
                "question": "What place would Mohamed's interview most likely to take place?"
            },
            "target": "(D) In his home",
            "arguments": [
                "What place would Mohamed's interview most likely to take place?\nA. (A) In his office\nB. (B) In a garden\nC. (C) In a beach\nD. (D) In his home\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d510>"
                    ]
                },
                102,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "94163280241ec45ea16e5c2c3d5706ae26f5864ddd69d42afc6fe38a52bacb22",
            "target_hash": "81e2071931e9e122da8ab4b200a288f6cd448641663df342e990ec76fc133148",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 103,
            "doc": {
                "time_reference": "61:45-61:45",
                "video": "EpMLAQbSYAw",
                "answer": "(A) Pigeon and crow",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Pigeon and crow",
                    "(B) Dog and pigeon",
                    "(C) Pigeon and bull",
                    "(D) Crow and bull"
                ],
                "type": "documentary",
                "question": "What animal is on the street next to the famous Wall Street statue?"
            },
            "target": "(A) Pigeon and crow",
            "arguments": [
                "What animal is on the street next to the famous Wall Street statue?\nA. (A) Pigeon and crow\nB. (B) Dog and pigeon\nC. (C) Pigeon and bull\nD. (D) Crow and bull\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e350>"
                    ]
                },
                103,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9e1a34e98ee93b69f8f4b62e11169b605a4620fee5217a26b46e27b988572a57",
            "target_hash": "f65e37aee53402e64251541a9fae333e0fafa4704e52e5b24ca1102c8c176b5b",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 104,
            "doc": {
                "time_reference": "62:18-62:46",
                "video": "EpMLAQbSYAw",
                "answer": "(D) Pen",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Cigarette",
                    "(B) Phone",
                    "(C) Food",
                    "(D) Pen"
                ],
                "type": "documentary",
                "question": "What does Jacoby have on his hand in the interview with Jeremy?"
            },
            "target": "(D) Pen",
            "arguments": [
                "What does Jacoby have on his hand in the interview with Jeremy?\nA. (A) Cigarette\nB. (B) Phone\nC. (C) Food\nD. (D) Pen\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0ee0>"
                    ]
                },
                104,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "76bbd41112d6f9eeeb83cb9f4d430065504fd86876697a81db31d63dc87a201f",
            "target_hash": "a415e5964f08f8bd72862780658745a0dd97ba191da681dadbd5ad3e221f8818",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 105,
            "doc": {
                "time_reference": "70:35-70:35",
                "video": "EpMLAQbSYAw",
                "answer": "(C) +71.88, 0.89%",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) +51.28, 1.86%",
                    "(B) +644.10, 1.44%",
                    "(C) +71.88, 0.89%",
                    "(D) +164.93, 2.90%"
                ],
                "type": "documentary",
                "question": "What are the changing trends of NASDAQ when the changing trends of Dow Jones are +457.21, 1.95%?"
            },
            "target": "(C) +71.88, 0.89%",
            "arguments": [
                "What are the changing trends of NASDAQ when the changing trends of Dow Jones are +457.21, 1.95%?\nA. (A) +51.28, 1.86%\nB. (B) +644.10, 1.44%\nC. (C) +71.88, 0.89%\nD. (D) +164.93, 2.90%\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bf430>"
                    ]
                },
                105,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0e33d6f11ab52373c0557ce466c22f774ddcb709aec7ff2f564321671a33f45c",
            "target_hash": "50d6681fd9c1e2c9afea6e278f9814da193baae7a12d993c350b31ef0e4d0654",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 106,
            "doc": {
                "time_reference": "84:13-84:13",
                "video": "EpMLAQbSYAw",
                "answer": "(D) Blue",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Yellow",
                    "(B) White",
                    "(C) Black",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What colour is Tom Kertis's clothes?"
            },
            "target": "(D) Blue",
            "arguments": [
                "What colour is Tom Kertis's clothes?\nA. (A) Yellow\nB. (B) White\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8970>"
                    ]
                },
                106,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "64b64b9309d2b0b1221cdc3351f9ae27b71d50bf6631bfb5dace5365f6a206e1",
            "target_hash": "2af8475f8d12112df3b1487762f27a4f3153636b07666dd9be3936b9d3a38176",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 107,
            "doc": {
                "time_reference": "00:00-113:00",
                "video": "EpMLAQbSYAw",
                "answer": "(C) Economist",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Prisident",
                    "(B) Unemployed",
                    "(C) Economist",
                    "(D) Worker"
                ],
                "type": "documentary",
                "question": "What is the most common job for the interviewees?"
            },
            "target": "(C) Economist",
            "arguments": [
                "What is the most common job for the interviewees?\nA. (A) Prisident\nB. (B) Unemployed\nC. (C) Economist\nD. (D) Worker\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8e80>"
                    ]
                },
                107,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c32cad4cf55d18f6d56ecded86943da2ef40c6ce0318a3590edf52a3a50b0fb8",
            "target_hash": "5d2cd4bd5d76ebd23f234200407875e72296aba438787740deb1e5157e374eef",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 108,
            "doc": {
                "time_reference": "00:00-113:00",
                "video": "EpMLAQbSYAw",
                "answer": "(B) A documentary about the economy",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) A documentary about America",
                    "(B) A documentary about the economy",
                    "(C) A documentary about the sport",
                    "(D) A documentary about the festival"
                ],
                "type": "documentary",
                "question": "What is the category of this video?"
            },
            "target": "(B) A documentary about the economy",
            "arguments": [
                "What is the category of this video?\nA. (A) A documentary about America\nB. (B) A documentary about the economy\nC. (C) A documentary about the sport\nD. (D) A documentary about the festival\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb730>"
                    ]
                },
                108,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1d8f1f164b6c1a883d384db20d740e63a566d448dc5a5a115e5c91c1006bb683",
            "target_hash": "40ef6c9885f5b743df00d67adb91a47376c68e721e9225db661ff29451a0f99f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 109,
            "doc": {
                "time_reference": "00:00-113:00",
                "video": "EpMLAQbSYAw",
                "answer": "(B) Suit",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Dress",
                    "(B) Suit",
                    "(C) Sportswear",
                    "(D) Swimwear"
                ],
                "type": "documentary",
                "question": "What is the most common costume in the video?"
            },
            "target": "(B) Suit",
            "arguments": [
                "What is the most common costume in the video?\nA. (A) Dress\nB. (B) Suit\nC. (C) Sportswear\nD. (D) Swimwear\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c820>"
                    ]
                },
                109,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4c6072409c5eee515d14e14eb96b5917627f9ceef1fc7aabcf78ab35c0e20c4f",
            "target_hash": "c485ccf7260c2ce1fcb6e4f9ebdc6ad6b891c71e5b088ae55c920cafc7c9ec24",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 110,
            "doc": {
                "time_reference": "00:00-113:00",
                "video": "EpMLAQbSYAw",
                "answer": "(C) Skyscraper",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Money",
                    "(B) Cars",
                    "(C) Skyscraper",
                    "(D) Trees"
                ],
                "type": "documentary",
                "question": "What is the most frequent element in the video?"
            },
            "target": "(C) Skyscraper",
            "arguments": [
                "What is the most frequent element in the video?\nA. (A) Money\nB. (B) Cars\nC. (C) Skyscraper\nD. (D) Trees\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d630>"
                    ]
                },
                110,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "01fdba18c0968b53b84c67d5a5459177372614cad8246450d208e9fea976c11e",
            "target_hash": "c670b97ecf0d91146e118206c917ed4260ad9ae126b3d238f79b06c58ad2056e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 111,
            "doc": {
                "time_reference": "03:56-05:00",
                "video": "KktLi3UifPY",
                "answer": "(C) Because they buy ramen from a convenience store, and there are no seats inside",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Because eating noodles is not allowed inside the 711 convenience store",
                    "(B) Because filming is not allowed inside the 711 convenience store",
                    "(C) Because they buy ramen from a convenience store, and there are no seats inside",
                    "(D) Because they are waiting for a bus at the bus stop"
                ],
                "type": "documentary",
                "question": "What is the possible reason why two hosts are standing on the roadside with bowls in their hands?"
            },
            "target": "(C) Because they buy ramen from a convenience store, and there are no seats inside",
            "arguments": [
                "What is the possible reason why two hosts are standing on the roadside with bowls in their hands?\nA. (A) Because eating noodles is not allowed inside the 711 convenience store\nB. (B) Because filming is not allowed inside the 711 convenience store\nC. (C) Because they buy ramen from a convenience store, and there are no seats inside\nD. (D) Because they are waiting for a bus at the bus stop\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d8d0>"
                    ]
                },
                111,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e157ee21f4cf50979927c9fa523f229f42e834483ad2f21651b622ca86a8d8cb",
            "target_hash": "3e9c751bdb128bb2bf19d7d4244a9a10af11b0446c8816e756a17732f7e05fa3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 112,
            "doc": {
                "time_reference": "14:16-14:16",
                "video": "KktLi3UifPY",
                "answer": "(C) $100",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) $10",
                    "(B) ￥100",
                    "(C) $100",
                    "(D) 100 Japanese Yen"
                ],
                "type": "documentary",
                "question": "How much does a ramen set cost in the second ramen shop?"
            },
            "target": "(C) $100",
            "arguments": [
                "How much does a ramen set cost in the second ramen shop?\nA. (A) $10\nB. (B) ￥100\nC. (C) $100\nD. (D) 100 Japanese Yen\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c09d0>"
                    ]
                },
                112,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "718f3e93aa8ecb7f86561fa1d13c935d29a1ade82045a95c09e5162df6dfa8ff",
            "target_hash": "8b1f568e8d2a5a3b0c740e4b4ebf2d57c61a4338cd55a6bf068b90cf462e5e66",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 113,
            "doc": {
                "time_reference": "21:23-21:23",
                "video": "KktLi3UifPY",
                "answer": "(A) The owner",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) The owner",
                    "(B) A food critic",
                    "(C) A special guest comedian",
                    "(D) A waiter"
                ],
                "type": "documentary",
                "question": "What is Keita Karino's identity in the store?"
            },
            "target": "(A) The owner",
            "arguments": [
                "What is Keita Karino's identity in the store?\nA. (A) The owner\nB. (B) A food critic\nC. (C) A special guest comedian\nD. (D) A waiter\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c11b0>"
                    ]
                },
                113,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1332a5185ce500b11e463f53136b26ad4ddeb0f403d7b5b410b7596e76b34b71",
            "target_hash": "77d62be806cf7a136fff0d6b5b5873fa06905580127a94c4e61ea8af99746f1d",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 114,
            "doc": {
                "time_reference": "27:17-27:17",
                "video": "KktLi3UifPY",
                "answer": "(B) 8 months ago",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 8 days ago",
                    "(B) 8 months ago",
                    "(C) 4 months ago",
                    "(D) 3 months ago"
                ],
                "type": "documentary",
                "question": "When does Oliver_B post his comment?"
            },
            "target": "(B) 8 months ago",
            "arguments": [
                "When does Oliver_B post his comment?\nA. (A) 8 days ago\nB. (B) 8 months ago\nC. (C) 4 months ago\nD. (D) 3 months ago\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9840>"
                    ]
                },
                114,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4d85de8c4511fd2cfb9fb5cc9deb960aec1225cbdd7297a8829deb5e505bc704",
            "target_hash": "1cd43fc04d85c9dffd2ed9694de181127aafe1d20947f42ed13075e0c8a9542d",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 115,
            "doc": {
                "time_reference": "28:06-33:00",
                "video": "KktLi3UifPY",
                "answer": "(D) Grasshoppers",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Natto",
                    "(B) Rice",
                    "(C) Soybeans",
                    "(D) Grasshoppers"
                ],
                "type": "documentary",
                "question": "What are the two hosts looking for in the field?"
            },
            "target": "(D) Grasshoppers",
            "arguments": [
                "What are the two hosts looking for in the field?\nA. (A) Natto\nB. (B) Rice\nC. (C) Soybeans\nD. (D) Grasshoppers\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea980>"
                    ]
                },
                115,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3d78cd912942fcb14135b02c2a062790892fff2a856a27db88af65d0e49a11ec",
            "target_hash": "869cb012d776146bbde30c0b3bce63756e4b6a91a33f8ebc1b12796f39d3e9d2",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 116,
            "doc": {
                "time_reference": "40:20-40:20",
                "video": "KktLi3UifPY",
                "answer": "(D) She frowns and appears puzzled",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) She is very sad",
                    "(B) She is very happy and can't wait to taste the dish",
                    "(C) She is angry and feels disgusted",
                    "(D) She frowns and appears puzzled"
                ],
                "type": "documentary",
                "question": "How does the female host express herself at 40:20?"
            },
            "target": "(D) She frowns and appears puzzled",
            "arguments": [
                "How does the female host express herself at 40:20?\nA. (A) She is very sad\nB. (B) She is very happy and can't wait to taste the dish\nC. (C) She is angry and feels disgusted\nD. (D) She frowns and appears puzzled\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb970>"
                    ]
                },
                116,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5541e074a7a848005b9aef2157c8c324edea10ae112b175c9cc0e57ac7264c7d",
            "target_hash": "2942c111a74bd61953a8285ac0ca81f5207680084fbc367a7a9c57fd22997f4c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 117,
            "doc": {
                "time_reference": "43:15-43:20",
                "video": "KktLi3UifPY",
                "answer": "(C) Cooking them",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Drying them",
                    "(B) Cleaning them",
                    "(C) Cooking them",
                    "(D) Mincing them"
                ],
                "type": "documentary",
                "question": "Why does the worker pour the small fish from the blue basket into the large container?"
            },
            "target": "(C) Cooking them",
            "arguments": [
                "Why does the worker pour the small fish from the blue basket into the large container?\nA. (A) Drying them\nB. (B) Cleaning them\nC. (C) Cooking them\nD. (D) Mincing them\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c940>"
                    ]
                },
                117,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9e3e384150aceebd7752c29d5ba5e7ec73b09209129fe565f8ccda8aecb79ee0",
            "target_hash": "1154f5fe1c9af48a65e477d28626ba355a10f924965f9903e9cf21bbed8e74ab",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 118,
            "doc": {
                "time_reference": "68:09-71:16",
                "video": "KktLi3UifPY",
                "answer": "(A) The grading process of Wagyu beef",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) The grading process of Wagyu beef",
                    "(B) The process of raising Wagyu beef",
                    "(C) The transportation of Wagyu beef to restaurants",
                    "(D) The storage of Wagyu beef in a factory"
                ],
                "type": "documentary",
                "question": "What is the main content introduced in the video segment from 68:09-71:16?"
            },
            "target": "(A) The grading process of Wagyu beef",
            "arguments": [
                "What is the main content introduced in the video segment from 68:09-71:16?\nA. (A) The grading process of Wagyu beef\nB. (B) The process of raising Wagyu beef\nC. (C) The transportation of Wagyu beef to restaurants\nD. (D) The storage of Wagyu beef in a factory\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d750>"
                    ]
                },
                118,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0d3aaa083d953517031ce6bb2c4e94d7fb0930eb569f0df5d9611bf819b37bb0",
            "target_hash": "190d7099111f5b285a7fe1697903f61d03c488fa1cd1d9a31f9e3e5efdfc2376",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 119,
            "doc": {
                "time_reference": "85:50-85:35",
                "video": "KktLi3UifPY",
                "answer": "(D) Inside fish meat dumplings",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) In the soup",
                    "(B) Inside pork dumplings",
                    "(C) In the rice",
                    "(D) Inside fish meat dumplings"
                ],
                "type": "documentary",
                "question": "Where is the fish head placed from 85:35-85:50?"
            },
            "target": "(D) Inside fish meat dumplings",
            "arguments": [
                "Where is the fish head placed from 85:35-85:50?\nA. (A) In the soup\nB. (B) Inside pork dumplings\nC. (C) In the rice\nD. (D) Inside fish meat dumplings\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e3e0>"
                    ]
                },
                119,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a2b0b112293b71918320c4b466a2937298beea453b485280f5de50fda7027073",
            "target_hash": "e6352ce20ae9461edfe8281c811bf40de966bb75f391cd7604132a8227efec25",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 120,
            "doc": {
                "time_reference": "88:07-88:07",
                "video": "KktLi3UifPY",
                "answer": "(D) Fukuoka",
                "question_type": [
                    "key information retrieval",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) Tokyo",
                    "(B) Sendai",
                    "(C) Fukushima",
                    "(D) Fukuoka"
                ],
                "type": "documentary",
                "question": "What text appears at 88:07?"
            },
            "target": "(D) Fukuoka",
            "arguments": [
                "What text appears at 88:07?\nA. (A) Tokyo\nB. (B) Sendai\nC. (C) Fukushima\nD. (D) Fukuoka\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c05e0>"
                    ]
                },
                120,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3f870909297ffcb5adc80e2ad1593996364cc7f450d4a65f4229e17a6eb7f142",
            "target_hash": "a0b195f766b3b11086637757f75e075ff9b8760c26b6160ef6ad6d4fbcca59dd",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 121,
            "doc": {
                "time_reference": "53:26-54:16",
                "video": "KktLi3UifPY",
                "answer": "(A) From hopeful to seasick",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) From hopeful to seasick",
                    "(B) From excited to disappointed",
                    "(C) From relaxed to anxious",
                    "(D) From curious to bored"
                ],
                "type": "documentary",
                "question": "How does the male host feel on the ship?"
            },
            "target": "(A) From hopeful to seasick",
            "arguments": [
                "How does the male host feel on the ship?\nA. (A) From hopeful to seasick\nB. (B) From excited to disappointed\nC. (C) From relaxed to anxious\nD. (D) From curious to bored\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bcc10>"
                    ]
                },
                121,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "67c3539403a8c2f4a5f235ac1d857f6085b0aa2c8ba8871a55ed2cc53349aad2",
            "target_hash": "658a1db71d8dc5c4f33652396680d9a8662d96e9aa41c5f7684b30a776e24580",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 122,
            "doc": {
                "time_reference": "00:00-100:54",
                "video": "KktLi3UifPY",
                "answer": "(B) Deer meat",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Beef",
                    "(B) Deer meat",
                    "(C) Chicken",
                    "(D) Horse meat"
                ],
                "type": "documentary",
                "question": "What is the last type of raw meat introduced in the video?"
            },
            "target": "(B) Deer meat",
            "arguments": [
                "What is the last type of raw meat introduced in the video?\nA. (A) Beef\nB. (B) Deer meat\nC. (C) Chicken\nD. (D) Horse meat\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e99f0>"
                    ]
                },
                122,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "262619edcdc7c7bba8e63d36d9f2cb316c8336af49c3415ea4497335c11b4bd4",
            "target_hash": "fea12f5829fe472c95128a72e23c1025267517993a34d311fcdbb4cf50b92a77",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 123,
            "doc": {
                "time_reference": "99:35-99:35",
                "video": "KktLi3UifPY",
                "answer": "(B) RPay",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Alipay",
                    "(B) RPay",
                    "(C) Apple Pay",
                    "(D) WeChat Pay"
                ],
                "type": "documentary",
                "question": "What payment method appears in the video at last?"
            },
            "target": "(B) RPay",
            "arguments": [
                "What payment method appears in the video at last?\nA. (A) Alipay\nB. (B) RPay\nC. (C) Apple Pay\nD. (D) WeChat Pay\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eaaa0>"
                    ]
                },
                123,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f6c1b884cfc44bddc5d25b4a9172511699b636798635c7df39a1aec9641b06c1",
            "target_hash": "3b9d38b65adf500e198b62271408c97d0e0e767f0628ef719e5932399d8288a0",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 124,
            "doc": {
                "time_reference": "01:09-01:15",
                "video": "wNCPgIVz15c",
                "answer": "(B) Homemade Masala",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Orange juice",
                    "(B) Homemade Masala",
                    "(C) Lemon juice",
                    "(D) Pepper water"
                ],
                "type": "documentary",
                "question": "What liquid is added on top of the grilled chicken at the beginning of the video?"
            },
            "target": "(B) Homemade Masala",
            "arguments": [
                "What liquid is added on top of the grilled chicken at the beginning of the video?\nA. (A) Orange juice\nB. (B) Homemade Masala\nC. (C) Lemon juice\nD. (D) Pepper water\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebb80>"
                    ]
                },
                124,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b7f5a99eeb61621c4b5359da0c97eeb590798c563b5a45e1766910fb3eec8bc5",
            "target_hash": "011603014badf4f69aea01031648402405a9075ee9191c5b75529244d0e6c40f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 125,
            "doc": {
                "time_reference": "07:38-07:38",
                "video": "wNCPgIVz15c",
                "answer": "(B) Orange",
                "question_type": [
                    "entity recognition",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Blue",
                    "(B) Orange",
                    "(C) White",
                    "(D) Black"
                ],
                "type": "documentary",
                "question": "What color are the chairs in the restaurant where the food travelers eat Peshawari chicken with roti?"
            },
            "target": "(B) Orange",
            "arguments": [
                "What color are the chairs in the restaurant where the food travelers eat Peshawari chicken with roti?\nA. (A) Blue\nB. (B) Orange\nC. (C) White\nD. (D) Black\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7ca60>"
                    ]
                },
                125,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6bdb77450b2c88fbfb531e7821c62190d08a937c5d70fbff697b629156786a2a",
            "target_hash": "1d5e0e9af1a6546cb8118be7370935727ec3fc75f5a4a1739b481b79605c418d",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 126,
            "doc": {
                "time_reference": "09:55-20:50",
                "video": "wNCPgIVz15c",
                "answer": "(A) A duckbill cap",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) A duckbill cap",
                    "(B) A top hat",
                    "(C) A headscarf",
                    "(D) He isn't wearing anything"
                ],
                "type": "documentary",
                "question": "What decoration is the man wearing on his head while sitting at the table and talking with the American man in the video, but not eating together?"
            },
            "target": "(A) A duckbill cap",
            "arguments": [
                "What decoration is the man wearing on his head while sitting at the table and talking with the American man in the video, but not eating together?\nA. (A) A duckbill cap\nB. (B) A top hat\nC. (C) A headscarf\nD. (D) He isn't wearing anything\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d870>"
                    ]
                },
                126,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a9e63c1ace0a82cbb0d82ad8d6ba8f0f514203963aefd883e7bfbc85681f635e",
            "target_hash": "627836e9e8ce07cc7fb3e644d502955dffb4f51cdc9d7feeb50095da71cdcea7",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 127,
            "doc": {
                "time_reference": "13:17-73:18",
                "video": "wNCPgIVz15c",
                "answer": "(B) 7",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 8",
                    "(B) 7",
                    "(C) 4",
                    "(D) 6"
                ],
                "type": "documentary",
                "question": "How many women does the American man have dinner face-to-face with in the video?"
            },
            "target": "(B) 7",
            "arguments": [
                "How many women does the American man have dinner face-to-face with in the video?\nA. (A) 8\nB. (B) 7\nC. (C) 4\nD. (D) 6\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e4d0>"
                    ]
                },
                127,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2e70f3c3680964574e7407b97a4440a8eab516a71ee9082aaed21654e9c6db2d",
            "target_hash": "31fdd1beedc916020bd5e4594805b0f8033d1061246b9e1a1d07c1aa44ef9588",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 128,
            "doc": {
                "time_reference": "01:58-84:27",
                "video": "wNCPgIVz15c",
                "answer": "(D) Hands",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Forks",
                    "(B) Spoons",
                    "(C) Chopsticks",
                    "(D) Hands"
                ],
                "type": "documentary",
                "question": "In this video, what is the primary method of eating?"
            },
            "target": "(D) Hands",
            "arguments": [
                "In this video, what is the primary method of eating?\nA. (A) Forks\nB. (B) Spoons\nC. (C) Chopsticks\nD. (D) Hands\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0fd0>"
                    ]
                },
                128,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "53ce7ce936af9c7ced709c5ed240ce70a01d4468780c94b218e875350b184b75",
            "target_hash": "d16a0374c9cc7d086585ced790b085c151c01a814c9ad6aa1786ac7253e8efa0",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 129,
            "doc": {
                "time_reference": "22:10-22:30",
                "video": "wNCPgIVz15c",
                "answer": "(A) Right",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Right",
                    "(B) Both",
                    "(C) Neither",
                    "(D) Left"
                ],
                "type": "documentary",
                "question": "Which foot does the person in the video use to cut open the cow's stomach?"
            },
            "target": "(A) Right",
            "arguments": [
                "Which foot does the person in the video use to cut open the cow's stomach?\nA. (A) Right\nB. (B) Both\nC. (C) Neither\nD. (D) Left\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6edb2bcc40>"
                    ]
                },
                129,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28ac1009f283884db11c56bac86672fabf32466f8083b45cc13cf3b33fa2b10c",
            "target_hash": "f1a02424e7457a4fecdeb04b1595eab44a1128281ae2135230dd5a92e5704e1a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 130,
            "doc": {
                "time_reference": "34:14-36:27",
                "video": "wNCPgIVz15c",
                "answer": "(C) 8",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 2",
                    "(B) 6",
                    "(C) 8",
                    "(D) 4"
                ],
                "type": "documentary",
                "question": "In the video, how many people are having breakfast with the American man? (Excluding himself)"
            },
            "target": "(C) 8",
            "arguments": [
                "In the video, how many people are having breakfast with the American man? (Excluding himself)\nA. (A) 2\nB. (B) 6\nC. (C) 8\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9b10>"
                    ]
                },
                130,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3a7ed1c40f684d70c8427c32bef49620a135a7b407e4edf15180f2953ab74195",
            "target_hash": "bd726fe4af424e6905afb3056d7c1cecc71eedb4ef7be1fe382f0fa6d8493970",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 131,
            "doc": {
                "time_reference": "44:08-44:11",
                "video": "wNCPgIVz15c",
                "answer": "(B) Taking a breathe of cool air",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Drinking water",
                    "(B) Taking a breathe of cool air",
                    "(C) Dancing",
                    "(D) Eating orange"
                ],
                "type": "documentary",
                "question": "What does the food traveller do after eating the very spicy food?"
            },
            "target": "(B) Taking a breathe of cool air",
            "arguments": [
                "What does the food traveller do after eating the very spicy food?\nA. (A) Drinking water\nB. (B) Taking a breathe of cool air\nC. (C) Dancing\nD. (D) Eating orange\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eabc0>"
                    ]
                },
                131,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1437690d5def4305e2358fed4849bc81d6c7e68c14dfe5b8c0641a4a450dc00d",
            "target_hash": "9efbd3efffcbedfd97508c1e763388c165d4c5e519661d3588fe5956372a69f0",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 132,
            "doc": {
                "time_reference": "01:00-84:27",
                "video": "wNCPgIVz15c",
                "answer": "(C) 5",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 10",
                    "(B) 1",
                    "(C) 5",
                    "(D) 2"
                ],
                "type": "documentary",
                "question": "How many different outfits has the American man worn in total in the video?"
            },
            "target": "(C) 5",
            "arguments": [
                "How many different outfits has the American man worn in total in the video?\nA. (A) 10\nB. (B) 1\nC. (C) 5\nD. (D) 2\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebca0>"
                    ]
                },
                132,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4151e7a10eca9dd30fc8e1eaeb658b2d512f948878b4989f15c53ac6448dac2b",
            "target_hash": "7ec5055a12d4c160c0b5a3ffe1114a5b4db86c4e3a865a6d6e80665b19e83ad5",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 133,
            "doc": {
                "time_reference": "63:30-63:30",
                "video": "wNCPgIVz15c",
                "answer": "(A) The sun setting behind the snowy mountains",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) The sun setting behind the snowy mountains",
                    "(B) The fish swimming",
                    "(C) The children crying",
                    "(D) The sprout becomes a tree"
                ],
                "type": "documentary",
                "question": "What scene appears while the American is chewing the corn-wheat bread?"
            },
            "target": "(A) The sun setting behind the snowy mountains",
            "arguments": [
                "What scene appears while the American is chewing the corn-wheat bread?\nA. (A) The sun setting behind the snowy mountains\nB. (B) The fish swimming\nC. (C) The children crying\nD. (D) The sprout becomes a tree\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cb80>"
                    ]
                },
                133,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1c0a3320dcc38075ea4da9e83c3414ec9ebdee54ec9434f87ad45bd2a8bdc071",
            "target_hash": "88d73eaead54e1468f9ce15de2a19b5e1e70e506d171f1510cce16dba1d988c5",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 134,
            "doc": {
                "time_reference": "64:59-65:01",
                "video": "wNCPgIVz15c",
                "answer": "(A) Counterclockwise",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Counterclockwise",
                    "(B) Both ways",
                    "(C) There's no rotation",
                    "(D) Clockwise"
                ],
                "type": "documentary",
                "question": "Is the lady in the video stirring the water in the pot clockwise or counterclockwise when making the onion garlic noodle soup?"
            },
            "target": "(A) Counterclockwise",
            "arguments": [
                "Is the lady in the video stirring the water in the pot clockwise or counterclockwise when making the onion garlic noodle soup?\nA. (A) Counterclockwise\nB. (B) Both ways\nC. (C) There's no rotation\nD. (D) Clockwise\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d990>"
                    ]
                },
                134,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c26f86f4325279cb494ef177d949f228d67a03ec9f767a9129d0ddabeadc3005",
            "target_hash": "d287f7453f750854d093b7aae8bf101c2cc0256c61c5df61198f519780668a2c",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 135,
            "doc": {
                "time_reference": "69:06-69:06",
                "video": "wNCPgIVz15c",
                "answer": "(D) Yak",
                "question_type": [
                    "reasoning",
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Dog",
                    "(B) Panda",
                    "(C) Sheep",
                    "(D) Yak"
                ],
                "type": "documentary",
                "question": "Which animal does Kareem take care of?"
            },
            "target": "(D) Yak",
            "arguments": [
                "Which animal does Kareem take care of?\nA. (A) Dog\nB. (B) Panda\nC. (C) Sheep\nD. (D) Yak\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e5f0>"
                    ]
                },
                135,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "46fb0d349533bf940aa1e2875fadc8bd7f188610549738bc204c6495e1720eb6",
            "target_hash": "a80d209d6a42e162c66051db0e6c305b3853c2ca62788668c97e7eba7e6633ed",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 136,
            "doc": {
                "time_reference": "69:40-69:48",
                "video": "wNCPgIVz15c",
                "answer": "(D) They are trying to lasso the yak",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) They are trying to lasso the dog",
                    "(B) They are trying to lasso the cat",
                    "(C) They are trying to lasso the Panda",
                    "(D) They are trying to lasso the yak"
                ],
                "type": "documentary",
                "question": "What are the people holding the yellow ropes trying to do at the place that has trees with blooms?"
            },
            "target": "(D) They are trying to lasso the yak",
            "arguments": [
                "What are the people holding the yellow ropes trying to do at the place that has trees with blooms?\nA. (A) They are trying to lasso the dog\nB. (B) They are trying to lasso the cat\nC. (C) They are trying to lasso the Panda\nD. (D) They are trying to lasso the yak\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0a90>"
                    ]
                },
                136,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d25a16d16c3fbeab29921b2276e6e5132481237a6091e7a1e00887a76443f304",
            "target_hash": "2a9b9b29920234b358ca10d1484b78ed62960ce3fe226013557b8ada28229682",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 137,
            "doc": {
                "time_reference": "81:30-84:21",
                "video": "wNCPgIVz15c",
                "answer": "(A) The women and children are eating together",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) The women and children are eating together",
                    "(B) The men and women are eating together",
                    "(C) The men and children are eating together",
                    "(D) They are all eating separately"
                ],
                "type": "documentary",
                "question": "Are the men, women, and children in the valley eating together in the video?"
            },
            "target": "(A) The women and children are eating together",
            "arguments": [
                "Are the men, women, and children in the valley eating together in the video?\nA. (A) The women and children are eating together\nB. (B) The men and women are eating together\nC. (C) The men and children are eating together\nD. (D) They are all eating separately\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8c10>"
                    ]
                },
                137,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "a8213016e57b4e372d14c7b3d41c927d888c4a38b26cf834ab9ecf28c4d201e1",
            "target_hash": "25fd5d753e833e65d63644e8be7ca6b050f324fe4f9420158c0a5759c517abb4",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 138,
            "doc": {
                "time_reference": "01:17-05:07",
                "video": "uW9mcG0rdLY",
                "answer": "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior",
                "question_type": [
                    "summarization",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) The photographer has recorded the magnificent marine landscape. While seated in a pneumatic boat and diving, he captured scenes of pods of sperm whales active in the area",
                    "(B) The photographer captured scenes of pods of orca active in the marine area from aboard a cruise ship",
                    "(C) Many sperm whales are active in this sea region, and the photographer, seated in a helicopter, pursues them to film and document their behavior",
                    "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior"
                ],
                "type": "documentary",
                "question": "What are the key events or actions that occur from 01:17 - 05:07?"
            },
            "target": "(D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior",
            "arguments": [
                "What are the key events or actions that occur from 01:17 - 05:07?\nA. (A) The photographer has recorded the magnificent marine landscape. While seated in a pneumatic boat and diving, he captured scenes of pods of sperm whales active in the area\nB. (B) The photographer captured scenes of pods of orca active in the marine area from aboard a cruise ship\nC. (C) Many sperm whales are active in this sea region, and the photographer, seated in a helicopter, pursues them to film and document their behavior\nD. (D) The documentary captures the terrain and weather conditions of the filming location, where humans are fishing in the marine area. Numerous orca are active in this sea region, and the photographer, seated in a pneumatic boat, dives to pursue and film their behavior\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9c30>"
                    ]
                },
                138,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b86fb7d3909e04224f33e487eb717bd036bc5672bcb29191a2e8a5392e958baf",
            "target_hash": "bc98ee0b6b59360c32badbb0b2ffdab4cd0277cc8b73b2342e7b5ed7ecf02296",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 139,
            "doc": {
                "time_reference": "05:42-06:30",
                "video": "uW9mcG0rdLY",
                "answer": "(B) Transported to the island in a box via helicopter",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Transported to the destination in a box via cruise ship",
                    "(B) Transported to the island in a box via helicopter",
                    "(C) Transported to the island from an inflatable rubber boat",
                    "(D) Transported to the destination from the mainland by pickup truck"
                ],
                "type": "documentary",
                "question": "How are the photographer's equipment transported to the island?"
            },
            "target": "(B) Transported to the island in a box via helicopter",
            "arguments": [
                "How are the photographer's equipment transported to the island?\nA. (A) Transported to the destination in a box via cruise ship\nB. (B) Transported to the island in a box via helicopter\nC. (C) Transported to the island from an inflatable rubber boat\nD. (D) Transported to the destination from the mainland by pickup truck\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eace0>"
                    ]
                },
                139,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9160ff2e8b609cd566498a80cb43dd6689e7b7f524efda52e771f3b81f5198ec",
            "target_hash": "797a2f3298abeedafed85d354bb27d091d5fa62c36b2a590cc23c3cb69e9a958",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 140,
            "doc": {
                "time_reference": "08:48-11:16",
                "video": "uW9mcG0rdLY",
                "answer": "(D) Wildlife documentary",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Arctic expedition documentary",
                    "(B) Film on human-ocean themes",
                    "(C) Marine environment conservation documentary",
                    "(D) Wildlife documentary"
                ],
                "type": "documentary",
                "question": "What category or genre does this video clip belong to?"
            },
            "target": "(D) Wildlife documentary",
            "arguments": [
                "What category or genre does this video clip belong to?\nA. (A) Arctic expedition documentary\nB. (B) Film on human-ocean themes\nC. (C) Marine environment conservation documentary\nD. (D) Wildlife documentary\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebdc0>"
                    ]
                },
                140,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "7cff7ef193681bc7706f0f31d3430bdc61c59ac8fee2233f17ac72f3e86857a7",
            "target_hash": "dd6aceb3504ac36046f7608fd97d67606cbcf49fdd6348e7b9481afcebdc3df2",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 141,
            "doc": {
                "time_reference": "12:59-13:29",
                "video": "uW9mcG0rdLY",
                "answer": "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
                "question_type": [
                    "reasoning",
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) This person witnessed people hunting orca on the beach, so the person was angry",
                    "(B) This person saw an orca hunting elephant seals on the beach, so he attempted to drive away the orca",
                    "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
                    "(D) This person captured a rare sighting of an orca on the beach, so the person was happy"
                ],
                "type": "documentary",
                "question": "Why did the person jump up at the end of the clip that has a white reticle at the center?"
            },
            "target": "(C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy",
            "arguments": [
                "Why did the person jump up at the end of the clip that has a white reticle at the center?\nA. (A) This person witnessed people hunting orca on the beach, so the person was angry\nB. (B) This person saw an orca hunting elephant seals on the beach, so he attempted to drive away the orca\nC. (C) This person and their companions successfully helped a stranded orca return to the ocean, so the person was happy\nD. (D) This person captured a rare sighting of an orca on the beach, so the person was happy\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7cca0>"
                    ]
                },
                141,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6ae56c51ecd1dd6abed2d48e0635dd7d16512e9d65cc4ec30824ef4707779226",
            "target_hash": "9938ef843c5f92aa778d3d2d69c3148caf875a009942d436d409196962613d5e",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 142,
            "doc": {
                "time_reference": "18:55-19:15",
                "video": "uW9mcG0rdLY",
                "answer": "(D) Following the cameraperson",
                "question_type": [
                    "temporal grounding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Looking at the camera",
                    "(B) Following its own parents",
                    "(C) Preening its feathers",
                    "(D) Following the cameraperson"
                ],
                "type": "documentary",
                "question": "What are the baby penguins doing from 18:55-19:15?"
            },
            "target": "(D) Following the cameraperson",
            "arguments": [
                "What are the baby penguins doing from 18:55-19:15?\nA. (A) Looking at the camera\nB. (B) Following its own parents\nC. (C) Preening its feathers\nD. (D) Following the cameraperson\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d420>"
                    ]
                },
                142,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c03474c1424ec9a96fdd034273067fe55c57863b05f3fb5930636d65d4d87407",
            "target_hash": "b140df4cb805d6ee78e71311201f15cce2577e10c05c2e1380ea161e5772096b",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 143,
            "doc": {
                "time_reference": "19:34-21:18",
                "video": "uW9mcG0rdLY",
                "answer": "(C) Orcas prey on king penguins",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) A group of orcas swims playfully in the sea",
                    "(B) A group of king penguins swims playfully near the shore",
                    "(C) Orcas prey on king penguins",
                    "(D) King penguins gathers on the rocks for rest"
                ],
                "type": "documentary",
                "question": "What key event takes place between the first appearance of the baby penguins and the elephant seal sunbathing on the shore?"
            },
            "target": "(C) Orcas prey on king penguins",
            "arguments": [
                "What key event takes place between the first appearance of the baby penguins and the elephant seal sunbathing on the shore?\nA. (A) A group of orcas swims playfully in the sea\nB. (B) A group of king penguins swims playfully near the shore\nC. (C) Orcas prey on king penguins\nD. (D) King penguins gathers on the rocks for rest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e710>"
                    ]
                },
                143,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ac8fac780cbc169f63f1e14b95e34c953eab52fa03e2bee0a73452fd8f3484a2",
            "target_hash": "295312a29ea08832a32356c6ac47c6219b47c70782db2378e3deb84998206f73",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 144,
            "doc": {
                "time_reference": "21:56-22:19",
                "video": "uW9mcG0rdLY",
                "answer": "(A) It is knocked over by a wave",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) It is knocked over by a wave",
                    "(B) It is getting ready to enter the sea",
                    "(C) It is preparing to catch fish",
                    "(D) It just finished swimming and comes ashore"
                ],
                "type": "documentary",
                "question": "Why is the penguin lying down at 22:14?"
            },
            "target": "(A) It is knocked over by a wave",
            "arguments": [
                "Why is the penguin lying down at 22:14?\nA. (A) It is knocked over by a wave\nB. (B) It is getting ready to enter the sea\nC. (C) It is preparing to catch fish\nD. (D) It just finished swimming and comes ashore\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0a60>"
                    ]
                },
                144,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dffb49465811d81d1d229e2b61c550f249440ac543c96acc4e3b4a215f35133c",
            "target_hash": "138375eb0b58419a5a3ba78d45a9e66b86b8103d89d60b0540a396ade6a18d35",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 145,
            "doc": {
                "time_reference": "35:12-35:12",
                "video": "uW9mcG0rdLY",
                "answer": "(D) Recording the behavior of wild animals",
                "question_type": [
                    "reasoning",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Sketching landscapes",
                    "(B) Writing a travel diary",
                    "(C) Calculating the weather on the island",
                    "(D) Recording the behavior of wild animals"
                ],
                "type": "documentary",
                "question": "What is the man in the scene most likely doing while on the left side, there are a blue sweatshirt, a black T-shirt, a mint towel, and two black and red socks hanging on a rope to dry?"
            },
            "target": "(D) Recording the behavior of wild animals",
            "arguments": [
                "What is the man in the scene most likely doing while on the left side, there are a blue sweatshirt, a black T-shirt, a mint towel, and two black and red socks hanging on a rope to dry?\nA. (A) Sketching landscapes\nB. (B) Writing a travel diary\nC. (C) Calculating the weather on the island\nD. (D) Recording the behavior of wild animals\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8ee0>"
                    ]
                },
                145,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "bd750232735f0f42ad6e12d06ec90be9ab1dfc0e3475d5a8e7cf5ef32e875d21",
            "target_hash": "0a148343e4cc2f42345823b056580109188d68dd64ae5598b7400af1b2b57ed9",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 146,
            "doc": {
                "time_reference": "49:35-49:35",
                "video": "uW9mcG0rdLY",
                "answer": "(D) I'm not sure",
                "question_type": [
                    "key information retrieval",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Though I'm",
                    "(B) I thought",
                    "(C) I feel that",
                    "(D) I'm not sure"
                ],
                "type": "documentary",
                "question": "What is written at the beginning of the last line on the squared notebook?"
            },
            "target": "(D) I'm not sure",
            "arguments": [
                "What is written at the beginning of the last line on the squared notebook?\nA. (A) Though I'm\nB. (B) I thought\nC. (C) I feel that\nD. (D) I'm not sure\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9d50>"
                    ]
                },
                146,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8122fffdff5a9b48e0117441ef2086d98cba4d3f8c993cfb2e3740fd07e0b105",
            "target_hash": "613ad347cb86bf1df73d8df75b71deb278a6f4103deec937f9096f7f75ff0cd9",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 147,
            "doc": {
                "time_reference": "00:00-01:30",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(B) White",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Grey",
                    "(B) White",
                    "(C) Purple",
                    "(D) Beige"
                ],
                "type": "documentary",
                "question": "What is the color of the sleeveless top one of the four people at the beginning of the video wear?"
            },
            "target": "(B) White",
            "arguments": [
                "What is the color of the sleeveless top one of the four people at the beginning of the video wear?\nA. (A) Grey\nB. (B) White\nC. (C) Purple\nD. (D) Beige\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eae00>"
                    ]
                },
                147,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3b57a2bcee822c933e0e694de203f955abdfba0a6fda77cf6c1039fe993e47e9",
            "target_hash": "c74ceb844b82bd28b9fb46026cac5532ac0117b653aaa63d7c1671daa6630c7c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 148,
            "doc": {
                "time_reference": "02:46-02:46",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(A) Because rising sea levels caused the water to inundate the road",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Because rising sea levels caused the water to inundate the road",
                    "(B) Due to a burst pipe",
                    "(C) Because of melting ice",
                    "(D) Because of heavy rainfall"
                ],
                "type": "documentary",
                "question": "Why is there so much water on the road in the video?"
            },
            "target": "(A) Because rising sea levels caused the water to inundate the road",
            "arguments": [
                "Why is there so much water on the road in the video?\nA. (A) Because rising sea levels caused the water to inundate the road\nB. (B) Due to a burst pipe\nC. (C) Because of melting ice\nD. (D) Because of heavy rainfall\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebee0>"
                    ]
                },
                148,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "dcbf20b540c20b8af7910d3e091cb3d9bc187b2b79a5a20d96eea1002a9bcde6",
            "target_hash": "dcf5e7e0e845715f7b65e62b9cf289d1b2555b9a34046b92f20e6ab63d8864e1",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 149,
            "doc": {
                "time_reference": "05:13-05:59",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(A) To inform the audience that some areas here were not submerged by water in the past",
                "question_type": [
                    "reasoning",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) To inform the audience that some areas here were not submerged by water in the past",
                    "(B) To demonstrate their photography hobby",
                    "(C) To share memories of their childhood",
                    "(D) To showcase their cooking skills"
                ],
                "type": "documentary",
                "question": "What is the intention behind the residents showing the photos in the video?"
            },
            "target": "(A) To inform the audience that some areas here were not submerged by water in the past",
            "arguments": [
                "What is the intention behind the residents showing the photos in the video?\nA. (A) To inform the audience that some areas here were not submerged by water in the past\nB. (B) To demonstrate their photography hobby\nC. (C) To share memories of their childhood\nD. (D) To showcase their cooking skills\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb580>"
                    ]
                },
                149,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "adabead6b3dfc6501d5282ec1e7bdd167443e28245273467fa4cdac366ff6271",
            "target_hash": "96efdd2e45e06f0c941571d5d2732b0f3801f851a6572e0e41a567cb9c9ae014",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 150,
            "doc": {
                "time_reference": "05:51-05:53",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(A) Coconut",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Coconut",
                    "(B) Cherry",
                    "(C) Apple",
                    "(D) Blueberry"
                ],
                "type": "documentary",
                "question": "Which fruit is shown in the video?"
            },
            "target": "(A) Coconut",
            "arguments": [
                "Which fruit is shown in the video?\nA. (A) Coconut\nB. (B) Cherry\nC. (C) Apple\nD. (D) Blueberry\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c340>"
                    ]
                },
                150,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9ef107c7a69fed4fb0b011c872bab613dd34e315d7f83dab176ac9b116b6f61d",
            "target_hash": "ac9379dec4b12fd31db8780db303eba72b948f190b8924aeecc274c96b231e08",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 151,
            "doc": {
                "time_reference": "06:42-06:42",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(C) Grandmother and granddaughter",
                "question_type": [
                    "entity recognition",
                    "reasoning"
                ],
                "candidates": [
                    "(A) Mother and daughter",
                    "(B) School principle and student",
                    "(C) Grandmother and granddaughter",
                    "(D) Teacher and student"
                ],
                "type": "documentary",
                "question": "What is the relation between the girl wearing a school uniform and the woman wearing a floral top and a purple dress?"
            },
            "target": "(C) Grandmother and granddaughter",
            "arguments": [
                "What is the relation between the girl wearing a school uniform and the woman wearing a floral top and a purple dress?\nA. (A) Mother and daughter\nB. (B) School principle and student\nC. (C) Grandmother and granddaughter\nD. (D) Teacher and student\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e830>"
                    ]
                },
                151,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "24d6eb03c7b458f277acf4e79ba2c011a4b1ee1ea50faaa29182cca476bbda59",
            "target_hash": "5a740c7049e05a60c5df34c7a920c88341f7e35aa1ddaafa60eb6d57a342722e",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 152,
            "doc": {
                "time_reference": "22:04-22:13",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(C) Cantaloupe",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Honeydew",
                    "(B) Pumpkin",
                    "(C) Cantaloupe",
                    "(D) Watermelon"
                ],
                "type": "documentary",
                "question": "What type of melon is cut?"
            },
            "target": "(C) Cantaloupe",
            "arguments": [
                "What type of melon is cut?\nA. (A) Honeydew\nB. (B) Pumpkin\nC. (C) Cantaloupe\nD. (D) Watermelon\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0b50>"
                    ]
                },
                152,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "9b789dcbfeaec9d943d3ccc48a9f4c6829e2f732520c3e0196394e62f4346425",
            "target_hash": "8f28ce14b00027ab51ded961c61b447c5421344eb2c8c9430f5eb32e9d3f8bac",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 153,
            "doc": {
                "time_reference": "22:00-23:40",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(A) They are females",
                "question_type": [
                    "event understanding",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) They are females",
                    "(B) They sing while cooking",
                    "(C) They wear hats",
                    "(D) They use the same type of utensils"
                ],
                "type": "documentary",
                "question": "What do the people preparing food in the video have in common?"
            },
            "target": "(A) They are females",
            "arguments": [
                "What do the people preparing food in the video have in common?\nA. (A) They are females\nB. (B) They sing while cooking\nC. (C) They wear hats\nD. (D) They use the same type of utensils\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8dc0>"
                    ]
                },
                153,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "592ffc8007a90378c664745324fb9dc393ded2f1f33aaf89bff1d3f2fdcb3903",
            "target_hash": "df4e80e45ec9357363f5c0fb992982860c3bfd2920408e8a788125de073db6da",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 154,
            "doc": {
                "time_reference": "27:00-27:09",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(B) Yes",
                "question_type": [
                    "reasoning",
                    "event understanding"
                ],
                "candidates": [
                    "(A) No",
                    "(B) Yes",
                    "(C) Currently in contact with the outside world",
                    "(D) Not sure"
                ],
                "type": "documentary",
                "question": "Did the islanders accept help from the outside?"
            },
            "target": "(B) Yes",
            "arguments": [
                "Did the islanders accept help from the outside?\nA. (A) No\nB. (B) Yes\nC. (C) Currently in contact with the outside world\nD. (D) Not sure\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9e70>"
                    ]
                },
                154,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "2ed75602f1477303934c7105578aee51e07b116e1eb5e1aa83dee3ceccfa9314",
            "target_hash": "ca4dde3dc6a6db93e265e665454c4b09148d8a5106829faa234e1702d4369845",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 155,
            "doc": {
                "time_reference": "27:13-27:13",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(C) Riding a bicycle",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Running",
                    "(B) Walking",
                    "(C) Riding a bicycle",
                    "(D) Swimming"
                ],
                "type": "documentary",
                "question": "What is the person doing while passing by the sign that has \"Climate Change Adaptation Support Project Pilot Site\" written on it?"
            },
            "target": "(C) Riding a bicycle",
            "arguments": [
                "What is the person doing while passing by the sign that has \"Climate Change Adaptation Support Project Pilot Site\" written on it?\nA. (A) Running\nB. (B) Walking\nC. (C) Riding a bicycle\nD. (D) Swimming\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eaf20>"
                    ]
                },
                155,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4af61b91f7944b2e4ab71f9c1d6aea529b8a0b922edbbe7c5f18aa6541179c02",
            "target_hash": "8769d39764bc0358e6026c86cc8e7109d7c81d915b68d864e584b4da9c4f0b3c",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 156,
            "doc": {
                "time_reference": "28:13-28:13",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(D) 9619",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 9964",
                    "(B) 9946",
                    "(C) 9679",
                    "(D) 9619"
                ],
                "type": "documentary",
                "question": "What is the number of the student trainer that is further away from the camera compared to the 9772 student trainer?"
            },
            "target": "(D) 9619",
            "arguments": [
                "What is the number of the student trainer that is further away from the camera compared to the 9772 student trainer?\nA. (A) 9964\nB. (B) 9946\nC. (C) 9679\nD. (D) 9619\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebf10>"
                    ]
                },
                156,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "c4e81f78b74045d8da42580c4c59340e25b0903adbb91f0edc4de83d0d3ee51b",
            "target_hash": "438dde9bfce74bff2f83db2008c7dbcbe94ed271c40b0075a7558e04b4cf365b",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 157,
            "doc": {
                "time_reference": "31:00-31:21",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(C) In the dormitory",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) At the beach",
                    "(B) In the kitchen",
                    "(C) In the dormitory",
                    "(D) In the forest"
                ],
                "type": "documentary",
                "question": "Where is the person with the number 9615 on their chest being interviewed in the video?"
            },
            "target": "(C) In the dormitory",
            "arguments": [
                "Where is the person with the number 9615 on their chest being interviewed in the video?\nA. (A) At the beach\nB. (B) In the kitchen\nC. (C) In the dormitory\nD. (D) In the forest\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb5e0>"
                    ]
                },
                157,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "07b143656fa93123c7ff1bde66308a53d83dcd6eef8cb3d0ab2d38e92760d507",
            "target_hash": "36314713baa75a62c5f2f270a7da4aaf1b11b6995961330c43fbc81bdd737944",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 158,
            "doc": {
                "time_reference": "33:00-33:00",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(A) Yellow",
                "question_type": [
                    "entity recognition",
                    "reasoning"
                ],
                "candidates": [
                    "(A) Yellow",
                    "(B) White",
                    "(C) Black",
                    "(D) Blue"
                ],
                "type": "documentary",
                "question": "What color tops are most of the elementary school students wearing in the video?"
            },
            "target": "(A) Yellow",
            "arguments": [
                "What color tops are most of the elementary school students wearing in the video?\nA. (A) Yellow\nB. (B) White\nC. (C) Black\nD. (D) Blue\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dab0>"
                    ]
                },
                158,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ad8d81e0b4be1493e0b3fccffe737749d80f1e3d6c07b9cbc1f9a63cdc649223",
            "target_hash": "17db5ef3256df7c9256c75ba0516a35208ef27948344ccc3af6325bf80ace62e",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 159,
            "doc": {
                "time_reference": "33:33-33:34",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(C) They sit on the floor",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) They sit on tables",
                    "(B) They sit on chairs",
                    "(C) They sit on the floor",
                    "(D) They stand"
                ],
                "type": "documentary",
                "question": "Where do the elementary school students sit when the teacher is teaching?"
            },
            "target": "(C) They sit on the floor",
            "arguments": [
                "Where do the elementary school students sit when the teacher is teaching?\nA. (A) They sit on tables\nB. (B) They sit on chairs\nC. (C) They sit on the floor\nD. (D) They stand\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7e950>"
                    ]
                },
                159,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0b304a1a2f93329b100bdee66b0b71b238c3da00c4aa115651fcf65a443d8f71",
            "target_hash": "47c8cde8c4aa6750accb161bf30457f54dc501d6157991905d12d89aee4e4b77",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 160,
            "doc": {
                "time_reference": "38:05-38:58",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(B) They are fishing",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) They are hiking",
                    "(B) They are fishing",
                    "(C) They are hunting",
                    "(D) They are cooking"
                ],
                "type": "documentary",
                "question": "What are the people holding a torch in their hand doing at night?"
            },
            "target": "(B) They are fishing",
            "arguments": [
                "What are the people holding a torch in their hand doing at night?\nA. (A) They are hiking\nB. (B) They are fishing\nC. (C) They are hunting\nD. (D) They are cooking\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c12a0>"
                    ]
                },
                160,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ae30367298002272249b9d6b9b08ee2de3e6fb229b262a9a7e75da3ff73582ed",
            "target_hash": "8b26b6bb471d62297c2065601b49def0dbc19d8648cccca784a8aecfa821692f",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 161,
            "doc": {
                "time_reference": "39:21-39:21",
                "video": "TZ0j6kr4ZJ0",
                "answer": "(D) Yellow",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Blue",
                    "(B) Red",
                    "(C) Black",
                    "(D) Yellow"
                ],
                "type": "documentary",
                "question": "What color is the oxygen tank that appears the last in the video?"
            },
            "target": "(D) Yellow",
            "arguments": [
                "What color is the oxygen tank that appears the last in the video?\nA. (A) Blue\nB. (B) Red\nC. (C) Black\nD. (D) Yellow\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e90c0>"
                    ]
                },
                161,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "44e1248488027d12fe2d4fd8a7ffcc81f1d5426927dca7300382caf074b9331a",
            "target_hash": "0b742116fd1f4311d53c40fdbfbf2f5ef15ee874a453ec11b4e98eb3b1a02775",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 162,
            "doc": {
                "time_reference": "03:19-03:25",
                "video": "ihfjEFGdZdc",
                "answer": "(D) Stephan Winklemann",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Stephen Winklemann",
                    "(B) Chris Piochon",
                    "(C) Christophe Piochon",
                    "(D) Stephan Winklemann"
                ],
                "type": "documentary",
                "question": "Who is the president of Bugatti?"
            },
            "target": "(D) Stephan Winklemann",
            "arguments": [
                "Who is the president of Bugatti?\nA. (A) Stephen Winklemann\nB. (B) Chris Piochon\nC. (C) Christophe Piochon\nD. (D) Stephan Winklemann\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9f90>"
                    ]
                },
                162,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "69dd0598f4d41fd5f0f997b74617f146501581386138d340c6802bcac81cf2c1",
            "target_hash": "2987ed5f1705bdd628cd6cae017f63df3eca7c8debdef4f85d3be495b54dc21c",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 163,
            "doc": {
                "time_reference": "04:18-04:32",
                "video": "ihfjEFGdZdc",
                "answer": "(D) Black",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Blue",
                    "(B) White",
                    "(C) Grey",
                    "(D) Black"
                ],
                "type": "documentary",
                "question": "What color are the staff uniforms of Bugatti workers?"
            },
            "target": "(D) Black",
            "arguments": [
                "What color are the staff uniforms of Bugatti workers?\nA. (A) Blue\nB. (B) White\nC. (C) Grey\nD. (D) Black\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb040>"
                    ]
                },
                163,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d85e40b02a8788def310fc67dd8b62ef588ab4f185823ec15b7f3ebfb53d4e51",
            "target_hash": "1def1ffd02b2328d50ae6a5baad3bbc3bf9cde1e18e001fe55fdc513f415383a",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 164,
            "doc": {
                "time_reference": "05:19-05:33",
                "video": "ihfjEFGdZdc",
                "answer": "(D) They put the car shell on the chassis",
                "question_type": [
                    "event understanding",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) They stand on the chassis",
                    "(B) They put car parts on the chassis",
                    "(C) They put the tools on the chassis",
                    "(D) They put the car shell on the chassis"
                ],
                "type": "documentary",
                "question": "What do workers do after they put up the chassis in 05:19?"
            },
            "target": "(D) They put the car shell on the chassis",
            "arguments": [
                "What do workers do after they put up the chassis in 05:19?\nA. (A) They stand on the chassis\nB. (B) They put car parts on the chassis\nC. (C) They put the tools on the chassis\nD. (D) They put the car shell on the chassis\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb070>"
                    ]
                },
                164,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "552bf9e6cb29bf4de59df49f41ec10fec348fcc065e42ca1ad0c36af801b1da3",
            "target_hash": "a48ae33a6797560eaa68bbc8a42548b64739940ce83ecda937f424a4250abdef",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 165,
            "doc": {
                "time_reference": "07:30-14:12",
                "video": "ihfjEFGdZdc",
                "answer": "(D) They are assembling the engine",
                "question_type": [
                    "temporal grounding",
                    "event understanding"
                ],
                "candidates": [
                    "(A) They are testing different engines",
                    "(B) They are putting the engine into the car",
                    "(C) They are installing the turbochargers",
                    "(D) They are assembling the engine"
                ],
                "type": "documentary",
                "question": "What are the workers doing from 07:30-14:12?"
            },
            "target": "(D) They are assembling the engine",
            "arguments": [
                "What are the workers doing from 07:30-14:12?\nA. (A) They are testing different engines\nB. (B) They are putting the engine into the car\nC. (C) They are installing the turbochargers\nD. (D) They are assembling the engine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c400>"
                    ]
                },
                165,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cf20a9df8c82ed04c04ce5140f35276fecdce9a10c85299120648af6c9db0e52",
            "target_hash": "950f19e123128a541aa6a24c36866570b9eddf3650ed43d48d7e2e818266eebc",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 166,
            "doc": {
                "time_reference": "18:59-20:30",
                "video": "ihfjEFGdZdc",
                "answer": "(A) He is testing the sound of the engine",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) He is testing the sound of the engine",
                    "(B) He is testing the exhaust gas from the engine",
                    "(C) He is testing the temperature of the engine",
                    "(D) He is testing the power of the engine"
                ],
                "type": "documentary",
                "question": "What does Dr. Conrad test?"
            },
            "target": "(A) He is testing the sound of the engine",
            "arguments": [
                "What does Dr. Conrad test?\nA. (A) He is testing the sound of the engine\nB. (B) He is testing the exhaust gas from the engine\nC. (C) He is testing the temperature of the engine\nD. (D) He is testing the power of the engine\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dbd0>"
                    ]
                },
                166,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ef7e90ad0f159dc16f2d7a7a15c5c44f6d86581b510427bd552b9deb099a4d8e",
            "target_hash": "7efdd86052b8da63e9aa1d4fb38c832402469d36130c7559a3accff02bf58b3c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 167,
            "doc": {
                "time_reference": "20:40-20:50",
                "video": "ihfjEFGdZdc",
                "answer": "(A) He puts on his coat",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) He puts on his coat",
                    "(B) He checks his car",
                    "(C) He takes off his coat",
                    "(D) He finds his car key"
                ],
                "type": "documentary",
                "question": "What does the test driver do before he enters the car with license plate \"W392DB\"?"
            },
            "target": "(A) He puts on his coat",
            "arguments": [
                "What does the test driver do before he enters the car with license plate \"W392DB\"?\nA. (A) He puts on his coat\nB. (B) He checks his car\nC. (C) He takes off his coat\nD. (D) He finds his car key\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7ea70>"
                    ]
                },
                167,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5d2b28cb2311b23cedf6dcc41e088422c4c1f36f2abfabd254851598df086d77",
            "target_hash": "23259aced898d1fd9c1178ccef2f9d59c7cb57256f53175b10f77537b46707cf",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 168,
            "doc": {
                "time_reference": "21:35-21:50",
                "video": "ihfjEFGdZdc",
                "answer": "(D) Black with two orange stripes on the top",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black with three orange stripes on the top",
                    "(B) Orange with two black stripes on the top",
                    "(C) Orange with three black stripes on the top",
                    "(D) Black with two orange stripes on the top"
                ],
                "type": "documentary",
                "question": "What colour is the car Andy test?"
            },
            "target": "(D) Black with two orange stripes on the top",
            "arguments": [
                "What colour is the car Andy test?\nA. (A) Black with three orange stripes on the top\nB. (B) Orange with two black stripes on the top\nC. (C) Orange with three black stripes on the top\nD. (D) Black with two orange stripes on the top\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0850>"
                    ]
                },
                168,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "1f8c5c5e3c4aae25b2c8150c4f169212806348e96d5434977814bcac5069d955",
            "target_hash": "77330d9ae729ee0e32cffc25bdd6ca76c3f96bbbb8640112ca999e5463e07c70",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 169,
            "doc": {
                "time_reference": "21:35-23:34",
                "video": "ihfjEFGdZdc",
                "answer": "(D) 304.77 MPH",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 304.47MPH",
                    "(B) 307.47MPH",
                    "(C) 307.44 MPH",
                    "(D) 304.77 MPH"
                ],
                "type": "documentary",
                "question": "What is Andy's top speed while testing the sports car?"
            },
            "target": "(D) 304.77 MPH",
            "arguments": [
                "What is Andy's top speed while testing the sports car?\nA. (A) 304.47MPH\nB. (B) 307.47MPH\nC. (C) 307.44 MPH\nD. (D) 304.77 MPH\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9150>"
                    ]
                },
                169,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0f19ee2ed5254dede5f54b42b42938e321ec3bebd6d1f3c059fa4dc5c37df875",
            "target_hash": "75b34cbb78d4acf3cbb41ecc7cff074a1a53ca9924a3b7185c04a3977275b146",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 170,
            "doc": {
                "time_reference": "23:39-23:50",
                "video": "ihfjEFGdZdc",
                "answer": "(C) Because Andy drives to a very fast speed",
                "question_type": [
                    "reasoning"
                ],
                "candidates": [
                    "(A) Because they really like the car",
                    "(B) Because Andy shows off his skills during the test drive",
                    "(C) Because Andy drives to a very fast speed",
                    "(D) Because there is no accident during the test drive"
                ],
                "type": "documentary",
                "question": "Why is everyone celebrating after the test drive is over?"
            },
            "target": "(C) Because Andy drives to a very fast speed",
            "arguments": [
                "Why is everyone celebrating after the test drive is over?\nA. (A) Because they really like the car\nB. (B) Because Andy shows off his skills during the test drive\nC. (C) Because Andy drives to a very fast speed\nD. (D) Because there is no accident during the test drive\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea0b0>"
                    ]
                },
                170,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "b8e7ef9270c21a3f0e17ae33542a362cd82907f4461b520f6eed2fd667e7a3bd",
            "target_hash": "83ad8ae37547fcb5db998949ec9f93db76f796f53a0e12fca533867a7bfb27c0",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 171,
            "doc": {
                "time_reference": "32:28-33:08",
                "video": "ihfjEFGdZdc",
                "answer": "(B) Rely on manpower",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Controlled by computer",
                    "(B) Rely on manpower",
                    "(C) Rely on a forklift",
                    "(D) Rely on gravity"
                ],
                "type": "documentary",
                "question": "How are the two parts, one for the front seats and one for the back seats, put together and then assembled?"
            },
            "target": "(B) Rely on manpower",
            "arguments": [
                "How are the two parts, one for the front seats and one for the back seats, put together and then assembled?\nA. (A) Controlled by computer\nB. (B) Rely on manpower\nC. (C) Rely on a forklift\nD. (D) Rely on gravity\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb160>"
                    ]
                },
                171,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d117c9fef6c335f656724c05d3c8cf89508b571bcd3ade304837ab4f902faa89",
            "target_hash": "3a83495094fd9a84a70b1b16cb9d4c82d142e9b919fc19c38a5b6a5d8d399c0b",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 172,
            "doc": {
                "time_reference": "34:31-35:12",
                "video": "ihfjEFGdZdc",
                "answer": "(C) Right rear wheel",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Left rear wheel",
                    "(B) Left front wheel",
                    "(C) Right rear wheel",
                    "(D) Right front wheel"
                ],
                "type": "documentary",
                "question": "Which tire is put on first during assembly?"
            },
            "target": "(C) Right rear wheel",
            "arguments": [
                "Which tire is put on first during assembly?\nA. (A) Left rear wheel\nB. (B) Left front wheel\nC. (C) Right rear wheel\nD. (D) Right front wheel\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ead10>"
                    ]
                },
                172,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "352625a42bc4c2d779f57c81311062fe5bdaee7553510f58040d88f25f09e6dc",
            "target_hash": "2d22a705e874befebffbcab4f3753152658cb305977552b47e7a4c36b184a434",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 173,
            "doc": {
                "time_reference": "35:00-36:00",
                "video": "ihfjEFGdZdc",
                "answer": "(A) Add various fluids to the car, including engine oil and gasoline",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) Add various fluids to the car, including engine oil and gasoline",
                    "(B) Install other casings on the car",
                    "(C) Test drive",
                    "(D) Power up the car"
                ],
                "type": "documentary",
                "question": "What is the installation process after installing the tires?"
            },
            "target": "(A) Add various fluids to the car, including engine oil and gasoline",
            "arguments": [
                "What is the installation process after installing the tires?\nA. (A) Add various fluids to the car, including engine oil and gasoline\nB. (B) Install other casings on the car\nC. (C) Test drive\nD. (D) Power up the car\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c850>"
                    ]
                },
                173,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "5af9ccfb1d1fd958d5b3d65c458789d56316fd7938cf4b8da3f0f6e327e6241b",
            "target_hash": "5328492e52f7cf03635b94978d93bd86791d3b84a334c484fc433ccd569c7467",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 174,
            "doc": {
                "time_reference": "38:14-38:02",
                "video": "ihfjEFGdZdc",
                "answer": "(B) Maxime Bohn",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) Maxium Born",
                    "(B) Maxime Bohn",
                    "(C) Maxime Born",
                    "(D) Maxium Bohn"
                ],
                "type": "documentary",
                "question": "Who is the assembly editor of Bugatti?"
            },
            "target": "(B) Maxime Bohn",
            "arguments": [
                "Who is the assembly editor of Bugatti?\nA. (A) Maxium Born\nB. (B) Maxime Bohn\nC. (C) Maxime Born\nD. (D) Maxium Bohn\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7dcf0>"
                    ]
                },
                174,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d8620c990a84f60cdcf7183f36509d32823038a46e8743b6fb83753ab1360796",
            "target_hash": "b2370d36e88d43d4419e9b3d677b908466c37333daedaf2315872cfe55266fa5",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 175,
            "doc": {
                "time_reference": "38:25-39:27",
                "video": "ihfjEFGdZdc",
                "answer": "(D) White",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Black",
                    "(B) Silver",
                    "(C) Gold",
                    "(D) White"
                ],
                "type": "documentary",
                "question": "What color is the car they assemble?"
            },
            "target": "(D) White",
            "arguments": [
                "What color is the car they assemble?\nA. (A) Black\nB. (B) Silver\nC. (C) Gold\nD. (D) White\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7eb90>"
                    ]
                },
                175,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6112df299a0a0cc2ba6b3ed635dfd5855f4570e83dc1691c0b005389b8ad8454",
            "target_hash": "ca4aeab00311a07ae35d4347bff25bdba0921bc404e350d70d22d507d40243dd",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 176,
            "doc": {
                "time_reference": "41:58-42:10",
                "video": "ihfjEFGdZdc",
                "answer": "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
                "question_type": [
                    "reasoning",
                    "temporal grounding"
                ],
                "candidates": [
                    "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
                    "(B) The blue dotted line outlines the shape of the front half of the car, and the red dotted line outlines the shape of the rear half of the car",
                    "(C) The blue dotted line represents where reach the wind and the red dotted line represents where will not hit by wind",
                    "(D) The blue dotted line represents strong wind and the red dotted line represents weak wind"
                ],
                "type": "documentary",
                "question": "What do the blue and red dotted lines in the animation represent at 42:04?"
            },
            "target": "(A) The blue dotted line represents cold wind and the red dotted line represents hot wind",
            "arguments": [
                "What do the blue and red dotted lines in the animation represent at 42:04?\nA. (A) The blue dotted line represents cold wind and the red dotted line represents hot wind\nB. (B) The blue dotted line outlines the shape of the front half of the car, and the red dotted line outlines the shape of the rear half of the car\nC. (C) The blue dotted line represents where reach the wind and the red dotted line represents where will not hit by wind\nD. (D) The blue dotted line represents strong wind and the red dotted line represents weak wind\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1210>"
                    ]
                },
                176,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ff9e6808d218e6862ddadc3a0a0e93ac4f2fbfa5f23052ce9e09fc5f9f01a409",
            "target_hash": "1cce37222f3603eeeddb3ec93c5da943e4ad05d3e007e54ad774d50b6674f575",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 177,
            "doc": {
                "time_reference": "47:02-48:15",
                "video": "ihfjEFGdZdc",
                "answer": "(C) Cloudy",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Snowy",
                    "(B) Sunny",
                    "(C) Cloudy",
                    "(D) Rainny"
                ],
                "type": "documentary",
                "question": "What is the weather when the last test drive?"
            },
            "target": "(C) Cloudy",
            "arguments": [
                "What is the weather when the last test drive?\nA. (A) Snowy\nB. (B) Sunny\nC. (C) Cloudy\nD. (D) Rainny\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9270>"
                    ]
                },
                177,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "556bb0c0b34d2009bddf95dc691be97942a88f2baf700510794f544135d2aeec",
            "target_hash": "396cefd975128fde7cba4b28cc7d000982f723effec5849e69fc9248de00f1e0",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 178,
            "doc": {
                "time_reference": "00:00-49:04",
                "video": "ihfjEFGdZdc",
                "answer": "(B) 10",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 13",
                    "(B) 10",
                    "(C) 7",
                    "(D) 16"
                ],
                "type": "documentary",
                "question": "How many people are interviewed in this video?"
            },
            "target": "(B) 10",
            "arguments": [
                "How many people are interviewed in this video?\nA. (A) 13\nB. (B) 10\nC. (C) 7\nD. (D) 16\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea1d0>"
                    ]
                },
                178,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "0f32ed60ea9370dd68a7af6e45399921dfb67daecff2d44be65c722f6a0cce3a",
            "target_hash": "f3db718437d9244cf730435bce31c8879a5d14a75c8156262698c2dc0edd786c",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 179,
            "doc": {
                "time_reference": "06:01-06:13",
                "video": "vaL_vSdZKZo",
                "answer": "(C) Pearl style earrings",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Star style earrings",
                    "(B) Diamond earrings",
                    "(C) Pearl style earrings",
                    "(D) Rose style earrings"
                ],
                "type": "documentary",
                "question": "What type of earrings does the woman who sits at the steering wheel wear?"
            },
            "target": "(C) Pearl style earrings",
            "arguments": [
                "What type of earrings does the woman who sits at the steering wheel wear?\nA. (A) Star style earrings\nB. (B) Diamond earrings\nC. (C) Pearl style earrings\nD. (D) Rose style earrings\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb280>"
                    ]
                },
                179,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "684ff9b375021604fc3e66a085bff7cddb8984dac190c363805be3d35647d7e5",
            "target_hash": "bef77d6945f11602f05f6ab6da9c0191a82f5704ed1def3867ea78eb15ec08d9",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 180,
            "doc": {
                "time_reference": "06:17-06:27",
                "video": "vaL_vSdZKZo",
                "answer": "(D) She wear a hard hat and shoot into the sea with a gun",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) She quickly picked up a gun and shot at the sea",
                    "(B) She put on sunglasses and shot at the sea with a gun",
                    "(C) She came back after touring the ship's safety problems",
                    "(D) She wear a hard hat and shoot into the sea with a gun"
                ],
                "type": "documentary",
                "question": "What did the woman with the pearl earrings do when she got down the ladder?"
            },
            "target": "(D) She wear a hard hat and shoot into the sea with a gun",
            "arguments": [
                "What did the woman with the pearl earrings do when she got down the ladder?\nA. (A) She quickly picked up a gun and shot at the sea\nB. (B) She put on sunglasses and shot at the sea with a gun\nC. (C) She came back after touring the ship's safety problems\nD. (D) She wear a hard hat and shoot into the sea with a gun\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ebac0>"
                    ]
                },
                180,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "f04420b9d8f4716cb6ff40df2f390537aeeb876e324537da04972331a2cc5e50",
            "target_hash": "a732ab9534bb3cc44686b05cc97bad0f09cf4ac1148363e9e3b4c0cd01aadcb2",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 181,
            "doc": {
                "time_reference": "29:33-29:48",
                "video": "vaL_vSdZKZo",
                "answer": "(D) Telescope",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Interphone",
                    "(B) Telephone",
                    "(C) Remote control",
                    "(D) Telescope"
                ],
                "type": "documentary",
                "question": "What's the man in black short sleeves holding?"
            },
            "target": "(D) Telescope",
            "arguments": [
                "What's the man in black short sleeves holding?\nA. (A) Interphone\nB. (B) Telephone\nC. (C) Remote control\nD. (D) Telescope\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7c3d0>"
                    ]
                },
                181,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "632c76babc9555c03c558f64d3ef0862c72fe344f14bf79b918d3ad683526bc0",
            "target_hash": "6ca9439e646c2d7a64707c169a0de3346166097e38b95928182a0c39c11a24d7",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 182,
            "doc": {
                "time_reference": "31:15-31:16",
                "video": "vaL_vSdZKZo",
                "answer": "(A) 3",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 3",
                    "(B) 2",
                    "(C) 1",
                    "(D) 4"
                ],
                "type": "documentary",
                "question": "How many people are there in the descending elevator?"
            },
            "target": "(A) 3",
            "arguments": [
                "How many people are there in the descending elevator?\nA. (A) 3\nB. (B) 2\nC. (C) 1\nD. (D) 4\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7de10>"
                    ]
                },
                182,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "47f00eb9bcf8b31974c9d40601a0fb352491fa07343ac1e75a76a16b1204aa3b",
            "target_hash": "9dd873a4c7f0a878dd73cc9b87bf5615b5daa1a2e6d25c9364d84c064d1b5854",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 183,
            "doc": {
                "time_reference": "31:20-31:44",
                "video": "vaL_vSdZKZo",
                "answer": "(D) ROGER MIDDLETON",
                "question_type": [
                    "entity recognition",
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) SERGEY SARANCHA",
                    "(B) PAUL AGATE",
                    "(C) ALI ARIF",
                    "(D) ROGER MIDDLETON"
                ],
                "type": "documentary",
                "question": "Who was the man who spoke between LORETTA NAPOLEONI and ATEPHEN ASKINS?"
            },
            "target": "(D) ROGER MIDDLETON",
            "arguments": [
                "Who was the man who spoke between LORETTA NAPOLEONI and ATEPHEN ASKINS?\nA. (A) SERGEY SARANCHA\nB. (B) PAUL AGATE\nC. (C) ALI ARIF\nD. (D) ROGER MIDDLETON\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7ecb0>"
                    ]
                },
                183,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "D"
                ]
            ],
            "filtered_resps": [
                "D"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "6371916af37e8afd48ab528bae158acf6a54b1d5ad2121c5c8aab6f8a31ebdd9",
            "target_hash": "bf7bd2d11a5154d99d2b94d5caf2f4ab1b1f0cbf31aa7969bee92afd19232bf6",
            "lvbench_mc_accuracy": {
                "pred_answer": "D",
                "gt_answer": "D",
                "score": 1
            }
        },
        {
            "doc_id": 184,
            "doc": {
                "time_reference": "32:01-32:05",
                "video": "vaL_vSdZKZo",
                "answer": "(A) 33",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 33",
                    "(B) 03",
                    "(C) 30",
                    "(D) 32"
                ],
                "type": "documentary",
                "question": "What was written on the right glass door when Paul Agate entered the front door?"
            },
            "target": "(A) 33",
            "arguments": [
                "What was written on the right glass door when Paul Agate entered the front door?\nA. (A) 33\nB. (B) 03\nC. (C) 30\nD. (D) 32\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c1630>"
                    ]
                },
                184,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "8eea90bb54a8849e49e13590f0bd62f6b9d8d49af70b7c5a5c56ec050a137b76",
            "target_hash": "3d56b3fa17bb28049479b4c1910eb05e324cea0d186af102f6192d91f204a310",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 185,
            "doc": {
                "time_reference": "32:51-33:00",
                "video": "vaL_vSdZKZo",
                "answer": "(C) 11",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 10",
                    "(B) 12",
                    "(C) 11",
                    "(D) 13"
                ],
                "type": "documentary",
                "question": "How many people dragged the blue and white wooden boat ashore at the same time?"
            },
            "target": "(C) 11",
            "arguments": [
                "How many people dragged the blue and white wooden boat ashore at the same time?\nA. (A) 10\nB. (B) 12\nC. (C) 11\nD. (D) 13\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9390>"
                    ]
                },
                185,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "be8c87138b5951bd641f64b3667ffb652520643df745d8a699931bd171c16e72",
            "target_hash": "653a00f9bbed6bafe4ef76a29a9ef720dd8998d86d4b1b1de989e516cc3fe8e4",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "C",
                "score": 1
            }
        },
        {
            "doc_id": 186,
            "doc": {
                "time_reference": "39:16-39:25",
                "video": "vaL_vSdZKZo",
                "answer": "(B) 25",
                "question_type": [
                    "key information retrieval"
                ],
                "candidates": [
                    "(A) 18",
                    "(B) 25",
                    "(C) 5",
                    "(D) 10"
                ],
                "type": "documentary",
                "question": "What number is highlighted with rectangle in the chart written with pencil in numerical increasing order for each name?"
            },
            "target": "(B) 25",
            "arguments": [
                "What number is highlighted with rectangle in the chart written with pencil in numerical increasing order for each name?\nA. (A) 18\nB. (B) 25\nC. (C) 5\nD. (D) 10\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea2f0>"
                    ]
                },
                186,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "28ecb598c64b3e74ed894d907553d24a4a0c51f7eddaf2b81b994889d8d3bf6f",
            "target_hash": "441a28e650a9ea5bb0939a76b079b1ad5b9d38b0b8da13c5fcefa87bf7488e61",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 187,
            "doc": {
                "time_reference": "41:28-41:35",
                "video": "vaL_vSdZKZo",
                "answer": "(B) There are five floors",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) There are eight floors",
                    "(B) There are five floors",
                    "(C) There are six floors",
                    "(D) There are four floors"
                ],
                "type": "documentary",
                "question": "Before Svetlana McGlaser spoke, how many floors did the building in the video have?"
            },
            "target": "(B) There are five floors",
            "arguments": [
                "Before Svetlana McGlaser spoke, how many floors did the building in the video have?\nA. (A) There are eight floors\nB. (B) There are five floors\nC. (C) There are six floors\nD. (D) There are four floors\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb3a0>"
                    ]
                },
                187,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "3c02a881975b8bf1b1fa473cdc50b98b0b5f1182b4b810ede5e3fd8de4d30271",
            "target_hash": "9a4ce3dadaf73e80fe3d59bf4e6c1b2f6e2f9eecfdf9f57d228a638d21780d09",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "B",
                "score": 0
            }
        },
        {
            "doc_id": 188,
            "doc": {
                "time_reference": "45:37-45:42",
                "video": "vaL_vSdZKZo",
                "answer": "(D) Green",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) White",
                    "(B) Yellow",
                    "(C) Red",
                    "(D) Green"
                ],
                "type": "documentary",
                "question": "What is the background color of \"EXIT\" on the door in the picture when the bearded man speaks?"
            },
            "target": "(D) Green",
            "arguments": [
                "What is the background color of \"EXIT\" on the door in the picture when the bearded man speaks?\nA. (A) White\nB. (B) Yellow\nC. (C) Red\nD. (D) Green\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e9d80>"
                    ]
                },
                188,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "84f18e0fb3b062f97e8800b6cc66e97e8547e96473d3d3f0c87ca91fee871c46",
            "target_hash": "1467faa1684c871eccb61d0fe03dff196c5c3cb3e35d92428a306428f10f86fc",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 189,
            "doc": {
                "time_reference": "47:26-47:40",
                "video": "vaL_vSdZKZo",
                "answer": "(B) A man named Hotten fired a shot",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) A man named Hotten wears goggles",
                    "(B) A man named Hotten fired a shot",
                    "(C) A man named Helen fired a shot",
                    "(D) A man named Helen put on goggles"
                ],
                "type": "documentary",
                "question": "What happens in the next frame when the red helicopter flies by?"
            },
            "target": "(B) A man named Hotten fired a shot",
            "arguments": [
                "What happens in the next frame when the red helicopter flies by?\nA. (A) A man named Hotten wears goggles\nB. (B) A man named Hotten fired a shot\nC. (C) A man named Helen fired a shot\nD. (D) A man named Helen put on goggles\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7d060>"
                    ]
                },
                189,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "B",
                    "B"
                ]
            ],
            "filtered_resps": [
                "B"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e37a4b8462407d427091a225dc08da33242e3b39b65950cb1b6830ba0b9707b3",
            "target_hash": "163781e77e956efcd5b4db36e1518d79cc9a57b7261a2d846513a2032043232c",
            "lvbench_mc_accuracy": {
                "pred_answer": "B",
                "gt_answer": "B",
                "score": 1
            }
        },
        {
            "doc_id": 190,
            "doc": {
                "time_reference": "48:12-48:20",
                "video": "vaL_vSdZKZo",
                "answer": "(D) 1",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) 3",
                    "(B) 4",
                    "(C) 6",
                    "(D) 1"
                ],
                "type": "documentary",
                "question": "Before GEIR-OVE spoke, how many of the runners had bottled water?"
            },
            "target": "(D) 1",
            "arguments": [
                "Before GEIR-OVE spoke, how many of the runners had bottled water?\nA. (A) 3\nB. (B) 4\nC. (C) 6\nD. (D) 1\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7df30>"
                    ]
                },
                190,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A",
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "e1c90b5b4cb8540502d114735316c4402762092d3a501e71192244eb1dca7a57",
            "target_hash": "130e553bd423b938999fb170663c3db89fe536737e0598ae8b8ff9079e800be9",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "D",
                "score": 0
            }
        },
        {
            "doc_id": 191,
            "doc": {
                "time_reference": "51:47-52:18",
                "video": "vaL_vSdZKZo",
                "answer": "(A) He clinked glasses with everyone",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) He clinked glasses with everyone",
                    "(B) He helped the others with their dishes",
                    "(C) He took off his braces and gargled",
                    "(D) He distributed chopsticks to everyone"
                ],
                "type": "documentary",
                "question": "What did Sergey Sarancha do before eating?"
            },
            "target": "(A) He clinked glasses with everyone",
            "arguments": [
                "What did Sergey Sarancha do before eating?\nA. (A) He clinked glasses with everyone\nB. (B) He helped the others with their dishes\nC. (C) He took off his braces and gargled\nD. (D) He distributed chopsticks to everyone\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f5a0aa7edd0>"
                    ]
                },
                191,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C",
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "d46dd8f9b871d812f711bf813ab77d1b89180c1d4977a6044195c61b26094fd8",
            "target_hash": "65756e8ee178cfa13774408f31eabb354c71d2029c74876029c1f24d71c34d15",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 192,
            "doc": {
                "time_reference": "52:40-52:31",
                "video": "vaL_vSdZKZo",
                "answer": "(A) A pair of glasses",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) A pair of glasses",
                    "(B) A cup of coffee",
                    "(C) A glass of milk",
                    "(D) A pair of gloves"
                ],
                "type": "documentary",
                "question": "What did the man interviewed after Sergey Sarancha hold in his hand?"
            },
            "target": "(A) A pair of glasses",
            "arguments": [
                "What did the man interviewed after Sergey Sarancha hold in his hand?\nA. (A) A pair of glasses\nB. (B) A cup of coffee\nC. (C) A glass of milk\nD. (D) A pair of gloves\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6eda1c0430>"
                    ]
                },
                192,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "4b71fe888d1612227f8cbf796a566cc69b65a4cc2a04395437da458f5ab2e117",
            "target_hash": "51fea79d7282348557a66b867f025739f66fa370139f64e948c9ecc262cd5016",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "A",
                "score": 1
            }
        },
        {
            "doc_id": 193,
            "doc": {
                "time_reference": "52:52-52:54",
                "video": "vaL_vSdZKZo",
                "answer": "(A) Five-pointed star",
                "question_type": [
                    "entity recognition"
                ],
                "candidates": [
                    "(A) Five-pointed star",
                    "(B) Flower",
                    "(C) Pochacco",
                    "(D) Stlch"
                ],
                "type": "documentary",
                "question": "What is the tattoo pattern on the arm of the woman who is using a telescope wearing a bracelet?"
            },
            "target": "(A) Five-pointed star",
            "arguments": [
                "What is the tattoo pattern on the arm of the woman who is using a telescope wearing a bracelet?\nA. (A) Five-pointed star\nB. (B) Flower\nC. (C) Pochacco\nD. (D) Stlch\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e94b0>"
                    ]
                },
                193,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "393eb59310ef5cdb958040c4ede80a2a5400518d3467f55273cbcbdde0a62c0c",
            "target_hash": "b15b56ac146de765c85864751a1ffa06347154b79e7de306d8d712eb893d03e3",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 194,
            "doc": {
                "time_reference": "54:18-54:27",
                "video": "vaL_vSdZKZo",
                "answer": "(A) No smoking",
                "question_type": [
                    "key information retrieval",
                    "entity recognition"
                ],
                "candidates": [
                    "(A) No smoking",
                    "(B) Safety first",
                    "(C) Don't be late",
                    "(D) Beware of danger"
                ],
                "type": "documentary",
                "question": "What was written on the building behind Arve Nordeide when he came from a distance?"
            },
            "target": "(A) No smoking",
            "arguments": [
                "What was written on the building behind Arve Nordeide when he came from a distance?\nA. (A) No smoking\nB. (B) Safety first\nC. (C) Don't be late\nD. (D) Beware of danger\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80ea410>"
                    ]
                },
                194,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "ae15b7511f29dfe4b07e7e89160501764fc828b7f6b140ad066a1a23ebd89d62",
            "target_hash": "d9da720eb1d622b51b76d6288bea2b8dd9469de791f87c36fa56520e4d2a29ed",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "A",
                "score": 0
            }
        },
        {
            "doc_id": 195,
            "doc": {
                "time_reference": "55:41-55:43",
                "video": "vaL_vSdZKZo",
                "answer": "(C) Press conference",
                "question_type": [
                    "reasoning",
                    "event understanding"
                ],
                "candidates": [
                    "(A) Signature ceremony",
                    "(B) Crew shooting",
                    "(C) Press conference",
                    "(D) Examination room"
                ],
                "type": "documentary",
                "question": "What is the occasion right after the last interview scene with Mohamed Abshir Waldo?"
            },
            "target": "(C) Press conference",
            "arguments": [
                "What is the occasion right after the last interview scene with Mohamed Abshir Waldo?\nA. (A) Signature ceremony\nB. (B) Crew shooting\nC. (C) Press conference\nD. (D) Examination room\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80eb460>"
                    ]
                },
                195,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "A"
                ]
            ],
            "filtered_resps": [
                "A"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "cc64bc30f20eea7891f19ac69bd1a089aa2ef2337b0f6b23ec35de9bfd75bd06",
            "target_hash": "931db6de9b4e280a0e8cb359c6e6f663dd8390c581a335e06bba1a36fc0d76fb",
            "lvbench_mc_accuracy": {
                "pred_answer": "A",
                "gt_answer": "C",
                "score": 0
            }
        },
        {
            "doc_id": 196,
            "doc": {
                "time_reference": "09:10-52:16",
                "video": "vaL_vSdZKZo",
                "answer": "(D) 2",
                "question_type": [
                    "event understanding"
                ],
                "candidates": [
                    "(A) 0",
                    "(B) 3",
                    "(C) 1",
                    "(D) 2"
                ],
                "type": "documentary",
                "question": "How many times did Sergey Sarancha's interview segments appear?"
            },
            "target": "(D) 2",
            "arguments": [
                "How many times did Sergey Sarancha's interview segments appear?\nA. (A) 0\nB. (B) 3\nC. (C) 1\nD. (D) 2\nAnswer with the option's letter from the given choices directly.",
                {
                    "max_new_tokens": 16,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_beams": 1,
                    "do_sample": false,
                    "modalities": [
                        "video"
                    ],
                    "stopping_criteria": [
                        "<videoxl2.videoxl2.mm_utils.KeywordsStoppingCriteria object at 0x7f6ab80e8e50>"
                    ]
                },
                196,
                "lvbench_documentary",
                "train"
            ],
            "resps": [
                [
                    "C"
                ]
            ],
            "filtered_resps": [
                "C"
            ],
            "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b",
            "prompt_hash": "19b2f18fa751788bb5d5e511380fb2180714cd76fcd974343438cc63b82d061f",
            "target_hash": "610f912643698b744baa44b5b7ac325d3350523ffbbbb3b9209c49f827b3123f",
            "lvbench_mc_accuracy": {
                "pred_answer": "C",
                "gt_answer": "D",
                "score": 0
            }
        }
    ],
    "time": "0629_1948"
}