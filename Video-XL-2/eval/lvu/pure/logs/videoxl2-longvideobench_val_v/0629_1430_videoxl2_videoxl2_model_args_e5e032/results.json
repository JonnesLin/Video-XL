{
    "results": {
        "longvideobench_val_v": {
            "alias": "longvideobench_val_v",
            "lvb_acc,none": 0.60209,
            "lvb_acc_stderr,none": "N/A",
            "submission,none": [],
            "submission_stderr,none": []
        }
    },
    "group_subtasks": {
        "longvideobench_val_v": []
    },
    "configs": {
        "longvideobench_val_v": {
            "task": "longvideobench_val_v",
            "dataset_path": "longvideobench/LongVideoBench",
            "dataset_kwargs": {
                "token": true
            },
            "test_split": "validation",
            "full_docs": false,
            "process_results_use_image": false,
            "doc_to_visual": "<function longvideobench_doc_to_visual_v at 0x7fe202945750>",
            "doc_to_text": "<function longvideobench_doc_to_text at 0x7fe202946170>",
            "doc_to_target": "correct_choice",
            "process_results": "<function longvideobench_process_results at 0x7fe2029470a0>",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "lvb_acc",
                    "aggregation": "<function longvideobench_aggregate_results at 0x7fe202947be0>",
                    "higher_is_better": true
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "max_new_tokens": 32,
                "temperature": 0.0,
                "do_sample": false,
                "until": [
                    "\n\n"
                ]
            },
            "repeats": 1,
            "should_decontaminate": false,
            "lmms_eval_specific_kwargs": {
                "default": {
                    "pre_prompt": "",
                    "post_prompt": "Answer with the option's letter from the given choices directly.\n"
                },
                "pre_prompt": "",
                "post_prompt": "Answer with the option's letter from the given choices directly.\n"
            }
        }
    },
    "versions": {
        "longvideobench_val_v": "Yaml"
    },
    "n-shot": {
        "longvideobench_val_v": 0
    },
    "higher_is_better": {
        "longvideobench_val_v": {
            "lvb_acc": true
        }
    },
    "n-samples": {
        "longvideobench_val_v": {
            "original": 1337,
            "effective": 1337
        }
    },
    "config": {
        "model": "videoxl2",
        "model_args": "pretrained=/share/minghao/Models2/Video-XL-2,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=400,fps=1,max_fps=4,video_decode_backend=decord",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": "",
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "git_hash": "21bf182",
    "date": "0629_1521"
}