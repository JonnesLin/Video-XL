{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mmvids 结果转为 gtchunk idx 的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562/562 [00:00<00:00, 19939.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def hms_to_seconds(hms_str):\n",
    "    \"\"\"Converts an HH:MM:SS or HH-MM-SS string to total seconds.\"\"\"\n",
    "    parts = re.split(r'[-:]', hms_str)\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Invalid time format. Expected HH:MM:SS or HH-MM-SS.\")\n",
    "    \n",
    "    hours = int(parts[0])\n",
    "    minutes = int(parts[1])\n",
    "    seconds = int(parts[2])\n",
    "    \n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "\n",
    "def parse_clip_filename(filename):\n",
    "\n",
    "    # 使用正则表达式匹配时间部分\n",
    "    match = re.search(r'(\\d{2}-\\d{2}-\\d{2})_to_(\\d{2}-\\d{2}-\\d{2})\\.mp4$', filename)\n",
    "\n",
    "    if match:\n",
    "        start_time_raw = match.group(1)\n",
    "        end_time_raw = match.group(2)\n",
    "\n",
    "        start_seconds = hms_to_seconds(start_time_raw)\n",
    "        end_seconds = hms_to_seconds(end_time_raw)\n",
    "\n",
    "        return [start_seconds, end_seconds]\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "mmvid_videomme_medium = '/share/minghao/VideoProjects/lmm-eval/mmvidgts/demo.json'\n",
    "mmvid_videomme_medium = '/share/minghao/VideoProjects/lmm-eval/mmvidgts/mlvu_test.json'\n",
    "\n",
    "datas = []\n",
    "with open(mmvid_videomme_medium, 'r') as file:\n",
    "    datas = json.load(file)\n",
    "\n",
    "print(len(datas))\n",
    "\n",
    "\n",
    "record = {}\n",
    "for data in tqdm(datas):\n",
    "    if 'topk_clip_paths' not in data:\n",
    "        continue\n",
    "    video_name = data['video_name']\n",
    "    question = data['question']\n",
    "    unqiue_id = video_name + '.mp4' + '_' + question\n",
    "    record[unqiue_id] = None\n",
    "    all_time_spans = []\n",
    "    for clip_name in data['topk_clip_paths']:\n",
    "        time_spans = parse_clip_filename(clip_name)\n",
    "        all_time_spans.append(time_spans)\n",
    "\n",
    "    record[unqiue_id] = all_time_spans\n",
    "\n",
    "print(f'size: {len(record)}')\n",
    "\n",
    "with open('/share/minghao/VideoProjects/lmm-eval/mmvidgts/mlvu_test_clean.json', 'w') as f:\n",
    "    json.dump(record, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/share/minghao/VideoProjects/lmm-eval/mmvidgts/videomme_mid_clean.json', 'r') as file:\n",
    "    datas_1 = json.load(file)\n",
    "\n",
    "with open('/share/minghao/VideoProjects/lmm-eval/mmvidgts/videomme_long_clean.json', 'r') as file:\n",
    "    datas_2 = json.load(file)\n",
    "\n",
    "datas_1.update(datas_2)\n",
    "with open('/share/minghao/VideoProjects/lmm-eval/mmvidgts/videomme_mid_long_clean.json', 'w') as f:\n",
    "    json.dump(datas_1, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算 cache reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br1 = 1\n",
    "tokens_per_chunk_br1 = 1440/br1\n",
    "br2 = 4\n",
    "tokens_per_chunk_br2 = 1440/br2\n",
    "total_chunk = 26 # 26 13 6\n",
    "high_chunk = 6 # \n",
    "low_chunk = 20\n",
    "full_tokens = tokens_per_chunk_br1 * total_chunk\n",
    "\n",
    "cache_reduction = 1 - (tokens_per_chunk_br1*high_chunk + tokens_per_chunk_br2*low_chunk)/full_tokens\n",
    "print(f'cache_reduction: {cache_reduction}')\n",
    "\n",
    "avg_tokens_num = (tokens_per_chunk_br1*high_chunk + tokens_per_chunk_br2*low_chunk)/(total_chunk*10)\n",
    "print(f'avg_tokens_num: {avg_tokens_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = 72.6\n",
    "ratio = (1 * 0.75 + 0.25 * rd/100)/1\n",
    "print(f'{ratio*100:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_x1 = 233.1\n",
    "speed_x2 = [139.7, 65.81, 56.3, 52.1, 51.4]\n",
    "speed_x3 = [73.7, 55.2, 48.1, 42.9, 40.2]\n",
    "\n",
    "for s in speed_x3:\n",
    "    print(f'{speed_x1/s:.1f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PostTraing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 数据\n",
    "result_dir = '/share/minghao/Projects/lmms-eval/mainexpres/Exp7/MLVU/cp5knextqa3k_allqkv_wobe_24x81632'\n",
    "file_names = os.listdir(result_dir)\n",
    "\n",
    "score_2_32_file_names = []\n",
    "score_2_file_names = []\n",
    "score_32_file_names = []\n",
    "\n",
    "for name in file_names:\n",
    "    if '_2_32' in name:\n",
    "        score_2_32_file_names.append(name)\n",
    "    elif '_2_' in name:\n",
    "        score_2_file_names.append(name)\n",
    "    elif '_32_' in name:\n",
    "        score_32_file_names.append(name)\n",
    "\n",
    "# 定义提取 ckpt 后面数字的函数\n",
    "def extract_ckpt_number(item):\n",
    "    match = re.search(r'ckpt(\\d+)', item)  # 使用正则表达式提取 ckpt 后面的数字\n",
    "    if match:\n",
    "        return int(match.group(1))  # 返回数字并转换为整数\n",
    "    return 0  # 如果没有找到，返回 0（或其他默认值）\n",
    "\n",
    "# # 按照 ckpt 后面的数字排序\n",
    "score_2_32_file_names = sorted(score_2_32_file_names, key=extract_ckpt_number)\n",
    "score_2_file_names = sorted(score_2_file_names, key=extract_ckpt_number)\n",
    "score_32_file_names = sorted(score_32_file_names, key=extract_ckpt_number)\n",
    "\n",
    "def get_score(file_name):\n",
    "    scores_dir = os.path.join(result_dir, file_name, 'shuyan__VideoXL_weight_8')\n",
    "    # 读入目录下的jsonl文件\n",
    "    json_files = [f for f in os.listdir(scores_dir) if f.endswith('.json')]\n",
    "    json_file = json_files[-1]\n",
    "    json_path = os.path.join(scores_dir, json_file)\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    score = result['results']['mlvu']['mlvu_perception_score,none']\n",
    "    return score\n",
    "\n",
    "score_2_32 = [get_score(name) for name in score_2_32_file_names]\n",
    "score_2 = [get_score(name) for name in score_2_file_names]\n",
    "score_32 = [get_score(name) for name in score_32_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 数据\n",
    "score_2_32_before = 64.55\n",
    "score_2_before = 64.64\n",
    "score_32_before = 62.67\n",
    "\n",
    "score_2_32 = [score_2_32_before] + score_2_32\n",
    "score_2 = [score_2_before] + score_2\n",
    "score_32 = [score_32_before] + score_32\n",
    "x1 = list(range(len(score_2_32)))  \n",
    "x2 = list(range(len(score_2)))  \n",
    "x3 = list(range(len(score_32)))  \n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x1, score_2_32, label=\"ratio=mix@top3\", marker='o', linestyle='-', color='#4C72B0')  # 蓝色\n",
    "plt.plot(x2, score_2, label=\"ratio=2\", marker='s', linestyle='--', color='#55A868')  # 绿色\n",
    "plt.plot(x3, score_32, label=\"ratio=32\", marker='^', linestyle='-.', color='#C44E52')  # 红色\n",
    "\n",
    "# 绘制水平虚线\n",
    "plt.axhline(y=score_2_before, color='g', linestyle=':', )  # 绿色虚线\n",
    "plt.axhline(y=score_32_before, color='r', linestyle=':', )  # 红色虚线\n",
    "plt.axhline(y=score_2_32_before, color='b', linestyle=':',)  # 蓝色虚线\n",
    "plt.xticks([0, 1, 2, 3, 4], ['0k', '1k', '2k', '3k', '4k'])  # 设置刻度位置和标签\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title(\"MLVU Score\")\n",
    "plt.xlabel(\"Training Data\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.grid(True)\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minor(list1, list2):\n",
    "    new_list = []\n",
    "\n",
    "    length = min(len(list1), len(list2))\n",
    "    list1 = list1[:length]\n",
    "    list2 = list2[:length]\n",
    "\n",
    "    for a, b in zip(list1, list2):\n",
    "        new_list.append(a - b)\n",
    "    return new_list\n",
    "\n",
    "list_1 = make_minor(score_2_32, score_2)\n",
    "list_2 = make_minor(score_2_32, score_32)\n",
    "\n",
    "score_2_32_before = 64.55\n",
    "score_2_before = 64.64\n",
    "score_32_before = 62.67\n",
    "\n",
    "delta_mix_2 = score_2_32_before - score_2_before\n",
    "delta_mix_32 = score_2_32_before - score_32_before\n",
    "\n",
    "x1= list(range(len(list_1)))  # 假设 x 轴是 1, 2, 3, 4\n",
    "x2= list(range(len(list_2)))\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x1, list_1, label=\"mix-x2\", marker='o', linestyle='-', color='b')  # 蓝色\n",
    "plt.plot(x2, list_2, label=\"mix-x32\", marker='s', linestyle='--', color='g')  # 绿色\n",
    "\n",
    "plt.axhline(y=delta_mix_2, color='b', linestyle=':', label=\"mix-2\")  # 绿色虚线\n",
    "plt.axhline(y=delta_mix_32, color='g', linestyle=':', label=\"mix-32\")  # 红色虚线\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title(\"Score Comparison\")\n",
    "plt.xlabel(\"X Axis\")\n",
    "plt.ylabel(\"Scores\")\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path1 = '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/MLVU/videoxl_br32/shuyan__VideoXL_weight_8/20250104_173408_samples_mlvu.jsonl'\n",
    "path2 = '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/MLVU/videoxl_br32/shuyan__VideoXL_weight_8/20250104_092228_samples_mlvu.jsonl'\n",
    "path3 = '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/MLVU/videoxl_br32/shuyan__VideoXL_weight_8/20250103_003101_samples_mlvu.jsonl'\n",
    "\n",
    "def load_json(path):\n",
    "    datas = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            datas.append(data)\n",
    "    return datas\n",
    "\n",
    "datas1 = load_json(path1)\n",
    "datas2 = load_json(path2)\n",
    "datas3 = load_json(path3)\n",
    "\n",
    "datas = datas1 + datas2 + datas3\n",
    "print(len(datas))\n",
    "\n",
    "save_path = '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/MLVU/videoxl_br32/shuyan__VideoXL_weight_8/all_samples_mlvu.jsonl'\n",
    "with open(save_path, 'w') as file:\n",
    "    for data in datas:\n",
    "        line = json.dumps(data)\n",
    "        file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {\n",
    "    \"AC\": {\n",
    "        \"total\": 206,\n",
    "        \"correct\": 79,\n",
    "        \"accuracy\": 38.349514563106794\n",
    "    },\n",
    "    \"AO\": {\n",
    "        \"total\": 259,\n",
    "        \"correct\": 161,\n",
    "        \"accuracy\": 62.16216216216216\n",
    "    },\n",
    "    \"ER\": {\n",
    "        \"total\": 352,\n",
    "        \"correct\": 179,\n",
    "        \"accuracy\": 50.85227272727273\n",
    "    },\n",
    "    \"NQA\": {\n",
    "        \"total\": 355,\n",
    "        \"correct\": 255,\n",
    "        \"accuracy\": 71.83098591549296\n",
    "    },\n",
    "    \"PQA\": {\n",
    "        \"total\": 539,\n",
    "        \"correct\": 374,\n",
    "        \"accuracy\": 69.38775510204081\n",
    "    },\n",
    "    \"AR\": {\n",
    "        \"total\": 200,\n",
    "        \"correct\": 105,\n",
    "        \"accuracy\":\n",
    "    },\n",
    "    \"TR\": {\n",
    "        \"total\": 264,\n",
    "        \"correct\": 210,\n",
    "        \"accuracy\": 79.54545454545455\n",
    "    },\n",
    "    \"global\": {\n",
    "        \"total\": 2175,\n",
    "        \"correct\": 1363,\n",
    "        \"accuracy\": 62.67\n",
    "    }\n",
    "}\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for name, info in accuracy_dict.items():\n",
    "    total = total + info['total']\n",
    "    correct = correct + info['correct']\n",
    "\n",
    "print(f'{total}')\n",
    "print(f'{correct}')\n",
    "print(f'{correct/total * 100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均压缩率 计算\n",
    "ratio1 = 1/2\n",
    "ratio2 = 1/32\n",
    "\n",
    "total_chunk = 26\n",
    "ratio1_chunk = 5\n",
    "ratio2_chunk = total_chunk - ratio1_chunk\n",
    "\n",
    "new_ratio = (ratio1_chunk * ratio1 + ratio2_chunk * ratio2)/total_chunk\n",
    "new_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究一下plotqa\n",
    "\n",
    "top1 和 top3 差异在哪？是什么决定了一个问题的对错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_ER_datas(path):\n",
    "    with open(path, 'r') as file:\n",
    "        datas = []\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['doc']['task_type'] == 'ER':\n",
    "                datas.append(data)\n",
    "        \n",
    "    return datas\n",
    "\n",
    "def get_PQA_datas(path):\n",
    "    with open(path, 'r') as file:\n",
    "        datas = []\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['doc']['task_type'] == 'PQA':\n",
    "                datas.append(data)\n",
    "        \n",
    "    return datas\n",
    "\n",
    "def get_datas(path, name='PQA'):\n",
    "    with open(path, 'r') as file:\n",
    "        datas = []\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['doc']['task_type'] == name:\n",
    "                datas.append(data)\n",
    "        \n",
    "    return datas\n",
    "\n",
    "def filter_error(datas):\n",
    "    record_error_dict = {}\n",
    "    for data in datas:\n",
    "        mlvu_percetion_score = data['mlvu_perception_score'] \n",
    "        question_id = mlvu_percetion_score['question_id']\n",
    "        pred_answer = mlvu_percetion_score['pred_answer']\n",
    "        answer = mlvu_percetion_score['answer']\n",
    "        if pred_answer != answer:\n",
    "            \n",
    "            pure_question = question_id.split('\\nOptions:')[0].split('Question: ')[-1]\n",
    "            unqiue_id = data['doc']['video_name'] + '_' + pure_question\n",
    "\n",
    "            if unqiue_id in record_error_dict:\n",
    "                print(f'need to dedup!')\n",
    "\n",
    "            record_error_dict[unqiue_id] = {\n",
    "                'pred_answer': pred_answer,\n",
    "                'answer': answer\n",
    "            }\n",
    "\n",
    "    return record_error_dict\n",
    "\n",
    "task_name = 'PQA'\n",
    "top1_path = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32_top1/shuyan__VideoXL_weight_8/20250115_145302_samples_mlvu.jsonl'\n",
    "# top1_datas = get_PQA_datas(top1_path)\n",
    "top1_datas = get_datas(top1_path, task_name)\n",
    "\n",
    "top3_path = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32_top3/shuyan__VideoXL_weight_8/20250115_114835_samples_mlvu.jsonl'\n",
    "top4_path = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32_top4/shuyan__VideoXL_weight_8/20250115_141359_samples_mlvu.jsonl'\n",
    "# top3_datats = get_PQA_datas(top3_path)\n",
    "top3_datats = get_datas(top4_path, task_name)\n",
    "\n",
    "# assert len(top1_datas)==len(top3_datats)==539, len(top1_datas)\n",
    "\n",
    "# 筛选出做错的那些\n",
    "top1_errors = filter_error(top1_datas)\n",
    "top3_errors = filter_error(top3_datats)\n",
    "\n",
    "top1_errors_question_ids_set = set(list(top1_errors.keys()))\n",
    "top3_errors_question_ids_set = set(list(top3_errors.keys()))\n",
    "\n",
    "print(f'top1 error size: {len(top1_errors_question_ids_set)}')\n",
    "print(f'top3 error size: {len(top3_errors_question_ids_set)}')\n",
    "\n",
    "top1_f_top3_t = top1_errors_question_ids_set - top3_errors_question_ids_set\n",
    "top1_t_top3_f = top3_errors_question_ids_set - top1_errors_question_ids_set\n",
    "\n",
    "print(f'top1 ❌ top4 ✅: {len(top1_f_top3_t)}') \n",
    "print(f'top1 ✅ top4 ❌: {len(top1_t_top3_f)}') \n",
    "# for ele in top1_errors_question_ids_set - top3_errors_question_ids_set:\n",
    "#     print(ele)\n",
    "\n",
    "# 看一下他们的 gt 和 predict chunk idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入 predict chunk json\n",
    "\n",
    "\n",
    "predict_chunk_path = '/share/minghao/Projects/LanguageBind/llmseval/plotqa.json'\n",
    "with open(predict_chunk_path, 'r') as f:\n",
    "    predict_chunk = json.load(f)\n",
    "\n",
    "# 载入 gt chunk json\n",
    "gt_chunk_path = '/share/minghao/Projects/Video-XL/videoxl/videoxl/myeval/datas/searchgt/plotqa_w_gt_256.json'\n",
    "with open(gt_chunk_path, 'r') as f:\n",
    "    gt_chunk = json.load(f)\n",
    "\n",
    "count = 0\n",
    "print(f'top3 做对, top1做错')\n",
    "for unique_id in top1_f_top3_t:\n",
    "   \n",
    "    if unique_id not in gt_chunk:\n",
    "        print(unique_id)\n",
    "        count = count + 1\n",
    "        continue\n",
    "\n",
    "    gt = gt_chunk[unique_id]\n",
    "    predict_top1 = predict_chunk[unique_id]['top1']\n",
    "    predict_top3 = predict_chunk[unique_id]['top3']\n",
    "    gt = gt_chunk[unique_id]['gt_chunk_idx']\n",
    "    print(f'gt: {gt}, top3: {predict_top3}, top1: {predict_top1}')\n",
    "\n",
    "print('='*80)\n",
    "print(f'top1 做对, top3做错')\n",
    "for unique_id in top1_t_top3_f:\n",
    "   \n",
    "    if unique_id not in gt_chunk:\n",
    "        # print(unique_id)\n",
    "        count = count + 1\n",
    "        continue\n",
    "\n",
    "    gt = gt_chunk[unique_id]\n",
    "    predict_top1 = predict_chunk[unique_id]['top1']\n",
    "    predict_top3 = predict_chunk[unique_id]['top3']\n",
    "    gt = gt_chunk[unique_id]['gt_chunk_idx']\n",
    "    print(f'gt: {gt}, top1: {predict_top1}, top3: {predict_top3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/minghao/Projects/lmms-eval/results/1128_1943_videoxl_videoxl_model_args_38784d/mlvu.json'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    datas = json.load(file)\n",
    "    \n",
    "path = '/share/minghao/Projects/lmms-eval/results/1128_1943_videoxl_videoxl_model_args_38784d/samples_mlvu.jsonl'\n",
    "\n",
    "with open(path, 'w') as file:\n",
    "    for data in datas['logs']:\n",
    "        line = json.dumps(data)\n",
    "        file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据字典\n",
    "datas_dict_1 = {0: 64.85, 25: 65.65, 50: 66.69, 70: 67.26, 100:68.70}\n",
    "\n",
    "# 提取x和y值\n",
    "x_values = list(datas_dict_1.keys())  # x轴数据\n",
    "y_values_1 = list(datas_dict_1.values())  # 数据集1的y值\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x_values, y_values_1, label='w high density', marker='o', color='blue')\n",
    "# 绘制基准线\n",
    "plt.axhline(y=baseline_score, color='red', linestyle='--', label='VideoXL')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Single Detail Task Score vs Hit Ratio')\n",
    "plt.xlabel('Hit Ratio')\n",
    "plt.ylabel('Avg Score')\n",
    "plt.xticks(x_values)\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 展示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据字典\n",
    "datas_dict_1 = {4: 67.09, 8: 67.34, 16: 66.29, 32: 64.85}\n",
    "datas_dict_2 = {4: 66.77, 8: 67.34, 16: 67.98, 32: 68.70}\n",
    "baseline_score = 66.61\n",
    "\n",
    "# 提取x和y值\n",
    "x_values = list(datas_dict_1.keys())  # x轴数据\n",
    "y_values_1 = list(datas_dict_1.values())  # 数据集1的y值\n",
    "y_values_2 = list(datas_dict_2.values())  # 数据集2的y值\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x_values, y_values_1, label='wo high density', marker='o', color='blue')\n",
    "plt.plot(x_values, y_values_2, label='w high density', marker='s', color='green')\n",
    "# 绘制基准线\n",
    "plt.axhline(y=baseline_score, color='red', linestyle='--', label='VideoXL')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Single Detail Task Score vs Compression Ratio')\n",
    "plt.xlabel('Compression Ration')\n",
    "plt.ylabel('Avg Score')\n",
    "plt.xticks(x_values)\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 展示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据字典\n",
    "datas_dict_1 = {4: 67.09, 8: 67.34, 18: 66.29, 32: 64.85}\n",
    "datas_dict_2 = {4: 66.77, 8: 67.34, 18: 67.98, 32: 68.70}\n",
    "\n",
    "# 假设 baseline 模型的分数为 65\n",
    "baseline_score = 65\n",
    "\n",
    "# 提取x和y值\n",
    "x_values = list(datas_dict_1.keys())  # x轴数据\n",
    "y_values_1 = list(datas_dict_1.values())  # 数据集1的y值\n",
    "y_values_2 = list(datas_dict_2.values())  # 数据集2的y值\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x_values, y_values_1, label='Data Set 1', marker='o', color='blue')\n",
    "plt.plot(x_values, y_values_2, label='Data Set 2', marker='s', color='green')\n",
    "\n",
    "# 绘制基准线\n",
    "plt.axhline(y=baseline_score, color='red', linestyle='--', label='Baseline')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Line Plot of Two Data Sets with Baseline')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 展示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画一些趋势图\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化图形美观\n",
    "plt.style.use('seaborn-poster')  # 使用美观的主题\n",
    "\n",
    "# 创建绘图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, marker='o', linestyle='-', color='#ff7f0e', linewidth=2.5, label='Trend Line')\n",
    "\n",
    "# 添加数据点的标签\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i] + 0.1, f'({x[i]}, {y[i]:.2f})', fontsize=12, ha='center', color='#1f77b4')\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title('Beautiful Trend of Data Points', fontsize=18, color='#2ca02c')\n",
    "plt.xlabel('X-axis', fontsize=14, color='#9467bd')\n",
    "plt.ylabel('Y-axis', fontsize=14, color='#9467bd')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据字典\n",
    "datas_dict = {2: 66.29, 4: 67.09, 8: 67.34}\n",
    "plot_datas_dict = {2: 66.23, 4: 68.09, 8: 69,20} \n",
    "needle_datas_dict = {2: 75.77, 4: 75.77, 8: 74.37}\n",
    "ego_datas_dict = {2: 56.82, 4: 56.82, 8: 57.39}\n",
    "\n",
    "# 提取x和y数据\n",
    "x = list(datas_dict.keys())\n",
    "y = list(datas_dict.values())\n",
    "\n",
    "\n",
    "# 优化图形美观\n",
    "plt.style.use('ggplot')  # 使用美观的主题\n",
    "\n",
    "# 创建绘图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, marker='o', linestyle='-', color='#ff7f0e', linewidth=2.5, label='Trend Line')\n",
    "\n",
    "# 添加数据点的标签\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], y[i] + 0.1, f'({x[i]}, {y[i]:.2f})', fontsize=12, ha='center', color='#1f77b4')\n",
    "\n",
    "# 图表标题和轴标签\n",
    "plt.title('PlotQA-NeedleQA-Ego', fontsize=18, color='#2ca02c')\n",
    "plt.xlabel('Compression Ratio', fontsize=14, color='#9467bd')\n",
    "plt.ylabel('Avg Score', fontsize=14, color='#9467bd')\n",
    "\n",
    "# 美化坐标轴\n",
    "plt.xticks(fontsize=12, color='#8c564b')\n",
    "plt.yticks(fontsize=12, color='#8c564b')\n",
    "plt.grid(color='#d3d3d3', linestyle='--', linewidth=1.0)\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize=12, loc='upper left')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据字典\n",
    "datas_dict = {2: 66.29, 4: 67.09, 8: 67.34}\n",
    "plot_datas_dict = {2: 66.23, 4: 68.09, 8: 69.20}\n",
    "needle_datas_dict = {2: 75.77, 4: 75.77, 8: 74.37}\n",
    "ego_datas_dict = {2: 56.82, 4: 56.82, 8: 57.39}\n",
    "\n",
    "# 提取x和y数据\n",
    "x = list(datas_dict.keys())\n",
    "\n",
    "# 优化图形美观\n",
    "plt.style.use('ggplot')  # 使用美观的主题\n",
    "\n",
    "# 创建绘图\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制不同的曲线\n",
    "plt.plot(x, list(datas_dict.values()), marker='o', linestyle='-', color='#ff7f0e', linewidth=2.5, label='Avg')\n",
    "plt.plot(x, list(plot_datas_dict.values()), marker='s', linestyle='--', color='#1f77b4', linewidth=2.5, label='PlotQA')\n",
    "plt.plot(x, list(needle_datas_dict.values()), marker='^', linestyle='-.', color='#2ca02c', linewidth=2.5, label='NeedleQA')\n",
    "plt.plot(x, list(ego_datas_dict.values()), marker='d', linestyle=':', color='#9467bd', linewidth=2.5, label='EgoQA')\n",
    "\n",
    "# 添加数据点的标签\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i], datas_dict[x[i]] + 0.1, f'({x[i]}, {datas_dict[x[i]]:.2f})', fontsize=12, ha='center', color='#ff7f0e')\n",
    "    plt.text(x[i], plot_datas_dict[x[i]] + 0.1, f'({x[i]}, {plot_datas_dict[x[i]]:.2f})', fontsize=12, ha='center', color='#1f77b4')\n",
    "    plt.text(x[i], needle_datas_dict[x[i]] + 0.1, f'({x[i]}, {needle_datas_dict[x[i]]:.2f})', fontsize=12, ha='center', color='#2ca02c')\n",
    "    plt.text(x[i], ego_datas_dict[x[i]] + 0.1, f'({x[i]}, {ego_datas_dict[x[i]]:.2f})', fontsize=12, ha='center', color='#9467bd')\n",
    "\n",
    "# 图表标题和轴标签\n",
    "plt.title('PlotQA-NeedleQA-Ego', fontsize=18)\n",
    "plt.xlabel('Compression Ratio', fontsize=14)\n",
    "plt.ylabel('Avg Score', fontsize=14)\n",
    "\n",
    "# 美化坐标轴\n",
    "plt.xticks(fontsize=12, color='#8c564b')\n",
    "plt.yticks(fontsize=12, color='#8c564b')\n",
    "plt.grid(color='#d3d3d3', linestyle='--', linewidth=1.0)\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(fontsize=12, loc='upper left')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 查看 做错的 needle 任务是什么分布（持续时间，clip持续时间，gt frame 数量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "sample_num = 256\n",
    "check_task_type = 'NQA'\n",
    "data_name = 'needle'\n",
    "video_dir = f'/share/junjie/code/videofactory/Evaluation_LVBench/LVBench_all/video/{data_name}'\n",
    "path = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32/shuyan__VideoXL_weight_8/20250102_163528_samples_mlvu.jsonl'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    datas = [json.loads(line) for line in file]\n",
    "\n",
    "def get_record_error(no_reload_datas):\n",
    "    record_error = {}\n",
    "    record_error_list = []\n",
    "    record = 0\n",
    "    for every_res in no_reload_datas:\n",
    "\n",
    "        mlvu_percetion_score = every_res['mlvu_perception_score']\n",
    "        task_type = mlvu_percetion_score['task_type']\n",
    "\n",
    "        unique_id = every_res['doc']['video_name'] + every_res['input']\n",
    "        \n",
    "        if task_type != check_task_type:\n",
    "            continue\n",
    "        \n",
    "        pred_answer = mlvu_percetion_score['pred_answer']\n",
    "        answer = mlvu_percetion_score['answer']\n",
    "\n",
    "        inp = every_res['doc']['question']\n",
    "        \n",
    "        if pred_answer != answer:\n",
    "            record_error[unique_id] = {'gt': answer, 'pred': pred_answer, 'inp':inp}\n",
    "\n",
    "    return record_error\n",
    "\n",
    "errors = get_record_error(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import decord\n",
    "from decord import VideoReader, cpu\n",
    "import numpy as np\n",
    "\n",
    "datas_with_gtspan_path = '/share/minghao/Projects/Video-XL/videoxl/videoxl/myeval/datas/needle.jsonl'\n",
    "\n",
    "video_meta_dict = {}\n",
    "\n",
    "with open(datas_with_gtspan_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        video = data['video']\n",
    "        video_meta = data['video_meta']\n",
    "        video_meta_dict[video] = video_meta\n",
    "\n",
    "\n",
    "situration_1 = 0\n",
    "situration_2 = 0\n",
    "situration_3 = 0\n",
    "\n",
    "all_keys = []\n",
    "record_obervations = {}\n",
    "for key, val in tqdm(errors.items()):\n",
    "    video = key.split('\\n')[0]\n",
    "    all_keys.append(video)\n",
    "    video_path = os.path.join(video_dir, video)\n",
    "    # 获取 duration\n",
    "    # 获取 256 采样下的 gt 数量\n",
    "\n",
    "    # read video\n",
    "    if video not in video_meta_dict:\n",
    "        # print(f'{video} not in video_meta_dict')\n",
    "        situration_1 = situration_1 + 1\n",
    "        continue\n",
    "    video_meta = video_meta_dict[video]\n",
    "    video_start_sec = video_meta[0]['video_start_sec']\n",
    "    video_end_sec = video_meta[0]['video_end_sec']\n",
    "\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    fps = vr.get_avg_fps()  # 获取视频帧率\n",
    "    max_frames_num = sample_num\n",
    "    total_frame_num = len(vr)\n",
    "    duration = total_frame_num/fps\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "\n",
    "    have_gt_flag = False\n",
    "    gt_indices = []\n",
    "    gt_frame_idx = []\n",
    "    for indice, this_idx in enumerate(frame_idx):\n",
    "        counter_time = this_idx / fps\n",
    "        if counter_time >= video_start_sec and counter_time <= video_end_sec:\n",
    "            have_gt_flag = True\n",
    "            gt_indices.append(this_idx)\n",
    "            gt_frame_idx.append(indice)\n",
    "\n",
    "    record_obervations[video] = {'gt_frames':len(gt_frame_idx), 'duration':duration, 'clip_duration':video_end_sec-video_start_sec}\n",
    "\n",
    "    if have_gt_flag:\n",
    "        # print(f'{video} have gt')\n",
    "        situration_2 = situration_2 + 1\n",
    "    else:\n",
    "        # print(f'{video} no gt')\n",
    "        situration_3 = situration_3 + 1\n",
    "\n",
    "print(f'situration_1: {situration_1}')\n",
    "print(f'situration_2: {situration_2}')\n",
    "print(f'situration_3: {situration_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_obervations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = record_obervations\n",
    "# Extract values for histograms\n",
    "gt_frames = [v['gt_frames'] for v in data.values()]\n",
    "durations = [v['duration'] for v in data.values()]\n",
    "clip_durations = [v['clip_duration'] for v in data.values()]\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Ground truth frames\n",
    "axes[0].hist(gt_frames, bins=range(min(gt_frames), max(gt_frames) + 2), color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Ground Truth Frames Distribution')\n",
    "axes[0].set_xlabel('GT Frames')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Durations\n",
    "axes[1].hist(durations, bins=7, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Video Duration Distribution')\n",
    "axes[1].set_xlabel('Duration (s)')\n",
    "\n",
    "# Clip durations\n",
    "axes[2].hist(clip_durations, bins=7, color='salmon', edgecolor='black')\n",
    "axes[2].set_title('Clip Duration Distribution')\n",
    "axes[2].set_xlabel('Clip Duration (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测 case 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "check_task_type = 'NQA'\n",
    "\n",
    "no_reload_result_path = '/share/minghao/Projects/lmms-eval/results/cp5knextqa3k_reload06_noreload/shuyan__VideoXL_weight_8/20241230_091641_samples_mlvu.jsonl'\n",
    "reload_result_path = '/share/minghao/Projects/lmms-eval/results/cp5knextqa3k_reload/shuyan__VideoXL_weight_8/20241230_161910_samples_mlvu.jsonl'\n",
    "\n",
    "with open(no_reload_result_path, 'r') as file:\n",
    "    no_reload_datas = [json.loads(line) for line in file]\n",
    "\n",
    "with open(reload_result_path, 'r') as file:\n",
    "    reload_datas = [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "def get_record_error(no_reload_datas):\n",
    "    record_error = {}\n",
    "    record_error_list = []\n",
    "    record = 0\n",
    "    for every_res in no_reload_datas:\n",
    "\n",
    "        mlvu_percetion_score = every_res['mlvu_perception_score']\n",
    "        task_type = mlvu_percetion_score['task_type']\n",
    "\n",
    "        unique_id = every_res['doc']['video_name'] + every_res['input']\n",
    "        \n",
    "        if task_type != check_task_type:\n",
    "            continue\n",
    "        \n",
    "        pred_answer = mlvu_percetion_score['pred_answer']\n",
    "        answer = mlvu_percetion_score['answer']\n",
    "\n",
    "        inp = every_res['doc']['question']\n",
    "        \n",
    "        if pred_answer != answer:\n",
    "            record_error[unique_id] = {'gt': answer, 'pred': pred_answer, 'inp':inp}\n",
    "\n",
    "    return record_error\n",
    "\n",
    "no_reload_error = get_record_error(no_reload_datas)\n",
    "reload_error = get_record_error(reload_datas)\n",
    "\n",
    "\n",
    "diff_keys = set(reload_error.keys()) - set(no_reload_error.keys())\n",
    "print(len(diff_keys))\n",
    "print(f'is subset: {set(reload_error.keys()) <= set(no_reload_error.keys())}')\n",
    "print(f'reload_error num: {len(set(reload_error.keys()))}')\n",
    "print(f'no_reload_error num: {len(set(no_reload_error.keys()))}')\n",
    "print(f'intersaction: {len(set(reload_error.keys()) & set(no_reload_error.keys()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload 之后仍然做错的\n",
    "still_error = set(reload_error.keys()) & set(no_reload_error.keys())\n",
    "print(f'still_error num: {len(still_error)}')\n",
    "# 本来是错的 reload 之后做对的\n",
    "correct_after_reload = set(no_reload_error.keys()) - set(reload_error.keys())\n",
    "print(f'correct_after_reload num: {len(correct_after_reload)}')\n",
    "# 本来是对的 reload 之后做错的\n",
    "error_after_reload = set(reload_error.keys()) - set(no_reload_error.keys())\n",
    "print(f'error_after_reload num: {len(error_after_reload)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = correct_after_reload\n",
    "# tmp = error_after_reload\n",
    "tmp = still_error\n",
    "\n",
    "# correct_after_reload_data_dict_list = []\n",
    "# error_after_reload_data_dict_list = []\n",
    "still_error_data_dict_list = []\n",
    "\n",
    "for unique_id in tmp:\n",
    "    \n",
    "    pos = unique_id.find('\\n')\n",
    "    video = unique_id[:pos]\n",
    "    inp = reload_error[unique_id]['inp']\n",
    "    answer = reload_error[unique_id]['gt']\n",
    "    lmms_eval_pred = reload_error[unique_id]['pred']\n",
    "\n",
    "    still_error_data_dict_list.append(\n",
    "        {\n",
    "            'video': video,\n",
    "            'inp': inp,\n",
    "            'answer': answer,\n",
    "            'lmms_eval_pred': lmms_eval_pred,\n",
    "        }\n",
    "    )\n",
    "\n",
    "save_path = f'/share/minghao/Projects/Video-XL/videoxl/videoxl/myeval/datas/badcase/still_error_data_dict_list.jsonl'\n",
    "\n",
    "import json\n",
    "\n",
    "with open(save_path, 'w') as file:\n",
    "    for data in still_error_data_dict_list:\n",
    "        line = json.dumps(data)\n",
    "        file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLVU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为每个数据集获得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly_reco 18 39 46.15\n",
      "sportsQA 17 36 47.22\n",
      "order 40 70 57.14\n",
      "needleQA 33 60 55.00\n",
      "plotQA 33 50 66.00\n",
      "topic_reasoning 63 91 69.23\n",
      "ego 31 53 58.49\n",
      "tutorialQA 12 43 27.91\n",
      "count 22 60 36.67\n",
      "holistic 22 502 59.04\n",
      "single 22 502 52.05\n",
      "multi 22 502 50.00\n",
      "global_correct: 269\n",
      "total: 502\n",
      "ACC: 53.59\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "result = '/share/minghao/VideoProjects/lmm-eval-sparse/logs/MLVUTest/videoxl-stage4-mlvu-test/0502_0831_videoxl_videoxl_model_args_43d856/mlvu_test.json'\n",
    "\n",
    "prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "if 'jsonl' in result:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = [json.loads(line) for line in file]\n",
    "else:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = json.load(file)\n",
    "        datas = datas['logs']\n",
    "\n",
    "logs = datas\n",
    "\n",
    "global_correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_by_task = {}\n",
    "total_by_task = {}\n",
    "\n",
    "records_pred_answer = []\n",
    "for every_res in logs:\n",
    "    if 'mlvu_percetion_score' in every_res:\n",
    "        mlvu_percetion_score = every_res['mlvu_percetion_score']\n",
    "    elif 'mlvu_perception_score' in every_res:\n",
    "        mlvu_percetion_score = every_res['mlvu_perception_score']\n",
    "    task_type = mlvu_percetion_score['task_type']\n",
    "    \n",
    "    pred_answer = mlvu_percetion_score['pred_answer']\n",
    "\n",
    "    answer = mlvu_percetion_score['answer']\n",
    "\n",
    "    if pred_answer == answer:\n",
    "        if task_type not in correct_by_task:\n",
    "            correct_by_task[task_type] = 1\n",
    "        else:\n",
    "            correct_by_task[task_type] += 1\n",
    "\n",
    "    if task_type not in total_by_task:\n",
    "        total_by_task[task_type] = 1\n",
    "    else:\n",
    "        total_by_task[task_type] += 1\n",
    "\n",
    "    if answer == pred_answer:\n",
    "        global_correct += 1\n",
    "\n",
    "    total = total + 1\n",
    "new_result_to_result = {'holistic':{}, 'single':{}, 'multi':{}}\n",
    "\n",
    "holistic = ['TR','AR','anomaly_reco','topic_reasoning']\n",
    "single = ['NQA','PQA', \"ER\", 'needle','ego','plotQA']\n",
    "multi = ['AC','AO','count','order']\n",
    "\n",
    "\n",
    "for items in correct_by_task.items():\n",
    "    task_type = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "    print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "\n",
    "    if task_type in holistic:\n",
    "        category = 'holistic'\n",
    "    elif task_type in single:\n",
    "        category = 'single'\n",
    "    elif task_type in multi:\n",
    "        category = 'multi'\n",
    "\n",
    "    if 'total' not in new_result_to_result[category]:\n",
    "        new_result_to_result[category]['total'] = total_by_task[task_type]\n",
    "        new_result_to_result[category]['correct'] = correct\n",
    "    else:\n",
    "        new_result_to_result[category]['total'] += total_by_task[task_type]\n",
    "        new_result_to_result[category]['correct'] += correct\n",
    "\n",
    "    new_result_to_result[task_type] = {\n",
    "        'total': total_by_task[task_type],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }    \n",
    "\n",
    "for category in ['holistic', 'single', 'multi']:\n",
    "    this_correct = new_result_to_result[category]['correct']\n",
    "    this_total = new_result_to_result[category]['total']\n",
    "    accuracy = this_correct/this_total * 100\n",
    "    new_result_to_result[category]['accuracy'] = accuracy\n",
    "    print(f'{category} {correct} {total} {accuracy:.2f}')\n",
    "\n",
    "print(f'global_correct: {global_correct}')\n",
    "print(f'total: {total}')\n",
    "print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "new_result_to_result['global'] = {\n",
    "    'total': total,\n",
    "    'correct': global_correct,\n",
    "    'accuracy': global_correct/total*100\n",
    "}\n",
    "\n",
    "task_types = list(new_result_to_result.keys())\n",
    "save_name = prefix + '_' + 'results_each_dataset'\n",
    "for tt in task_types:\n",
    "    if tt == 'global':\n",
    "        continue\n",
    "    save_name = save_name + '_' + tt\n",
    "save_name = save_name + '.json'\n",
    "\n",
    "save_dir = os.path.dirname(result)\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(new_result_to_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.53444444444444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(36.67 + 27.91 + 58.49 + 69.23 + 66.00 + 55.00 + 57.14 + 47.22 + 46.15)/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 获得最优解，打印markdown形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name | topic_reasoning | anomaly_reco | needle | plotQA | ego | count | order | global |\n",
      "|------------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
      "|  mlvu-sparse-256-b4_p3_top8_c4_cmpr4  | 81.75 | 71.00 | 85.07 | 78.48 | 68.18 | 44.66 | 77.61 | 74.29 |\n",
      "|  mlvu-sparse-256-b4_p3_top1_c4_cmpr4  | 76.05 | 68.00 | 84.51 | 72.54 | 66.76 | 44.66 | 63.32 | 69.83 |\n",
      "|  mlvu-sparse-256-b4_p3_top4_c4_cmpr4  | 81.37 | 69.00 | 84.51 | 77.18 | 68.18 | 45.63 | 72.59 | 73.14 |\n",
      "|  mlvu-sparse-256-b4_p3_top7_c4_cmpr4  | 81.75 | 71.00 | 85.07 | 78.66 | 67.90 | 43.69 | 78.38 | 74.29 |\n",
      "|  mlvu-sparse-256-b4_p3_top6_c4_cmpr4  | 81.37 | 69.50 | 84.79 | 79.04 | 68.18 | 44.17 | 77.61 | 74.15 |\n",
      "|  mlvu-sparse-256-b4_p3_top2_c4_cmpr4  | 79.47 | 68.00 | 84.23 | 75.32 | 67.61 | 45.63 | 67.57 | 71.62 |\n",
      "|  mlvu-sparse-256-b4_p3_top3_c4_cmpr4  | 80.23 | 68.00 | 84.79 | 75.51 | 68.18 | 45.15 | 70.66 | 72.26 |\n",
      "|  mlvu-sparse-256-b4_p3_top5_c4_cmpr4  | 81.37 | 68.50 | 85.07 | 77.37 | 68.18 | 44.17 | 76.06 | 73.51 |\n",
      "\n",
      "\n",
      "\n",
      "topic_reasoning: correct 215, total 263, acc 81.75, source mlvu-sparse-256-b4_p3_top8_c4_cmpr4\n",
      "anomaly_reco: correct 142, total 200, acc 71.00, source mlvu-sparse-256-b4_p3_top8_c4_cmpr4\n",
      "needle: correct 302, total 355, acc 85.07, source mlvu-sparse-256-b4_p3_top8_c4_cmpr4\n",
      "plotQA: correct 426, total 539, acc 79.04, source mlvu-sparse-256-b4_p3_top6_c4_cmpr4\n",
      "ego: correct 240, total 352, acc 68.18, source mlvu-sparse-256-b4_p3_top8_c4_cmpr4\n",
      "count: correct 94, total 206, acc 45.63, source mlvu-sparse-256-b4_p3_top4_c4_cmpr4\n",
      "order: correct 203, total 259, acc 78.38, source mlvu-sparse-256-b4_p3_top7_c4_cmpr4\n",
      "\n",
      "Best Performance: 74.61\n",
      "Avg Acc: 72.72\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_each_dataset = []\n",
    "\n",
    "results_each_dataset = None\n",
    "if results_each_dataset is None:\n",
    "    import glob\n",
    "    directory = \"/share/minghao/VideoProjects/lmm-eval-sparse/logs/MLVU\"\n",
    "    # 递归搜索所有子目录，匹配包含 'qin' 的 .jsonl 文件\n",
    "    results_each_dataset = glob.glob(f\"{directory}/**/*results_each_dataset*.json\", recursive=True)\n",
    "\n",
    "# results_each_dataset = [tmp for tmp in results_each_dataset if \"notrain\" not in tmp]\n",
    "results_each_dataset = [tmp for tmp in results_each_dataset if \"stage4\" not in tmp]\n",
    "\n",
    "columns = ['topic_reasoning', 'anomaly_reco', 'needle', 'plotQA', 'ego', 'count', 'order', 'global']\n",
    "# columns = ['topic_reasoning', 'anomaly_reco', 'needleQA', 'plotQA', 'ego', 'count', 'order', 'global', 'sportsQA', 'tutorialQA']\n",
    "\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(columns) + \"\\n\"\n",
    "\n",
    "max_performance_record = {}\n",
    "for result_path in results_each_dataset:\n",
    "    # print(result_path)\n",
    "    with open(result_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = result_path.split('/')[-3]\n",
    "    # 提取列名和准确率\n",
    "    accuracies = []\n",
    "    for col in columns:\n",
    "        accuracies.extend([f\"{data[col]['accuracy']:.2f}\" ])\n",
    "        if (col not in max_performance_record or data[col]['accuracy'] > max_performance_record[col]['accuracy']) and col != 'global':\n",
    "            max_performance_record[col] = data[col]\n",
    "            max_performance_record[col]['source'] = model_name\n",
    "\n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n",
    "print()\n",
    "# print(max_performance_record)\n",
    "print()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "avg_acc = 0\n",
    "\n",
    "for key, val in max_performance_record.items():\n",
    "    print(f\"{key}: correct {val['correct']}, total {val['total']}, acc {val['accuracy']:.2f}, source {val['source']}\")\n",
    "    correct = correct + val['correct']\n",
    "    total = total + val['total']\n",
    "    avg_acc = avg_acc + val['accuracy']\n",
    "\n",
    "print(f\"\\nBest Performance: {correct / total*100:.2f}\")\n",
    "print(f\"Avg Acc: {avg_acc / len(max_performance_record):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 74.93\n"
     ]
    }
   ],
   "source": [
    "# DEV\n",
    "rec = {'topic_reasoning':(216, 263),'anomaly_reco':(148, 200),'needle':(302, 355),'plotQA':(426, 539),'ego':(240, 352),'count':(94, 206),'order':(203, 259) }\n",
    "\n",
    "# TEST\n",
    "# rec = {'topic_reasoning':(63, 91),'anomaly_reco':(18, 39),'needle':(38, 60),'plotQA':(32, 50),'ego':(34, 53),'count':(20, 60),'order':(37, 70),'sportsQA':(16, 36) ,'tutorialQA':(13, 43)}\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for key,(c, t) in rec.items():\n",
    "    total = total + t\n",
    "    correct = correct + c\n",
    "print(f'acc: {correct/total*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Videomme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为每个数据集获得分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium 557 900 61.89\n",
      "Object Recognition 90 132 68.18\n",
      "Object Reasoning 89 134 66.42\n",
      "OCR Problems 39 68 57.35\n",
      "Temporal Reasoning 37 73 50.68\n",
      "Attribute Perception 46 73 63.01\n",
      "Action Recognition 75 119 63.03\n",
      "Counting Problem 36 95 37.89\n",
      "Spatial Reasoning 14 18 77.78\n",
      "Action Reasoning 33 58 56.90\n",
      "Spatial Perception 10 21 47.62\n",
      "Information Synopsis 66 78 84.62\n",
      "Temporal Perception 22 31 70.97\n",
      "global_correct: 557\n",
      "total: 900\n",
      "ACC: 61.89\n"
     ]
    }
   ],
   "source": [
    "# 获取各个长度下各个任务的分数\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "result = '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme_mid-sparse-400-b4_p3_top5_c12_cmpr4/0524_1843_videoxl_videoxl_model_args_1a77cd/videomme_mid.json'\n",
    "# result = '/share/LXRlxr0_0/code/videoxl2/lmm-eval/logs/Sparse5/videomme-bsz32/0515_1340_videoxl_videoxl_model_args_79c069/videomme_mid.json'\n",
    "\n",
    "prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "if 'jsonl' in result:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = [json.loads(line) for line in file]\n",
    "else:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = json.load(file)\n",
    "\n",
    "    datas = datas['logs']\n",
    "\n",
    "logs = datas\n",
    "\n",
    "global_correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_by_duration = {}\n",
    "correct_by_task = {}\n",
    "total_by_duration = {}\n",
    "total_by_task = {}\n",
    "\n",
    "records_pred_answer = []\n",
    "for every_res in logs:\n",
    "    if 'videomme_percetion_score' in every_res:\n",
    "        mlvu_percetion_score = every_res['videomme_percetion_score'] # \n",
    "    else:\n",
    "        mlvu_percetion_score = every_res['videomme_perception_score']\n",
    "    duration = mlvu_percetion_score['duration']\n",
    "    task_type = mlvu_percetion_score['task_category']\n",
    "    \n",
    "    pred_answer = mlvu_percetion_score['pred_answer']\n",
    "    answer = mlvu_percetion_score['answer']\n",
    "\n",
    "    # pred_answer = every_res['filtered_resps'][0]\n",
    "    # if len(pred_answer) > 5:\n",
    "    #     records_pred_answer.append(pred_answer)\n",
    "    #     continue\n",
    "\n",
    "    if pred_answer == answer:\n",
    "        if task_type not in correct_by_task:\n",
    "            correct_by_task[task_type] = 1\n",
    "        else:\n",
    "            correct_by_task[task_type] += 1\n",
    "\n",
    "        if duration not in correct_by_duration:\n",
    "            correct_by_duration[duration] = 1\n",
    "        else:\n",
    "            correct_by_duration[duration] += 1\n",
    "\n",
    "    if task_type not in total_by_task:\n",
    "        total_by_task[task_type] = 1\n",
    "    else:\n",
    "        total_by_task[task_type] += 1\n",
    "\n",
    "    if duration not in total_by_duration:\n",
    "        total_by_duration[duration] = 1\n",
    "    else:\n",
    "        total_by_duration[duration] += 1\n",
    "\n",
    "    if answer == pred_answer:\n",
    "        global_correct += 1\n",
    "\n",
    "    total = total + 1\n",
    "\n",
    "new_result_to_result = {}\n",
    "\n",
    "for items in correct_by_duration.items():\n",
    "    duration = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_duration[duration]/total_by_duration[duration]*100\n",
    "    print(f'{duration} {correct_by_duration[duration]} {total_by_duration[duration]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[duration] = {\n",
    "        'total': total_by_duration[duration],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "for items in correct_by_task.items():\n",
    "    task_type = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "    print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[task_type] = {\n",
    "        'total': total_by_task[task_type],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "print(f'global_correct: {global_correct}')\n",
    "print(f'total: {total}')\n",
    "print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "new_result_to_result['global'] = {\n",
    "    'total': total,\n",
    "    'correct': global_correct,\n",
    "    'accuracy': global_correct/total*100\n",
    "}\n",
    "\n",
    "task_types = list(new_result_to_result.keys())\n",
    "save_name = prefix + '_' + 'results_each_dataset'\n",
    "for tt in task_types:\n",
    "    if ' ' in tt:\n",
    "        index = tt.find(' ')\n",
    "        tt = tt[0] + tt[index+1]\n",
    "    if tt == 'global':\n",
    "        continue\n",
    "    save_name = save_name + '_' + str(tt)\n",
    "save_name = save_name + '.json'\n",
    "\n",
    "save_dir = os.path.dirname(result)\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "save_path\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(new_result_to_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印 markdown 形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name | Counting Problem | Object Reasoning | Temporal Reasoning | Attribute Perception | OCR Problems | Temporal Perception | Object Recognition | Action Reasoning | Information Synopsis | Spatial Reasoning | Action Recognition | Spatial Perception | global |\n",
      "|------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
      "|  videomme-sparse-400-b4_p3_top2_c6_cmpr4  | 40.30 | 60.35 | 48.59 | 72.07 | 64.75 | 74.55 | 63.56 | 57.89 | 81.11 | 76.79 | 68.37 | 61.11 | 63.00 |\n",
      "|  videomme-sparse-400-b4_p3_top4_c6_cmpr4  | 45.15 | 60.13 | 49.15 | 74.32 | 68.35 | 74.55 | 64.41 | 59.65 | 80.80 | 78.57 | 70.29 | 61.11 | 64.37 |\n",
      "|  videomme-sparse-400-b4_p3_top3_c6_cmpr4  | 42.91 | 60.35 | 48.02 | 72.97 | 65.47 | 74.55 | 63.56 | 58.25 | 81.11 | 78.57 | 69.33 | 62.96 | 63.56 |\n",
      "|  videomme-sparse-400-b4_p3_top5_c6_cmpr4  | 45.90 | 60.79 | 50.85 | 73.87 | 66.91 | 76.36 | 67.80 | 59.65 | 81.42 | 80.36 | 70.29 | 62.96 | 65.19 |\n",
      "|  videomme_mid_long-sparse-400-b4_p3_top6_c6_cmpr4  | 37.76 | 57.22 | 48.17 | 64.00 | 56.10 | 67.57 | 60.75 | 57.14 | 79.67 | 75.86 | 62.09 | 41.67 | 59.33 |\n",
      "|  videomme-sparse-400-b4_p3_top1_c6_cmpr4  | 39.93 | 58.37 | 45.76 | 70.72 | 61.15 | 70.91 | 63.84 | 58.25 | 81.11 | 76.79 | 66.45 | 62.96 | 61.96 |\n",
      "\n",
      "\n",
      "Counting Problem: correct 123, total 268, acc 45.90, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Object Reasoning: correct 276, total 454, acc 60.79, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Temporal Reasoning: correct 90, total 177, acc 50.85, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Attribute Perception: correct 165, total 222, acc 74.32, source videomme-sparse-400-b4_p3_top4_c6_cmpr4\n",
      "OCR Problems: correct 95, total 139, acc 68.35, source videomme-sparse-400-b4_p3_top4_c6_cmpr4\n",
      "Temporal Perception: correct 42, total 55, acc 76.36, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Object Recognition: correct 240, total 354, acc 67.80, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Action Reasoning: correct 170, total 285, acc 59.65, source videomme-sparse-400-b4_p3_top4_c6_cmpr4\n",
      "Information Synopsis: correct 263, total 323, acc 81.42, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Spatial Reasoning: correct 45, total 56, acc 80.36, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
      "Action Recognition: correct 220, total 313, acc 70.29, source videomme-sparse-400-b4_p3_top4_c6_cmpr4\n",
      "Spatial Perception: correct 34, total 54, acc 62.96, source videomme-sparse-400-b4_p3_top3_c6_cmpr4\n",
      "\n",
      "Best Performance: 65.30\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_each_dataset = [\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/videomme/videomme_2_32_top1_wbe/shuyan__VideoXL_weight_8/20250306_144757_results_each_dataset_short_medium_long_OR_TR_AP_OP_TP_OR_AR_IS_SR_AR_CP_SP.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/videomme/videomme_2_32_top2_wbe/shuyan__VideoXL_weight_8/20250306_133324_results_each_dataset_short_medium_long_OR_TR_AP_OP_TP_OR_AR_IS_SR_AR_CP_SP.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/videomme/videomme_2_32_top5_wbe/shuyan__VideoXL_weight_8/20250306_103657_results_each_dataset_short_medium_long_OR_TR_AP_OP_TP_OR_AR_IS_SR_AR_CP_SP.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/videomme/videomme_2_32_top4_wbe/shuyan__VideoXL_weight_8/20250306_121911_results_each_dataset_short_medium_long_OR_TR_AP_OP_TP_OR_AR_IS_SR_AR_CP_SP.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/videomme/videomme_2_32_top3_wbe/shuyan__VideoXL_weight_8/20250306_103627_results_each_dataset_short_medium_long_OR_TR_AP_OP_TP_OR_AR_IS_SR_AR_CP_SP.json'\n",
    "]\n",
    "\n",
    "\n",
    "results_each_dataset = None\n",
    "if results_each_dataset is None:\n",
    "    import glob\n",
    "    directory = \"/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME\"\n",
    "    # 递归搜索所有子目录，匹配包含 'qin' 的 .jsonl 文件\n",
    "    results_each_dataset = glob.glob(f\"{directory}/**/*results_each_dataset*.json\", recursive=True)\n",
    "\n",
    "results_each_dataset = [tmp for tmp in results_each_dataset if 'stage4_sparse' not in tmp]\n",
    "\n",
    "columns = ['short', 'medium', 'long', 'global']\n",
    "# columns = ['medium', 'long']\n",
    "columns = [\n",
    " 'Counting Problem',\n",
    " 'Object Reasoning',\n",
    " 'Temporal Reasoning',\n",
    " 'Attribute Perception',\n",
    " 'OCR Problems',\n",
    " 'Temporal Perception',\n",
    " 'Object Recognition',\n",
    " 'Action Reasoning',\n",
    " 'Information Synopsis',\n",
    " 'Spatial Reasoning',\n",
    " 'Action Recognition',\n",
    " 'Spatial Perception',\n",
    " 'global']\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(columns) + \"\\n\"\n",
    "\n",
    "max_performance_record = {}\n",
    "for result_path in results_each_dataset:\n",
    "    # print(result_path)\n",
    "    with open(result_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = result_path.split('/')[-3]\n",
    "    # 提取列名和准确率\n",
    "    accuracies = []\n",
    "    for col in columns:\n",
    "        accuracies.extend([f\"{data[col]['accuracy']:.2f}\" ])\n",
    "        if (col not in max_performance_record or data[col]['accuracy'] > max_performance_record[col]['accuracy']) and col != 'global':\n",
    "            max_performance_record[col] = data[col]\n",
    "            max_performance_record[col]['source'] = model_name\n",
    "            \n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n",
    "print()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for key, val in max_performance_record.items():\n",
    "    if key == 'global':\n",
    "        continue\n",
    "    print(f\"{key}: correct {val['correct']}, total {val['total']}, acc {val['accuracy']:.2f}, source {val['source']}\")\n",
    "    correct = correct + val['correct']\n",
    "    total = total + val['total']\n",
    "\n",
    "print(f\"\\nBest Performance: {correct / total*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整每个长度下的任务得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name | CP | OR | TR | AP | OP | TP | OR | AR | IS | SR | AR | SP | global |\n",
      "|------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
      "|  videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4  | 48.00 | 77.50 | 61.54 | 82.79 | 91.23 | 88.89 | 76.19 | 76.60 | 91.46 | 88.89 | 80.92 | 80.00 | 76.89 |\n",
      "|  videomme_w_subtitle-sparse-400-b4_p3_top4_c12_cmpr4  | 48.00 | 77.50 | 61.54 | 82.79 | 91.23 | 88.89 | 76.19 | 76.60 | 91.46 | 88.89 | 80.15 | 80.00 | 76.78 |\n",
      "|  videomme_w_subtitle-sparse-400-b4_p3_top7_c12_cmpr4  | 47.20 | 77.50 | 61.54 | 85.25 | 91.23 | 88.89 | 75.60 | 76.60 | 90.24 | 88.89 | 83.21 | 80.00 | 77.22 |\n",
      "|  videomme_w_subtitle-sparse-400-b4_p3_top6_c12_cmpr4  | 48.00 | 77.50 | 69.23 | 84.43 | 91.23 | 88.89 | 75.00 | 76.60 | 90.24 | 88.89 | 81.68 | 80.00 | 77.00 |\n",
      "\n",
      "\n",
      "Perception Tasks !!!!\n",
      "Action Recognition: correct 109, total 131, acc 83.21, source videomme_w_subtitle-sparse-400-b4_p3_top7_c12_cmpr4\n",
      "Object Recognition: correct 128, total 168, acc 76.19, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "Attribute Perception: correct 104, total 122, acc 85.25, source videomme_w_subtitle-sparse-400-b4_p3_top7_c12_cmpr4\n",
      "Spatial Perception: correct 24, total 30, acc 80.00, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "Temporal Perception: correct 16, total 18, acc 88.89, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "\n",
      "Reasoning Tasks !!!!\n",
      "Spatial Reasoning: correct 24, total 27, acc 88.89, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "Temporal Reasoning: correct 9, total 13, acc 69.23, source videomme_w_subtitle-sparse-400-b4_p3_top6_c12_cmpr4\n",
      "Action Reasoning: correct 36, total 47, acc 76.60, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "Object Reasoning: correct 62, total 80, acc 77.50, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "\n",
      "Other Tasks !!!!\n",
      "Information Synopsis: correct 75, total 82, acc 91.46, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "Counting Problem: correct 60, total 125, acc 48.00, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "OCR Problems: correct 52, total 57, acc 91.23, source videomme_w_subtitle-sparse-400-b4_p3_top5_c12_cmpr4\n",
      "\n",
      "Best Performance: 77.67\n"
     ]
    }
   ],
   "source": [
    "# 获取各个长度下各个任务的分数\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def work(result, specified_duration='medium'):\n",
    "\n",
    "    prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "    if 'jsonl' in result:\n",
    "        with open(result, 'r') as file:\n",
    "            datas = [json.loads(line) for line in file]\n",
    "    else:\n",
    "        with open(result, 'r') as file:\n",
    "            datas = json.load(file)['logs']\n",
    "\n",
    "    logs = datas\n",
    "\n",
    "    global_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_by_duration = {}\n",
    "    correct_by_task = {}\n",
    "    total_by_duration = {}\n",
    "    total_by_task = {}\n",
    "\n",
    "    for every_res in logs:\n",
    "        mlvu_percetion_score = every_res['videomme_percetion_score']\n",
    "        duration = mlvu_percetion_score['duration']\n",
    "        if duration != specified_duration:\n",
    "            continue\n",
    "\n",
    "        task_type = mlvu_percetion_score['task_category']\n",
    "        \n",
    "        pred_answer = mlvu_percetion_score['pred_answer']\n",
    "        answer = mlvu_percetion_score['answer']\n",
    "\n",
    "        if pred_answer == answer:\n",
    "            if task_type not in correct_by_task:\n",
    "                correct_by_task[task_type] = 1\n",
    "            else:\n",
    "                correct_by_task[task_type] += 1\n",
    "\n",
    "            if duration not in correct_by_duration:\n",
    "                correct_by_duration[duration] = 1\n",
    "            else:\n",
    "                correct_by_duration[duration] += 1\n",
    "\n",
    "        if task_type not in total_by_task:\n",
    "            total_by_task[task_type] = 1\n",
    "        else:\n",
    "            total_by_task[task_type] += 1\n",
    "\n",
    "        if duration not in total_by_duration:\n",
    "            total_by_duration[duration] = 1\n",
    "        else:\n",
    "            total_by_duration[duration] += 1\n",
    "\n",
    "        if answer == pred_answer:\n",
    "            global_correct += 1\n",
    "\n",
    "        total = total + 1\n",
    "\n",
    "        new_result_to_result = {}\n",
    "\n",
    "    if len(correct_by_duration) == 0:\n",
    "        return {}\n",
    "\n",
    "    for items in correct_by_duration.items():\n",
    "        duration = items[0]\n",
    "        correct = items[1]\n",
    "        accuracy = correct_by_duration[duration]/total_by_duration[duration]*100\n",
    "        # print(f'{duration} {correct_by_duration[duration]} {total_by_duration[duration]} {accuracy:.2f}')\n",
    "\n",
    "        new_result_to_result[duration] = {\n",
    "            'total': total_by_duration[duration],\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    for items in correct_by_task.items():\n",
    "        task_type = items[0]\n",
    "        correct = items[1]\n",
    "        accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "\n",
    "        # print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "        new_result_to_result[task_type] = {\n",
    "            'total': total_by_task[task_type],\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    # print(f'global_correct: {global_correct}')\n",
    "    # print(f'total: {total}')\n",
    "    # print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "    new_result_to_result['global'] = {\n",
    "        'total': total,\n",
    "        'correct': global_correct,\n",
    "        'accuracy': global_correct/total*100\n",
    "    }\n",
    "\n",
    "    return new_result_to_result\n",
    "    \n",
    "\n",
    "# results = [\n",
    "#     '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme-sparse-400-b4_p3_top1_c6_cmpr4/0524_0225_videoxl_videoxl_model_args_351fea/videomme.json',\n",
    "#     '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme-sparse-400-b4_p3_top2_c6_cmpr4/0524_0419_videoxl_videoxl_model_args_b50391/videomme.json',\n",
    "#     '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme-sparse-400-b4_p3_top3_c6_cmpr4/0524_0612_videoxl_videoxl_model_args_240515/videomme.json',\n",
    "#     '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme-sparse-400-b4_p3_top4_c6_cmpr4/0524_0807_videoxl_videoxl_model_args_5556e3/videomme.json',\n",
    "#     '/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME/videomme-sparse-400-b4_p3_top5_c6_cmpr4/0524_1000_videoxl_videoxl_model_args_fe82fa/videomme.json'\n",
    "#     ]\n",
    "\n",
    "results = None\n",
    "if results is None:\n",
    "    import glob\n",
    "    directory = \"/share/minghao/VideoProjects/lmm-eval-sparse/logs/VideoMME_sub\"\n",
    "    # 递归搜索所有子目录，匹配包含 'qin' 的 .jsonl 文件\n",
    "    results = glob.glob(f\"{directory}/**/*videomme.json\", recursive=True)\n",
    "\n",
    "\n",
    "# results = [tmp for tmp in results if 'c12' in tmp]\n",
    "results = [tmp for tmp in results if 'stage4' not in tmp]\n",
    "# results = [tmp for tmp in results if 'mmvid' not in tmp]\n",
    "\n",
    "specified_duration = 'short'\n",
    "\n",
    "all_datas = {}\n",
    "for result_path in results:\n",
    "    model_name = result_path.split('/')[-3]\n",
    "    result_dict = work(result_path, specified_duration=specified_duration)\n",
    "    if len(result_dict) == 0:\n",
    "        continue\n",
    "    all_datas[model_name] = result_dict\n",
    "\n",
    "\n",
    "# columns = ['short', 'medium', 'long', 'global']\n",
    "# columns = ['medium']\n",
    "columns = [\n",
    " 'Counting Problem',\n",
    " 'Object Reasoning',\n",
    " 'Temporal Reasoning',\n",
    " 'Attribute Perception',\n",
    " 'OCR Problems',\n",
    " 'Temporal Perception',\n",
    " 'Object Recognition',\n",
    " 'Action Reasoning',\n",
    " 'Information Synopsis',\n",
    " 'Spatial Reasoning',\n",
    " 'Action Recognition',\n",
    " 'Spatial Perception',\n",
    " 'global']\n",
    "\n",
    "\n",
    "new_columns = []\n",
    "for tmp in columns:\n",
    "    if ' ' in tmp:\n",
    "        tmp = tmp.split(' ')\n",
    "        new_name = tmp[0][0] + tmp[1][0]\n",
    "        new_columns.append(new_name)\n",
    "    else:\n",
    "        new_columns.append(tmp)\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(new_columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(new_columns) + \"\\n\"\n",
    "\n",
    "max_performance_record = {}\n",
    "for model_name, data in all_datas.items():\n",
    "    # 提取列名和准确率\n",
    "    accuracies = []\n",
    "    for col in columns:\n",
    "        if col not in data:\n",
    "            continue\n",
    "        accuracies.extend([f\"{data[col]['accuracy']:.2f}\" ])\n",
    "        if (col not in max_performance_record or data[col]['accuracy'] > max_performance_record[col]['accuracy']) and col != 'global':\n",
    "            max_performance_record[col] = data[col]\n",
    "            max_performance_record[col]['source'] = model_name\n",
    "\n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n",
    "print()\n",
    "# print(max_performance_record)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "perception_tasks = [\n",
    "    'Action Recognition',\n",
    "    'Object Recognition',\n",
    "    'Attribute Perception',\n",
    "    'Spatial Perception',\n",
    "    'Temporal Perception',\n",
    "]\n",
    "\n",
    "reasoning_tasks = [\n",
    "    'Spatial Reasoning',\n",
    "    'Temporal Reasoning',\n",
    "    'Action Reasoning',\n",
    "    'Object Reasoning',\n",
    "]\n",
    "\n",
    "other_tasks = [\n",
    "    'Information Synopsis',\n",
    "    'Counting Problem',\n",
    "    'OCR Problems',\n",
    "]\n",
    "\n",
    "print(f'Perception Tasks !!!!')\n",
    "for task in perception_tasks:\n",
    "    if task not in max_performance_record:\n",
    "        continue\n",
    "    print(f\"{task}: correct {max_performance_record[task]['correct']}, total {max_performance_record[task]['total']}, acc {max_performance_record[task]['accuracy']:.2f}, source {max_performance_record[task]['source']}\")\n",
    "    correct = correct + max_performance_record[task]['correct']\n",
    "    total = total + max_performance_record[task]['total']\n",
    "\n",
    "print(f'\\nReasoning Tasks !!!!')\n",
    "for task in reasoning_tasks:\n",
    "    if task not in max_performance_record:\n",
    "        continue\n",
    "    print(f\"{task}: correct {max_performance_record[task]['correct']}, total {max_performance_record[task]['total']}, acc {max_performance_record[task]['accuracy']:.2f}, source {max_performance_record[task]['source']}\")\n",
    "    correct = correct + max_performance_record[task]['correct']\n",
    "    total = total + max_performance_record[task]['total']\n",
    "\n",
    "print(f'\\nOther Tasks !!!!')\n",
    "for task in other_tasks:\n",
    "    if task not in max_performance_record:\n",
    "        continue\n",
    "    print(f\"{task}: correct {max_performance_record[task]['correct']}, total {max_performance_record[task]['total']}, acc {max_performance_record[task]['accuracy']:.2f}, source {max_performance_record[task]['source']}\")\n",
    "    correct = correct + max_performance_record[task]['correct']\n",
    "    total = total + max_performance_record[task]['total']\n",
    "\n",
    "# for key, val in max_performance_record.items():\n",
    "#     if key == 'global':\n",
    "#         continue\n",
    "#     print(f\"{key}: correct {val['correct']}, total {val['total']}, acc {val['accuracy']:.2f}, source {val['source']}\")\n",
    "#     correct = correct + val['correct']\n",
    "#     total = total + val['total']\n",
    "\n",
    "print(f\"\\nBest Performance: {correct / total*100:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(68.22 + 61.65 + 77.67)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Information Synopsis: correct 71, total 82, acc 86.59, source videomme-sparse-400-b4_p3_top2_c6_cmpr4\n",
    "\n",
    "\n",
    "Action Recognition: correct 105, total 131, acc 80.15, source videomme_short-sparse-400-b4_p3_top6_c12_cmpr4\n",
    "Spatial Perception: correct 24, total 30, acc 80.00, source videomme-sparse-400-b4_p3_top3_c6_cmpr4\n",
    "Temporal Perception: correct 17, total 18, acc 94.44, source videomme_short-sparse-400-b4_p3_top6_c12_cmpr4\n",
    "Object Recognition: correct 127, total 168, acc 75.60, source videomme_short-sparse-400-b4_p3_top6_c12_cmpr4\n",
    "Attribute Perception: correct 102, total 122, acc 83.61, source videomme_short-sparse-400-b4_p3_top7_c12_cmpr4\n",
    "\n",
    "Spatial Reasoning: correct 23, total 27, acc 88.89, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
    "Object Reasoning: correct 60, total 80, acc 75.00, source videomme_short-sparse-400-b4_p3_top8_c12_cmpr4\n",
    "Temporal Reasoning: correct 8, total 13, acc 61.54, source videomme_short-sparse-400-b4_p3_top6_c12_cmpr4\n",
    "Action Reasoning: correct 35, total 47, acc 74.47, source videomme_short-sparse-400-b4_p3_top6_c12_cmpr4\n",
    "\n",
    "\n",
    "Counting Problem: correct 68, total 125, acc 54.40, source videomme-sparse-400-b4_p3_top5_c6_cmpr4\n",
    "OCR Problems: correct 49, total 57, acc 85.96, source videomme_short-sparse-400-b4_p3_top8_c12_cmpr4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7655555555555555"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(71 + 23 + 105 + 24 + 17 + 127 + 102 + 60 + 49 + 68 + 35 + 8)/900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.76666666666667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64.9 + 58.8 + 76.6)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best videomme-long\n",
    "Information Synopsis: 127, 163\n",
    "\n",
    "Perception\n",
    "Action Recognition: 40, 63, c6_cmpr4\n",
    "Object Recognition  24, 54  c6_cmpr4 \n",
    "Temporal Perception 3, 6, c6_cmpr4\n",
    "Attribute Perception: 16,27 c6_cmpr4\n",
    "\n",
    "Reasoning\n",
    "Spatial Reasoning 6, 11, c12_cmpr4\n",
    "Action Reasoning, 103, 180, c12_cmpr4\n",
    "Temporal Reasoning 49 91 c12_cmpr4\n",
    "Object Reasoning  134, 240 c12_cmpr4_mmvid\n",
    "\n",
    "其它：\n",
    "OCR Problems: 8, 14, c6_cmpr4\n",
    "Counting Problem: 19, 48, c12_cmpr4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.77777777777777"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 900\n",
    "correct = 127 + 40 + 24 + 3 + 16 + 6 + 103 + 49 + 134 + 8 + 19\n",
    "correct/total*100 # 64.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best videomme-mid\n",
    "Information Synopsis: 66, 78\n",
    "\n",
    "Perception\n",
    "Spatial Perception: 11, 21 .b4_p3_c6_cmpr4_mmvid\n",
    "Action Recognition: 76, 119, b4_p3_c4_cmpr8\n",
    "Object Recognition  93, 132  c6_cmpr4_mmvid\n",
    "Temporal Perception 24, 31, c6_cmpr4_mmvid\n",
    "Attribute Perception: 51,73 c6_cmpr4\n",
    "\n",
    "Reasoning\n",
    "Spatial Reasoning 14, 18, c12_cmpr4\n",
    "Action Reasoning, 33, 58, c6c12_cmpr4\n",
    "Temporal Reasoning 45 73 c12_cmpr4_mmvid\n",
    "Object Reasoning  92, 134 c12_cmpr4_mmvid\n",
    "\n",
    "其它：\n",
    "OCR Problems: 40, 68, c12_cmpr4_mmvid\n",
    "Counting Problem：39, 95, c12_cmpr4\n",
    "\n",
    "total = 900\n",
    "correct = 66 + 11 + 76 + 93 + 24 + 51 + 14 + 33 + 45 + 92 + 40 + 39\n",
    "correct/total*100 # 64.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/share/minghao/Projects/lmms-eval/results3/videoxl_videomme/shuyan__VideoXL_weight_8/20250115_200416_results_each_dataset_short_medium_long.json'\n",
    "path2 = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32_top5_videomme/shuyan__VideoXL_weight_8/20250116_005741_results_each_dataset_short_medium_long.json'\n",
    "path2 = '/share/minghao/Projects/lmms-eval/results2/videoxl_adaptivebeacon_2_32_top3_videomme/shuyan__VideoXL_weight_8/20250116_004725_results_each_dataset_short_medium_long.json'\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        datas = json.load(f)\n",
    "    return datas\n",
    "\n",
    "\n",
    "datas1 = load_json(path1)\n",
    "datas2 = load_json(path2)\n",
    "\n",
    "result = []\n",
    "result.append(\"| Task | data1 | data2 | relation |\")\n",
    "result.append(\"|------|-------|-------|-------|\")\n",
    "\n",
    "for key1, value1 in datas1.items():\n",
    "    if key1 == 'global':\n",
    "        continue\n",
    "    if key1 != 'long':\n",
    "        continue\n",
    "    if key1 in datas2:\n",
    "        value2 = datas2[key1]\n",
    "        for key11, value11 in value1.items():\n",
    "            if key11 in value2:\n",
    "                value22 = value2[key11]\n",
    "                acc1 = value11['accuracy']\n",
    "                acc2 = value22['accuracy']\n",
    "                acc1 = round(acc1, 2)\n",
    "                acc2 = round(acc2, 2)\n",
    "\n",
    "                if acc1 > acc2:\n",
    "                    relation = '>'\n",
    "                elif acc1 == acc2:\n",
    "                    relation = '='\n",
    "                else:\n",
    "                    relation = '<'\n",
    "                # Append a row in Markdown format\n",
    "                result.append(f\"| {key1}-{key11} | {acc1} | {acc2} | {relation} |\")\n",
    "\n",
    "# Print the Markdown table\n",
    "print(\"\\n\".join(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# VNBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取各个任务的得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "result = '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/videomme/videomme_2_32_top4_wbe/shuyan__VideoXL_weight_8/20250303_005024_samples_videomme.jsonl'\n",
    "\n",
    "prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "if 'jsonl' in result:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = [json.loads(line) for line in file]\n",
    "else:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = json.load(file)\n",
    "\n",
    "logs = datas\n",
    "\n",
    "logs_by_video_id = {}\n",
    "for every_res in logs:\n",
    "    vnbench_perception_score = every_res['vnbench_perception_score']\n",
    "    video_id = vnbench_perception_score['video_pure_name']\n",
    "    if video_id not in logs_by_video_id:\n",
    "        logs_by_video_id[video_id] = {\n",
    "            \"correct\": 1 if vnbench_perception_score['answer'] == vnbench_perception_score['pred_answer'] else 0,\n",
    "            \"task_type\": vnbench_perception_score['task_type']\n",
    "        }\n",
    "    else:\n",
    "        if vnbench_perception_score['pred_answer'] == vnbench_perception_score['answer']:\n",
    "            logs_by_video_id[video_id]['correct'] += 1\n",
    "\n",
    "\n",
    "global_correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_by_task = {}\n",
    "total_by_task = {}\n",
    "for video_id, info in logs_by_video_id.items():\n",
    "\n",
    "    task_type = info['task_type']\n",
    "    correct_count = info['correct']\n",
    "\n",
    "    if correct_count == 4:\n",
    "        if task_type not in correct_by_task:\n",
    "            correct_by_task[task_type] = 1\n",
    "        else:\n",
    "            correct_by_task[task_type] += 1\n",
    "\n",
    "        global_correct += 1\n",
    "\n",
    "    if task_type not in total_by_task:\n",
    "        total_by_task[task_type] = 1\n",
    "    else:\n",
    "        total_by_task[task_type] += 1\n",
    "\n",
    "    total = total + 1\n",
    "\n",
    "new_result_to_result = {}\n",
    "\n",
    "for items in correct_by_task.items():\n",
    "    task_type = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "    print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[task_type] = {\n",
    "        'total': total_by_task[task_type],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "print(f'global_correct: {global_correct}')\n",
    "print(f'total: {total}')\n",
    "print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "new_result_to_result['global'] = {\n",
    "    'total': total,\n",
    "    'correct': global_correct,\n",
    "    'accuracy': global_correct/total*100\n",
    "}\n",
    "\n",
    "task_types = list(new_result_to_result.keys())\n",
    "save_name = prefix + '_' + 'results_each_dataset'\n",
    "for tt in task_types:\n",
    "    if tt == 'global':\n",
    "        continue\n",
    "    save_name = save_name + '_' + tt\n",
    "save_name = save_name + '.json'\n",
    "\n",
    "save_dir = os.path.dirname(result)\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(new_result_to_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印 markdown 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_each_dataset = [\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/vnbench/vnbench_2_32_top1/shuyan__VideoXL_weight_8/20250212_122354_results_each_dataset_ret_insert1_ret_edit1_ord_edit1_ord_insert2_cnt_edit1_cnt_edit2_cnt_insert1_ret_insert2_ord_insert1.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/vnbench/vnbench_2_32_top2/shuyan__VideoXL_weight_8/20250212_131007_results_each_dataset_ret_insert1_ret_edit1_ord_edit1_ord_insert2_cnt_edit1_cnt_edit2_cnt_insert1_ret_insert2_ord_insert1.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/vnbench/vnbench_2_32_top3/shuyan__VideoXL_weight_8/20250212_135735_results_each_dataset_ret_insert1_ret_edit1_ord_edit1_ord_insert2_cnt_edit1_cnt_edit2_cnt_insert1_ret_insert2_ord_insert1.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/vnbench/vnbench_2_32_top4/shuyan__VideoXL_weight_8/20250212_140359_results_each_dataset_ret_insert1_ret_edit1_ord_edit1_ord_insert2_cnt_edit1_cnt_edit2_cnt_insert1_ret_insert2_ord_insert1.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp1/vnbench/vnbench_2_32_top5/shuyan__VideoXL_weight_8/20250212_144856_results_each_dataset_ret_insert1_ret_edit1_ord_edit1_ord_insert2_cnt_edit1_cnt_edit2_cnt_insert1_ret_insert2_ord_insert1.json'\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"ret_insert1\",\n",
    "\"ret_insert2\",\n",
    "\"ret_edit1\",\n",
    "\"cnt_insert1\",\n",
    "\"cnt_edit1\",\n",
    "\"cnt_edit2\",\n",
    "\"ord_insert1\",\n",
    "\"ord_insert2\",\n",
    "\"ord_edit1\",]\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(columns) + \"\\n\"\n",
    "\n",
    "for result_path in results_each_dataset:\n",
    "    with open(result_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = result_path.split('/')[-3]\n",
    "\n",
    "    # 提取列名和准确率\n",
    "    accuracies = [f\"{data[col]['accuracy']:.2f}\" for col in columns]\n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LongVideo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取各任务分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 262 412 63.59\n",
      "60 128 172 74.42\n",
      "15 132 189 69.84\n",
      "3600 284 564 50.35\n",
      "T2A 50 79 63.29\n",
      "T2O 44 76 57.89\n",
      "O2E 53 87 60.92\n",
      "S2E 64 93 68.82\n",
      "T3E 39 73 53.42\n",
      "E3E 63 94 67.02\n",
      "T3O 45 74 60.81\n",
      "SSS 46 97 47.42\n",
      "TOS 28 73 38.36\n",
      "E2O 49 65 75.38\n",
      "S2A 68 88 77.27\n",
      "SAA 39 72 54.17\n",
      "O3O 40 66 60.61\n",
      "S2O 40 72 55.56\n",
      "SOS 55 81 67.90\n",
      "T2E 44 65 67.69\n",
      "TAA 39 82 47.56\n",
      "global_correct: 806\n",
      "total: 1337\n",
      "ACC: 60.28\n"
     ]
    }
   ],
   "source": [
    "# 获取各个长度下各个任务的分数\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "result = '/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo/longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4/0524_1123_videoxl_videoxl_model_args_68f9ae/longvideobench_val_v.json'\n",
    "\n",
    "prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "if 'jsonl' in result:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = [json.loads(line) for line in file]\n",
    "else:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = json.load(file)['logs']\n",
    "\n",
    "logs = datas\n",
    "\n",
    "global_correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_by_duration = {}\n",
    "correct_by_task = {}\n",
    "total_by_duration = {}\n",
    "total_by_task = {}\n",
    "\n",
    "for every_res in logs:\n",
    "    mlvu_percetion_score = every_res['lvb_acc']\n",
    "    duration = mlvu_percetion_score['duration_group']\n",
    "    task_type = mlvu_percetion_score['question_category']\n",
    "    \n",
    "    pred_answer = mlvu_percetion_score['parsed_pred']\n",
    "    answer = mlvu_percetion_score['answer']\n",
    "\n",
    "    if pred_answer == answer:\n",
    "        if task_type not in correct_by_task:\n",
    "            correct_by_task[task_type] = 1\n",
    "        else:\n",
    "            correct_by_task[task_type] += 1\n",
    "\n",
    "        if duration not in correct_by_duration:\n",
    "            correct_by_duration[duration] = 1\n",
    "        else:\n",
    "            correct_by_duration[duration] += 1\n",
    "\n",
    "    if task_type not in total_by_task:\n",
    "        total_by_task[task_type] = 1\n",
    "    else:\n",
    "        total_by_task[task_type] += 1\n",
    "\n",
    "    if duration not in total_by_duration:\n",
    "        total_by_duration[duration] = 1\n",
    "    else:\n",
    "        total_by_duration[duration] += 1\n",
    "\n",
    "    if answer == pred_answer:\n",
    "        global_correct += 1\n",
    "\n",
    "    total = total + 1\n",
    "\n",
    "new_result_to_result = {}\n",
    "\n",
    "for items in correct_by_duration.items():\n",
    "    duration = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_duration[duration]/total_by_duration[duration]*100\n",
    "    print(f'{duration} {correct_by_duration[duration]} {total_by_duration[duration]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[duration] = {\n",
    "        'total': total_by_duration[duration],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "for items in correct_by_task.items():\n",
    "    task_type = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "    print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[task_type] = {\n",
    "        'total': total_by_task[task_type],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "print(f'global_correct: {global_correct}')\n",
    "print(f'total: {total}')\n",
    "print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "new_result_to_result['global'] = {\n",
    "    'total': total,\n",
    "    'correct': global_correct,\n",
    "    'accuracy': global_correct/total*100\n",
    "}\n",
    "\n",
    "task_types = list(new_result_to_result.keys())\n",
    "save_name = prefix + '_' + 'results_each_dataset'\n",
    "for tt in task_types:\n",
    "    if tt == 'global':\n",
    "        continue\n",
    "    save_name = save_name + '_' + str(tt)\n",
    "save_name = save_name + '.json'\n",
    "\n",
    "save_dir = os.path.dirname(result)\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "save_path\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(new_result_to_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name | E2O | E3E | O2E | O3O | S2A | S2E | S2O | SAA | SOS | SSS | T2A | T2E | T2O | T3E | T3O | TAA | TOS |\n",
      "|------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
      "|  longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4  | 76.92 | 67.02 | 60.92 | 60.61 | 77.27 | 69.89 | 54.17 | 51.39 | 69.14 | 47.42 | 64.56 | 66.15 | 57.89 | 54.79 | 58.11 | 46.34 | 41.10 |\n",
      "|  longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4  | 75.38 | 67.02 | 60.92 | 60.61 | 77.27 | 68.82 | 55.56 | 54.17 | 67.90 | 47.42 | 63.29 | 67.69 | 57.89 | 53.42 | 60.81 | 47.56 | 38.36 |\n",
      "|  longvideobench_val_v-sparse-400-b6_p5_top6_c6_cmpr4  | 75.38 | 65.96 | 59.77 | 63.64 | 76.14 | 68.82 | 58.33 | 50.00 | 67.90 | 46.39 | 63.29 | 66.15 | 56.58 | 53.42 | 59.46 | 48.78 | 39.73 |\n",
      "\n",
      "\n",
      "E2O: correct 50, total 65, acc 76.92, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "E3E: correct 63, total 94, acc 67.02, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "O2E: correct 53, total 87, acc 60.92, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "O3O: correct 42, total 66, acc 63.64, source longvideobench_val_v-sparse-400-b6_p5_top6_c6_cmpr4\n",
      "S2A: correct 68, total 88, acc 77.27, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "S2E: correct 65, total 93, acc 69.89, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "S2O: correct 42, total 72, acc 58.33, source longvideobench_val_v-sparse-400-b6_p5_top6_c6_cmpr4\n",
      "SAA: correct 39, total 72, acc 54.17, source longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4\n",
      "SOS: correct 56, total 81, acc 69.14, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "SSS: correct 46, total 97, acc 47.42, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "T2A: correct 51, total 79, acc 64.56, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "T2E: correct 44, total 65, acc 67.69, source longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4\n",
      "T2O: correct 44, total 76, acc 57.89, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "T3E: correct 40, total 73, acc 54.79, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "T3O: correct 45, total 74, acc 60.81, source longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4\n",
      "TAA: correct 40, total 82, acc 48.78, source longvideobench_val_v-sparse-400-b6_p5_top6_c6_cmpr4\n",
      "TOS: correct 30, total 73, acc 41.10, source longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4\n",
      "\n",
      "Best Performance: 61.18\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_each_dataset = [\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/longvideobench/longvideobench_2_64_top1_wobe/shuyan__VideoXL_weight_8/20250306_015010_results_each_dataset_600_60_15_3600_E3E_TOS_S2A_O3O_T3O_O2E_SAA_S2O_S2E_T2E_E2O_T2O_TAA_T3E_SSS_T2A_SOS.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/longvideobench/longvideobench_2_64_top2_wobe/shuyan__VideoXL_weight_8/20250306_014443_results_each_dataset_600_60_15_3600_E3E_TOS_S2A_O3O_O2E_SAA_S2O_S2E_T2E_E2O_T2O_TAA_T3O_T3E_SSS_T2A_SOS.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/longvideobench/longvideobench_2_64_top3_wobe/shuyan__VideoXL_weight_8/20250306_014127_results_each_dataset_600_60_15_3600_E3E_TOS_S2A_O3O_O2E_SAA_S2O_S2E_T2E_E2O_T2O_TAA_T3O_T3E_SSS_T2A_SOS.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/longvideobench/longvideobench_2_64_top4_wobe/shuyan__VideoXL_weight_8/20250306_014133_results_each_dataset_600_60_15_3600_E3E_TOS_S2A_O3O_T3O_O2E_SAA_S2O_S2E_TAA_T2E_E2O_T2O_T3E_SSS_T2A_SOS.json',\n",
    "    '/share/minghao/Projects/lmms-eval/mainexpres/Exp7_2/cp5knextqa3k_allqkv/longvideobench/longvideobench_2_64_top5_wobe/shuyan__VideoXL_weight_8/20250306_091016_results_each_dataset_600_60_15_3600_E3E_TOS_S2A_O3O_O2E_SAA_S2O_S2E_TAA_T2E_E2O_T2O_T3O_T3E_SSS_T2A_SOS.json'\n",
    "]\n",
    "import glob\n",
    "\n",
    "directory = \"/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo\"\n",
    "results_each_dataset = None\n",
    "if results_each_dataset is None:\n",
    "    # 递归搜索所有子目录，匹配包含 'qin' 的 .jsonl 文件\n",
    "    results_each_dataset = glob.glob(f\"{directory}/**/*results_each_dataset*.json\", recursive=True)\n",
    "\n",
    "results_each_dataset = [tmp for tmp in results_each_dataset if 'stage4' not in tmp]\n",
    "\n",
    "# columns = ['15','60','600','3600']\n",
    "columns = ['E2O',\n",
    " 'E3E',\n",
    " 'O2E',\n",
    " 'O3O',\n",
    " 'S2A',\n",
    " 'S2E',\n",
    " 'S2O',\n",
    " 'SAA',\n",
    " 'SOS',\n",
    " 'SSS',\n",
    " 'T2A',\n",
    " 'T2E',\n",
    " 'T2O',\n",
    " 'T3E',\n",
    " 'T3O',\n",
    " 'TAA',\n",
    " 'TOS']\n",
    "\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(columns) + \"\\n\"\n",
    "\n",
    "max_performance_record = {}\n",
    "\n",
    "for result_path in results_each_dataset:\n",
    "    with open(result_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = result_path.split('/')[-3]\n",
    "\n",
    "    # 提取列名和准确率\n",
    "    accuracies = []\n",
    "    for col in columns:\n",
    "        accuracies.extend([f\"{data[col]['accuracy']:.2f}\" ])\n",
    "        if col not in max_performance_record or data[col]['accuracy'] > max_performance_record[col]['accuracy']:\n",
    "            max_performance_record[col] = data[col]\n",
    "            max_performance_record[col]['source'] = model_name\n",
    "\n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n",
    "print()\n",
    "# print(max_performance_record)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for key, val in max_performance_record.items():\n",
    "    if key == 'global':\n",
    "        continue\n",
    "    print(f\"{key}: correct {val['correct']}, total {val['total']}, acc {val['accuracy']:.2f}, source {val['source']}\")\n",
    "    correct = correct + val['correct']\n",
    "    total = total + val['total']\n",
    "\n",
    "print(f\"\\nBest Performance: {correct / total*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo/longvideobench_val_v-sparse-400-b6_p5_top8_c6_cmpr4/0524_1123_videoxl_videoxl_model_args_3b3bab/longvideobench_val_v.json_results_each_dataset_600_60_15_3600_T2A_T2O_O2E_S2E_T3E_E3E_T3O_SSS_TOS_E2O_S2A_O3O_S2O_SOS_T2E_SAA_TAA.json',\n",
       " '/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo/longvideobench_stage4/longvideobench_val_v.json_results_each_dataset_600_60_15_3600_T2A_T2O_O2E_S2E_T3E_E3E_T3O_SSS_TOS_E2O_S2A_SAA_O3O_S2O_SOS_T2E_TAA.json',\n",
       " '/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo/longvideobench_val_v-sparse-400-b6_p5_top10_c6_cmpr4/0524_1123_videoxl_videoxl_model_args_68f9ae/longvideobench_val_v.json_results_each_dataset_600_60_15_3600_T2A_T2O_O2E_S2E_T3E_E3E_T3O_SSS_TOS_E2O_S2A_SAA_O3O_S2O_SOS_T2E_TAA.json',\n",
       " '/share/minghao/VideoProjects/lmm-eval-sparse/logs/LongVideo/longvideobench_val_v-sparse-400-b6_p5_top6_c6_cmpr4/0524_1128_videoxl_videoxl_model_args_88b610/longvideobench_val_v.json_results_each_dataset_600_60_15_3600_T2A_T2O_O2E_S2E_T3E_E3E_T3O_SSS_TOS_E2O_S2A_O3O_S2O_SOS_T2E_SAA_TAA.json']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_each_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'E2O': {'total': 65, 'correct': 39, 'accuracy': 60.0}, 'E3E': {'total': 94, 'correct': 59, 'accuracy': 62.76595744680851}, 'O2E': {'total': 87, 'correct': 49, 'accuracy': 56.32183908045977}, 'O3O': {'total': 66, 'correct': 33, 'accuracy': 50.0}, 'S2A': {'total': 88, 'correct': 59, 'accuracy': 67.04545454545455}, 'S2E': {'total': 93, 'correct': 52, 'accuracy': 55.91397849462365}, 'S2O': {'total': 72, 'correct': 35, 'accuracy': 48.61111111111111}, 'SAA': {'total': 72, 'correct': 37, 'accuracy': 51.388888888888886}, 'SOS': {'total': 81, 'correct': 54, 'accuracy': 66.66666666666666}, 'SSS': {'total': 97, 'correct': 37, 'accuracy': 38.144329896907216}, 'T2A': {'total': 79, 'correct': 40, 'accuracy': 50.63291139240506}, 'T2E': {'total': 65, 'correct': 38, 'accuracy': 58.46153846153847}, 'T2O': {'total': 76, 'correct': 38, 'accuracy': 50.0}, 'T3E': {'total': 73, 'correct': 40, 'accuracy': 54.794520547945204}, 'T3O': {'total': 74, 'correct': 35, 'accuracy': 47.2972972972973}, 'TAA': {'total': 82, 'correct': 35, 'accuracy': 42.68292682926829}, 'TOS': {'total': 73, 'correct': 29, 'accuracy': 39.726027397260275}}\n",
    "\n",
    "assert len(res) == 17\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for key, val in res.items():\n",
    "    total += val['total']\n",
    "    correct += val['correct']\n",
    "\n",
    "assert total == 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "709/1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/share/LXRlxr0_0/code/videoxl2/lmm-eval/logs/videoxl-stage4-mvbench/0512_1919_videoxl_videoxl_model_args_bd8108/results.json'\n",
    "datas = json.load(open(path, 'r'))\n",
    "\n",
    "results = datas['results']\n",
    "\n",
    "acc = 0\n",
    "count = 0\n",
    "for res in results:\n",
    "    key = \"mvbench_accuracy,none\"\n",
    "    if key in results[res]:\n",
    "        acc = acc + results[res][key]\n",
    "        count = count + 1\n",
    "        print(f'{res}: {results[res][key]}')\n",
    "\n",
    "acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整 prefill 加速的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macc: 47.33\n",
      "lvbench_live, total: 262, correct: 124,  acc: 47.33\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "results_dir = '/share/minghao/VideoProjects/lmm-eval/logs/Sparse_exp8/lvbench/lvbench-sparse-600-debug/0524_0136_videoxl_videoxl_model_args_889e95'\n",
    "results_dir = '/share/minghao/VideoProjects/lmm-eval/logs/Sparse_exp8/lvbench/lvbench_live-sparse-600-ds6_prev8_rollcache_notrain/0524_0845_videoxl_videoxl_model_args_4a0b52'\n",
    "\n",
    "result_names = os.listdir(results_dir)\n",
    "result_names = [name for name in result_names if 'lvbench' in name]\n",
    "result_paths = [os.path.join(results_dir, name) for name in result_names]\n",
    "\n",
    "all_datas = []\n",
    "results_by_videotype = {}\n",
    "\n",
    "for path in result_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        video_type = path.split('/')[-1].split('.json')[0]\n",
    "        datas = json.load(f)\n",
    "        all_datas.append(datas['logs'])\n",
    "        results_by_videotype[video_type] = datas\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for datas in all_datas:\n",
    "    for data in datas:\n",
    "        correct = correct + data['lvbench_mc_accuracy']['score']\n",
    "        total = total + 1\n",
    "\n",
    "print(f'macc: {correct/total*100:.2f}')\n",
    "\n",
    "record = {}\n",
    "for video_type, datas in results_by_videotype.items():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in datas['logs']:\n",
    "        correct = correct + data['lvbench_mc_accuracy']['score']\n",
    "        total = total + 1\n",
    "    record[video_type] = {'total':total, 'correct':correct}\n",
    "    print(f'{video_type}, total: {total}, correct: {correct},  acc: {correct/total*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 1549, correct: 728,  acc: 47.00\n"
     ]
    }
   ],
   "source": [
    "results_dict = {'lvbench_live': {'total': 262, 'correct': 124}, # 122, 124\n",
    " 'lvbench_tv': {'total': 250, 'correct': 116},\n",
    " 'lvbench_documentary': {'total': 197, 'correct': 99},\n",
    " 'lvbench_selfmedia': {'total': 317, 'correct': 148},\n",
    " 'lvbench_cartoon': {'total': 283, 'correct': 125}, #124, 125\n",
    " 'lvbench_sport': {'total': 240, 'correct': 116}} #114, 116\n",
    " \n",
    "t = 0\n",
    "c = 0\n",
    "for v_type, info in results_dict.items():\n",
    "    t += info['total']\n",
    "    c += info['correct']\n",
    "print(f'total: {t}, correct: {c},  acc: {c/t*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48333333333333334"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "116/240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_anno_path = '/share/LXRlxr0_0/code/videoxl2/lmm-eval/video_info.meta.jsonl'\n",
    "annos = []\n",
    "with open(raw_anno_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        annos.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 557436.53it/s]\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35270 (\\N{CJK UNIFIED IDEOGRAPH-89C6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 39057 (\\N{CJK UNIFIED IDEOGRAPH-9891}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26102 (\\N{CJK UNIFIED IDEOGRAPH-65F6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 38271 (\\N{CJK UNIFIED IDEOGRAPH-957F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30452 (\\N{CJK UNIFIED IDEOGRAPH-76F4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26041 (\\N{CJK UNIFIED IDEOGRAPH-65B9}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22270 (\\N{CJK UNIFIED IDEOGRAPH-56FE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/share/minghao/Envs/videoxl_train/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 38047 (\\N{CJK UNIFIED IDEOGRAPH-949F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "duration_minutes = []\n",
    "for anno in tqdm(annos):\n",
    "    # print(anno)\n",
    "    duration_minutes.append(anno['video_info']['duration_minutes'])\n",
    "\n",
    "# --- 绘制直方图 ---\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # 设置图的大小\n",
    "\n",
    "# 使用 seaborn 的 histplot 绘制直方图\n",
    "# kde=True 会绘制核密度估计（KDE）曲线，展示数据分布的平滑近似\n",
    "# bins 可以控制直方图的条形数量或具体的边界\n",
    "sns.histplot(duration_minutes, bins=10, kde=True, color='skyblue')\n",
    "\n",
    "plt.title('视频时长分布直方图', fontsize=16) # 设置图表标题\n",
    "plt.xlabel('时长 (分钟)', fontsize=14) # 设置 X 轴标签\n",
    "plt.ylabel('视频数量', fontsize=14) # 设置 Y 轴标签\n",
    "plt.grid(axis='y', alpha=0.75) # 添加 Y 轴网格线\n",
    "\n",
    "plt.show() # 显示图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专攻检索 decoding 的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity recognition 46 105 43.81\n",
      "key information retrieval 25 42 59.52\n",
      "summarization 3 15 20.00\n",
      "event understanding 51 103 49.51\n",
      "reasoning 25 40 62.50\n",
      "temporal grounding 3 12 25.00\n",
      "global_correct: 153\n",
      "total: 317\n",
      "ACC: 48.26\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "result = '/share/minghao/VideoProjects/lmm-eval-sparse/logs/LVBench/media/lvbench_selfmedia-sparse-600-ds4_prev11_rollcache_notrain_top8/0525_0210_videoxl_videoxl_model_args_32bfd6/lvbench_selfmedia.json'\n",
    "\n",
    "prefix = result.split('/')[-1].split('_samples')[0]\n",
    "\n",
    "if 'jsonl' in result:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = [json.loads(line) for line in file]\n",
    "else:\n",
    "    with open(result, 'r') as file:\n",
    "        datas = json.load(file)\n",
    "        datas = datas['logs']\n",
    "\n",
    "logs = datas\n",
    "\n",
    "global_correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_by_task = {}\n",
    "total_by_task = {}\n",
    "\n",
    "records_pred_answer = []\n",
    "for every_res in logs:\n",
    "    mlvu_percetion_score = every_res['lvbench_mc_accuracy']\n",
    "    task_type = every_res['doc']['question_type'][0]\n",
    "\n",
    "\n",
    "    pred_answer = mlvu_percetion_score['pred_answer']\n",
    "\n",
    "    answer = mlvu_percetion_score['gt_answer']\n",
    "\n",
    "    if pred_answer == answer:\n",
    "        if task_type not in correct_by_task:\n",
    "            correct_by_task[task_type] = 1\n",
    "        else:\n",
    "            correct_by_task[task_type] += 1\n",
    "\n",
    "    if task_type not in total_by_task:\n",
    "        total_by_task[task_type] = 1\n",
    "    else:\n",
    "        total_by_task[task_type] += 1\n",
    "\n",
    "    if answer == pred_answer:\n",
    "        global_correct += 1\n",
    "\n",
    "    total = total + 1\n",
    "\n",
    "new_result_to_result = {}\n",
    "\n",
    "for items in correct_by_task.items():\n",
    "    task_type = items[0]\n",
    "    correct = items[1]\n",
    "    accuracy = correct_by_task[task_type]/total_by_task[task_type]*100\n",
    "    print(f'{task_type} {correct_by_task[task_type]} {total_by_task[task_type]} {accuracy:.2f}')\n",
    "\n",
    "    new_result_to_result[task_type] = {\n",
    "        'total': total_by_task[task_type],\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }    \n",
    "\n",
    "\n",
    "print(f'global_correct: {global_correct}')\n",
    "print(f'total: {total}')\n",
    "print(f'ACC: {global_correct/total*100:.2f}')\n",
    "\n",
    "new_result_to_result['global'] = {\n",
    "    'total': total,\n",
    "    'correct': global_correct,\n",
    "    'accuracy': global_correct/total*100\n",
    "}\n",
    "\n",
    "task_types = list(new_result_to_result.keys())\n",
    "save_name = prefix + '_' + 'results_each_dataset'\n",
    "for tt in task_types:\n",
    "    if tt == 'global':\n",
    "        continue\n",
    "    save_name = save_name + '_' + tt\n",
    "save_name = save_name + '.json'\n",
    "\n",
    "save_dir = os.path.dirname(result)\n",
    "save_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(new_result_to_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name | event understanding | entity recognition | reasoning | summarization | temporal grounding | key information retrieval | global |\n",
      "|------------|-------|-------|-------|-------|-------|-------|-------|\n",
      "|  lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top5  | 58.93 | 49.43 | 54.17 | 60.00 | 66.67 | 36.36 | 51.78 |\n",
      "|  lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top4  | 57.14 | 50.57 | 54.17 | 60.00 | 66.67 | 40.91 | 52.28 |\n",
      "|  lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top6  | 58.93 | 49.43 | 54.17 | 60.00 | 66.67 | 36.36 | 51.78 |\n",
      "|  lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top8  | 53.57 | 50.57 | 54.17 | 60.00 | 66.67 | 31.82 | 50.25 |\n",
      "|  lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top7  | 55.36 | 49.43 | 58.33 | 60.00 | 66.67 | 31.82 | 50.76 |\n",
      "\n",
      "\n",
      "\n",
      "event understanding: correct 33, total 56, acc 58.93, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top5\n",
      "entity recognition: correct 44, total 87, acc 50.57, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top4\n",
      "reasoning: correct 14, total 24, acc 58.33, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top7\n",
      "summarization: correct 3, total 5, acc 60.00, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top5\n",
      "temporal grounding: correct 2, total 3, acc 66.67, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top5\n",
      "key information retrieval: correct 9, total 22, acc 40.91, source lvbench_documentary-sparse-600-ds4_prev11_rollcache_notrain_top4\n",
      "\n",
      "Best Performance: 53.30\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_each_dataset = []\n",
    "\n",
    "results_each_dataset = None\n",
    "if results_each_dataset is None:\n",
    "    import glob\n",
    "    directory = \"/share/minghao/VideoProjects/lmm-eval-sparse/logs/LVBench/doc\"\n",
    "    # 递归搜索所有子目录，匹配包含 'qin' 的 .jsonl 文件\n",
    "    results_each_dataset = glob.glob(f\"{directory}/**/*results_each_dataset*.json\", recursive=True)\n",
    "\n",
    "# results_each_dataset = [tmp for tmp in results_each_dataset if \"notrain\" not in tmp]\n",
    "# results_each_dataset = [tmp for tmp in results_each_dataset if \"stage4\" not in tmp]\n",
    "\n",
    "columns = ['event understanding', 'entity recognition', 'reasoning', 'summarization', 'temporal grounding', 'key information retrieval', 'global']\n",
    "\n",
    "\n",
    "markdown_table = \"| Model Name | \" + \" | \".join(columns) + \" |\\n\"\n",
    "markdown_table += \"|\" + \"------------|\" + \"-------|\" * len(columns) + \"\\n\"\n",
    "\n",
    "max_performance_record = {}\n",
    "for result_path in results_each_dataset:\n",
    "    # print(result_path)\n",
    "    with open(result_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = result_path.split('/')[-3]\n",
    "    # 提取列名和准确率\n",
    "    accuracies = []\n",
    "    for col in columns:\n",
    "        accuracies.extend([f\"{data[col]['accuracy']:.2f}\" ])\n",
    "        if (col not in max_performance_record or data[col]['accuracy'] > max_performance_record[col]['accuracy']) and col != 'global':\n",
    "            max_performance_record[col] = data[col]\n",
    "            max_performance_record[col]['source'] = model_name\n",
    "\n",
    "    # 构造 Markdown 表格\n",
    "    markdown_table += f\"|  {model_name}  | \" + \" | \".join(accuracies) + \" |\\n\"\n",
    "\n",
    "# 打印结果\n",
    "print(markdown_table)\n",
    "print()\n",
    "# print(max_performance_record)\n",
    "print()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "avg_acc = 0\n",
    "\n",
    "for key, val in max_performance_record.items():\n",
    "    print(f\"{key}: correct {val['correct']}, total {val['total']}, acc {val['accuracy']:.2f}, source {val['source']}\")\n",
    "    correct = correct + val['correct']\n",
    "    total = total + val['total']\n",
    "    avg_acc = avg_acc + val['accuracy']\n",
    "\n",
    "print(f\"\\nBest Performance: {correct / total*100:.2f}\")\n",
    "# print(f\"Avg Acc: {avg_acc / len(max_performance_record):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 1549, correct: 728,  acc: 47.00\n",
      "acc: 48.62\n"
     ]
    }
   ],
   "source": [
    "results_dict = {'lvbench_live': {'total': 262, 'correct': 124, 'acc':46.18}, # 122, 124\n",
    " 'lvbench_tv': {'total': 250, 'correct': 116, 'acc':48.40},\n",
    " 'lvbench_documentary': {'total': 197, 'correct': 99, 'acc':53.30},\n",
    " 'lvbench_selfmedia': {'total': 317, 'correct': 148, 'acc':50.79},\n",
    " 'lvbench_cartoon': {'total': 283, 'correct': 125, 'acc':45.58}, #124, 125\n",
    " 'lvbench_sport': {'total': 240, 'correct': 116, 'acc':48.41}} #114, 116\n",
    " \n",
    "t = 0\n",
    "c = 0\n",
    "acc = 0\n",
    "for v_type, info in results_dict.items():\n",
    "    t += info['total']\n",
    "    c += info['correct']\n",
    "    acc += info['acc'] * info['total']\n",
    "print(f'total: {t}, correct: {c},  acc: {c/t*100:.2f}')\n",
    "print(f'acc: {acc/t:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
