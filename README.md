<p align="center">
    <img src="https://github.com/VectorSpaceLab/XL-VLM/blob/main/assets/logo.jpg" width="200" style="margin-bottom: 0.2;"/>
</p>

<h3 align="center" style="font-size: 50px;">
    <a style="color:#9C276A;">
        XL-VLM: Extra-Long Vision Language Model for Hour-Scale Video Understanding
    </a>
</h3>
<h5 align="center"> If our project helps you, please give us a star ‚≠ê on GitHub to support us. üôèüôè </h5>

## Overview
XL-VLM is an extra-long vision language model for hour-scale video understanding. With LLM compression, XL-VLM can easily extend VLM to longer visual contexts wihout inforamtion loss. 

‚ú® Highlights:

(i) Comprehensive long video understanding. XL-VLM 7B achieves the leading performance among 7B models on MLVU, VideoMME, VNBench and LongVideoBench.

(ii) Efficient Long visual context processing. XL-VLM can process 1024 frames on an 80G GPU and achieves 100% accuracy on Needle-in-a-haystack evaluation.

(iii) XL-VLM shows strong ability in some real-world scenarios, like video summarization, surveillance anomaly detection and Ad placement identification.

![Results on Needle-in-a-a-haystack evaluation on a single 80G gpu.](./assets/needle.png)

## Plan

 - [ ] Technical Report
 - [ ] Model
 - [ ] Code
 - [ ] Data


